{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# import pyvista as pv\n",
    "# pv.set_jupyter_backend('trame')  # or 'panel' if using panel\n",
    "\n",
    "from scipy.constants import epsilon_0, e as q_e\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.collections import LineCollection\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "import trimesh\n",
    "import open3d as o3d\n",
    "import h5py\n",
    "from trimesh.points import PointCloud\n",
    "\n",
    "from common_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN STACKED SPHERES GEOMETRY ## \n",
    "\n",
    "stacked_spheres = trimesh.load_mesh('../sphere-charging/geometry/stacked_spheres_frompython_cropped.stl') \n",
    "\n",
    "## INTERPOLATE MESH TO MAKE FINNER FOR THE ANALYSIS ##\n",
    "\n",
    "# # Convert Trimesh â†’ Open3D\n",
    "# mesh_o3d = o3d.geometry.TriangleMesh(\n",
    "#     vertices=o3d.utility.Vector3dVector(stacked_spheres.vertices),\n",
    "#     triangles=o3d.utility.Vector3iVector(stacked_spheres.faces)\n",
    "# )\n",
    "\n",
    "# # Compute normals (optional but useful for remeshing)\n",
    "# mesh_o3d.compute_vertex_normals()\n",
    "\n",
    "# # Apply Loop subdivision\n",
    "# mesh_remesh = mesh_o3d.subdivide_loop(number_of_iterations=2)\n",
    "\n",
    "# # Convert Open3D â†’ Trimesh\n",
    "# vertices = np.asarray(mesh_remesh.vertices)\n",
    "# faces = np.asarray(mesh_remesh.triangles)\n",
    "# stacked_spheres = trimesh.Trimesh(vertices=vertices, faces=faces, process=False)\n",
    "\n",
    "# Visualize with Trimesh\n",
    "stacked_spheres.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Visual Representation of the Electric Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### read raw fieldmap from build folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "\n",
    "configIN = \"onlyphotoemission\"\n",
    "directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output110525/smallerworld-initial8max0.8final12/\"\n",
    "\n",
    "if iteration <10 :\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*00{iteration}*{configIN}*.txt\")) #{iteration}\n",
    "else:\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*{iteration}*{configIN}*.txt\")) #{iteration}\n",
    "print(filenames)\n",
    "\n",
    "df  = read_data_format_efficient(filenames,scaling=True)\n",
    "\n",
    "# check to make sure this matches the total nodes in outputlogs\n",
    "#len(df[iteration][\"E_mag\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS HERE ARE OPTIMIZED FOR ITERATION 86 ##\n",
    "\n",
    "fieldIN = df[iteration]\n",
    "\n",
    "N_DOWNSAMPLE_EMAG = 1\n",
    "ARROW_VOXEL_SPACING = 0.02 \n",
    "Y_SLICE = 0.0\n",
    "THICKNESS = 0.001\n",
    "VECTOR_SCALE_FACTOR = 9e-7 #2e-7 #2e-3 #5e-6 #2e-3 #5e-6 # Global scaling for glyphs\n",
    "FIELD_AVERAGE_RADIUS = 2e-3\n",
    "\n",
    "vmin, vmax = (-2e4, 2e4) # in log(E_mag) units\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) # \n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Voxel Downsampling Helper Function\n",
    "# Ensures uniform spatial distribution of points in the slice\n",
    "# ----------------------------------------------------\n",
    "def voxel_downsample_points(points, spacing):\n",
    "    \"\"\"\n",
    "    Selects one point per voxel defined by the spacing.\n",
    "    Assumes points are 3D, but only uses X and Z for 2D density control.\n",
    "    \"\"\"\n",
    "    # 1. Normalize coordinates to voxel indices (focus on X and Z for the 2D slice)\n",
    "    min_x, _, min_z = points.min(axis=0)\n",
    "    \n",
    "    # Calculate bin indices for the points\n",
    "    # We use X (column 0) and Z (column 2)\n",
    "    x_indices = np.floor((points[:, 0] - min_x) / spacing).astype(int)\n",
    "    z_indices = np.floor((points[:, 2] - min_z) / spacing).astype(int)\n",
    "    \n",
    "    # Combine X and Z indices into a unique hash/key\n",
    "    max_x_index = x_indices.max() + 1\n",
    "    voxel_keys = z_indices * max_x_index + x_indices\n",
    "\n",
    "    # 2. Find the unique keys and their first occurrence\n",
    "    # `return_index=True` gives the index of the first occurrence of each unique key\n",
    "    unique_keys, unique_indices = np.unique(voxel_keys, return_index=True)\n",
    "    \n",
    "    return unique_indices\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 0: Load, Filter, and Downsample Data (Single Pass)\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    "\n",
    "# Apply initial filtering (z > 0 and magnitude > 0)\n",
    "initial_mask = (points[:, 2] > 0) & (magnitudes > 0)\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    "\n",
    "# Aggressive Downsample (for point cloud, typically N_DOWNSAMPLE_EMAG=1 is best)\n",
    "points_ds = points[::N_DOWNSAMPLE_EMAG]\n",
    "vectors_ds = vectors[::N_DOWNSAMPLE_EMAG]\n",
    "magnitudes_ds = magnitudes[::N_DOWNSAMPLE_EMAG]\n",
    "\n",
    "# Create a PyVista Point Cloud (PolyData)\n",
    "point_cloud = pv.PolyData(points_ds)\n",
    "point_cloud[\"E_mag\"] = magnitudes_ds   # Store log magnitude for visualization\n",
    "point_cloud[\"Ex_val\"] = vectors_ds[:,0] # Store vectors\n",
    "point_cloud[\"Ez_val\"] = vectors_ds[:,2] # Store vectors\n",
    "\n",
    "print(f\"Starting points (filtered by z > 0 & mag > 0): {len(points)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup and Slicing\n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    "\n",
    "# 1a. Load and Crop Geometry\n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    "\n",
    "# Define bounding box based on the downsampled field data\n",
    "bbox_bounds = point_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = pv_spheres.clip_box(bbox, invert=False)\n",
    "\n",
    "# 1b. Define the slice plane (ZX plane, normal along Y)\n",
    "center = (point_cloud.center[0], Y_SLICE, point_cloud.center[2])\n",
    "normal = [0, 1, 0] # ZX plane (normal along Y)\n",
    "\n",
    "# Create a plane mesh for interpolation (this will be the magnitude slice)\n",
    "plane_bounds = [\n",
    "    point_cloud.bounds[0], point_cloud.bounds[1], # X bounds\n",
    "    Y_SLICE, Y_SLICE,                             # Y (fixed)\n",
    "    point_cloud.bounds[4], point_cloud.bounds[5]  # Z bounds\n",
    "]\n",
    "\n",
    "field_slice_mesh = pv.Plane(\n",
    "    center=center, \n",
    "    direction=normal,\n",
    "    j_size=bbox_bounds[1] - bbox_bounds[0], # X span\n",
    "    i_size=bbox_bounds[5] - bbox_bounds[4], # Z span\n",
    "    i_resolution=250, \n",
    "    j_resolution=250\n",
    ")\n",
    "\n",
    "# --- MODIFIED INTERPOLATION CALL FOR NEAREST NEIGHBOR ---\n",
    "field_slice_interpolated = field_slice_mesh.interpolate(\n",
    "    point_cloud,\n",
    "    sharpness=3.0,      # High sharpness often helps with point data\n",
    "    radius=0.001, #1e-12,       # Set radius to near-zero to minimize interpolation\n",
    "    \n",
    "    # 1. Provide a float placeholder to satisfy the TypeError\n",
    "    null_value=1, \n",
    "    \n",
    "    # 2. Force the strategy to use the nearest point (Nearest Neighbor)\n",
    "    strategy='closest_point' # <--- This achieves the extrapolation you want\n",
    ")\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "# Slice the geometry using the plane (more precise than slice_orthogonal)\n",
    "geo_slice = pv_spheres_cropped.slice(normal=normal, origin=field_slice_mesh.center)\n",
    "print(f\"Geometry and slicing preparation complete in {time.time() - start_time_geo:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 2: Vector Field Glyphs (Arrows)\n",
    "# ----------------------------------------------------\n",
    "start_time_vectors = time.time()\n",
    "\n",
    "# 2a. Filter the downsampled points again to extract only those in the slice volume\n",
    "# We use NumPy masking directly on the downsampled data (points_ds)\n",
    "vector_mask = np.abs(points_ds[:, 1] - Y_SLICE) < THICKNESS\n",
    "points_slice_full = points_ds[vector_mask]\n",
    "vectors_slice_full = vectors_ds[vector_mask]\n",
    "magnitudes_slice_full = magnitudes_ds[vector_mask]\n",
    "\n",
    "# 2b. Apply Voxel Downsampling to achieve uniform density\n",
    "unique_indices = voxel_downsample_points(points_slice_full, ARROW_VOXEL_SPACING)\n",
    "\n",
    "points_slice = points_slice_full[unique_indices]\n",
    "vectors_slice = vectors_slice_full[unique_indices]\n",
    "magnitudes_slice = magnitudes_slice_full[unique_indices]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# MODIFICATION: Calculate Clamping Limit and Apply Clamping\n",
    "# ----------------------------------------------------\n",
    "# The maximum allowed length of an arrow is ARROW_VOXEL_SPACING.\n",
    "# The glyph length = magnitude * VECTOR_SCALE_FACTOR * arrow_length_in_geom (which is 1.0 for pv.Arrow).\n",
    "# To ensure: glyph_length <= ARROW_VOXEL_SPACING\n",
    "# We need: magnitude * VECTOR_SCALE_FACTOR <= ARROW_VOXEL_SPACING\n",
    "# Therefore: magnitude_clamped <= ARROW_VOXEL_SPACING / VECTOR_SCALE_FACTOR\n",
    "\n",
    "# Define the maximum magnitude allowed\n",
    "MAGNITUDE_MAX_CLAMP = ARROW_VOXEL_SPACING / VECTOR_SCALE_FACTOR /2\n",
    "\n",
    "# Apply the clamping (upper bound) to the magnitude array\n",
    "magnitudes_slice_clamped = np.clip(magnitudes_slice, a_min=None, a_max=MAGNITUDE_MAX_CLAMP)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# 2c. Create a PolyData object for glyphs\n",
    "points_slice[:,1] = Y_SLICE - 2*THICKNESS# Force y-coordinate to the slice plane for visualization\n",
    "vectors_slice[:,1] = 0.0 - 2* THICKNESS# Zero out Y component for 2D slice visualization\n",
    "slice_mesh_vectors = pv.PolyData(points_slice)\n",
    "slice_mesh_vectors['vectors'] = vectors_slice\n",
    "# Use the CLAMPED magnitude array for scaling\n",
    "slice_mesh_vectors['magnitude'] = magnitudes_slice_clamped\n",
    "#slice_mesh_vectors['magnitude'] = np.log10(magnitudes_slice)\n",
    "\n",
    "# # 2c. Create a PolyData object for glyphs\n",
    "# points_slice[:,1] = Y_SLICE - 2*THICKNESS# Force y-coordinate to the slice plane for visualization\n",
    "# vectors_slice[:,1] = 0.0 - 2* THICKNESS# Zero out Y component for 2D slice visualization\n",
    "# slice_mesh_vectors = pv.PolyData(points_slice)\n",
    "# slice_mesh_vectors['vectors'] = vectors_slice\n",
    "# #slice_mesh_vectors['magnitude'] = np.log10(magnitudes_slice)\n",
    "# slice_mesh_vectors['magnitude'] = magnitudes_slice\n",
    "\n",
    "print(f\"Points in vector slice (after density control): {len(points_slice)}, old length: {len(points_slice_full)}...\")\n",
    "\n",
    "# 2d. Create the glyphs\n",
    "arrow = pv.Arrow(tip_length=0.3, tip_radius=0.2, shaft_radius=0.04)\n",
    "glyphs = slice_mesh_vectors.glyph(\n",
    "    orient='vectors',\n",
    "    scale='magnitude',\n",
    "    factor=VECTOR_SCALE_FACTOR,\n",
    "    geom=arrow\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 3: Visualization\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"Ex_val\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    opacity=1,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # <-- set fixed color range here\n",
    "    # --- COLORBAR POSITIONING FIX ---\n",
    "    scalar_bar_args={\n",
    "        'title':None, # r'log$_{10}$(E$_{mag}$)', # Updated title format\n",
    "        'vertical': False,            # Make it horizontal\n",
    "        'position_x': 0.20,           # User-specified start position\n",
    "        'position_y': 0.12,           # User-specified vertical position\n",
    "        'width': 0.6,                 # User-specified width\n",
    "        'height': 0.05,               # User-specified height\n",
    "    }\n",
    "    # -------------------------------\n",
    ")\n",
    "\n",
    "# Add sliced geometry (outline only)\n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=5,opacity=0.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='black', show_scalar_bar=False, line_width=4,opacity=1)\n",
    "\n",
    "# # Optional marker\n",
    "# sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "# pl.add_mesh(sphere, color=\"red\", opacity=0.5)\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "\n",
    "# Align camera perpendicular to the slice\n",
    "pl.view_xz() \n",
    "\n",
    "# --- ADD THIS LINE BEFORE pl.show() ---\n",
    "pl.screenshot(f'figures/fieldvectors_iteration{iteration}.jpeg', scale=4)\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static') #jupyter_backend='static'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Enable LaTeX rendering\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Set the global font size\n",
    "mpl.rcParams.update({'font.size': 12})\n",
    "\n",
    "cmap = plt.cm.YlGnBu \n",
    "vmin, vmax = (-2e5, 2e5) # in log(E_mag) units\n",
    "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 0.1))\n",
    "\n",
    "# --- 1. Create and Configure the ScalarFormatter ---\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "\n",
    "formatter.set_useOffset(False) \n",
    "formatter.set_powerlimits((0, 0)) \n",
    "\n",
    "# --- 2. Create the Colorbar and apply the Formatter ---\n",
    "cb = mpl.colorbar.ColorbarBase(\n",
    "    ax, \n",
    "    cmap=cmap, \n",
    "    norm=norm, \n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Apply the formatter to the colorbar's x-axis\n",
    "cb.ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# --- 3. Display the Plot ---\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS HERE ARE OPTIMIZED FOR ITERATION 1 ##\n",
    "\n",
    "N_DOWNSAMPLE_EMAG = 1\n",
    "N_DOWNSAMPLE_BASE = 500 # This is the initial downsample applied to the full field before slicing\n",
    "\n",
    "vmin, vmax = (-2e5, 2e5) #(-2e4, 2e4) \n",
    "VECTOR_SCALE_FACTOR = 1e-7 #2e-6 #5e-6 # Global scaling for glyphs\n",
    "\n",
    "Y_SLICE = 0.0\n",
    "THICKNESS = 0.002 # Original, thin slice thickness\n",
    "EXCLUSION_DISTANCE = 0.011\n",
    "\n",
    "FIELD_AVERAGE_RADIUS = 2e-3\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015+ 0.039]) #  \n",
    "LOG_MAG_NULL_VALUE = 1.0 \n",
    "\n",
    "fieldIN = df[iteration]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 0: Load, Filter, and Downsample Data (Single Pass)\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    "\n",
    "# Apply initial filtering (z > 0 and magnitude > 0)\n",
    "initial_mask = (points[:, 2] > 0) & (magnitudes > 0)\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    "\n",
    "if N_DOWNSAMPLE_EMAG > 1:\n",
    "    print(f\"Applying downsample of {N_DOWNSAMPLE_EMAG} to E_mag data.\")\n",
    "    #  Downsample (Base Downsample)\n",
    "    points = points[::N_DOWNSAMPLE_EMAG]\n",
    "    vectors = vectors[::N_DOWNSAMPLE_EMAG]\n",
    "    magnitudes = magnitudes[::N_DOWNSAMPLE_EMAG]\n",
    "\n",
    "# Create a PyVista Point Cloud (PolyData)\n",
    "point_cloud = pv.PolyData(points)\n",
    "point_cloud[\"E_mag\"] = np.log10(magnitudes) # Store log magnitude for visualization\n",
    "point_cloud[\"E_vec\"] = vectors[:,0]             # Store vectors (E_x component)\n",
    "\n",
    "print(f\"Starting points (filtered by z > 0 & mag > 0): {len(points)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup and Slicing \n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    "\n",
    "# 1a. Load and Crop Geometry \n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    "bbox_bounds = point_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = pv_spheres.clip_box(bbox, invert=False)\n",
    "\n",
    "# 1b. Define the slice plane\n",
    "center = (point_cloud.center[0], Y_SLICE, point_cloud.center[2])\n",
    "normal = [0, 1, 0] # ZX plane (normal along Y)\n",
    "\n",
    "plane_bounds = [\n",
    "    point_cloud.bounds[0], point_cloud.bounds[1], # X bounds\n",
    "    Y_SLICE, Y_SLICE,                             # Y (fixed)\n",
    "    point_cloud.bounds[4], point_cloud.bounds[5]  # Z bounds\n",
    "]\n",
    "\n",
    "field_slice_mesh = pv.Plane(\n",
    "    center=center, \n",
    "    direction=normal,\n",
    "    j_size=bbox_bounds[1] - bbox_bounds[0], \n",
    "    i_size=bbox_bounds[5] - bbox_bounds[4], \n",
    "    i_resolution=250, \n",
    "    j_resolution=250\n",
    ")\n",
    "\n",
    "# --- MODIFIED INTERPOLATION CALL FOR NEAREST NEIGHBOR ---\n",
    "field_slice_interpolated = field_slice_mesh.interpolate(\n",
    "    point_cloud,\n",
    "    sharpness=3.0,      # High sharpness often helps with point data\n",
    "    radius=0.002, #1e-12,       # Set radius to near-zero to minimize interpolation\n",
    "    \n",
    "    # 1. Provide a float placeholder to satisfy the TypeError\n",
    "    null_value=1, \n",
    "    \n",
    "    # 2. Force the strategy to use the nearest point (Nearest Neighbor)\n",
    "    strategy='closest_point' # <--- This achieves the extrapolation you want\n",
    ")\n",
    "\n",
    "geo_slice = pv_spheres_cropped.slice(normal=normal, origin=field_slice_mesh.center)\n",
    "print(f\"Geometry and slicing preparation complete in {time.time() - start_time_geo:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## Step 2: Filter Data Based on Distance to geo_slice\n",
    "# ----------------------------------------------------\n",
    "start_time_filter = time.time()\n",
    "\n",
    "# --- 2a. Thin Slice Filtering ---\n",
    "vector_mask = np.abs(points[:, 1] - Y_SLICE) < THICKNESS\n",
    "points_slice = points[vector_mask]\n",
    "vectors_slice = vectors[vector_mask]\n",
    "magnitudes_slice = magnitudes[vector_mask]\n",
    "\n",
    "# Aggressive Downsample (Base Downsample)\n",
    "points_slice_ds = points_slice[::N_DOWNSAMPLE_BASE]\n",
    "vectors_slice_ds = vectors_slice[::N_DOWNSAMPLE_BASE]\n",
    "magnitudes_slice_ds = magnitudes_slice[::N_DOWNSAMPLE_BASE]\n",
    "\n",
    "# 2. Find the distance to the nearest neighbor in geo_slice for every point in points_ds\n",
    "# We use query with k=1 (nearest neighbor)\n",
    "tree = cKDTree(geo_slice.points)\n",
    "distance_to_geometry, _ = tree.query(points_slice_ds, k=1)\n",
    "\n",
    "# 3. Create a mask: keep only points whose distance is greater than the threshold\n",
    "keep_mask = distance_to_geometry > EXCLUSION_DISTANCE\n",
    "\n",
    "# 4. Apply the mask to the base data arrays\n",
    "points_slice_ds = points_slice_ds[keep_mask]\n",
    "vectors_slice_ds = vectors_slice_ds[keep_mask]\n",
    "magnitudes_slice_ds = magnitudes_slice_ds[keep_mask]          \n",
    "\n",
    "# 2b. Create PolyData for thin glyphs\n",
    "#points_slice_ds[:,1] = Y_SLICE  -THICKNESS*2# Force y-coordinate to the slice plane for visualization\n",
    "slice_mesh_vectors_noRim = pv.PolyData(points_slice_ds)\n",
    "slice_mesh_vectors_noRim['vectors'] = vectors_slice_ds\n",
    "slice_mesh_vectors_noRim['magnitude'] = magnitudes_slice_ds\n",
    "\n",
    "# 2c. Create the THIN glyphs\n",
    "arrow = pv.Arrow(tip_length=0.2, tip_radius=0.08, shaft_radius=0.02)\n",
    "glyphs = slice_mesh_vectors_noRim.glyph(\n",
    "    orient='vectors',\n",
    "    scale='magnitude',\n",
    "    factor=VECTOR_SCALE_FACTOR,\n",
    "    geom=arrow\n",
    ")\n",
    "\n",
    "print(f\"Distance filtering removed {len(keep_mask) - len(points_slice_ds)} points. New length: {len(points_slice_ds)}\")\n",
    "print(f\"Vector field preparation complete in {time.time() - start_time_filter:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## Step 3: Visualization ðŸ“Š\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"E_vec\",\n",
    "    cmap=\"viridis\",\n",
    "    opacity=0.9,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # Use the defined log range\n",
    "    scalar_bar_args={\n",
    "        # Title updated to reflect the log-magnitude plot, matching the data\n",
    "        'title': None, #r'log$_{10}$(E$_{mag}$)', \n",
    "        'vertical': False,            \n",
    "        'position_x': 0.22,           \n",
    "        'position_y': 0.16,           \n",
    "        'width': 0.6,                 \n",
    "        'height': 0.05,               \n",
    "    }\n",
    ")\n",
    "\n",
    "# Add sliced geometry \n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=2.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='white', show_scalar_bar=False)\n",
    "#pl.add_mesh(glyphs, show_scalar_bar=False, cmap='coolwarm') # color='white', show_scalar_bar=False)\n",
    "\n",
    "# Optional marker \n",
    "#sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "#pl.add_mesh(sphere, color=\"red\", opacity=0.5)\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "pl.view_xz()\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS HERE ARE OPTIMIZED FOR ITERATION 86 ##\n",
    "\n",
    "N_DOWNSAMPLE_EMAG = 1\n",
    "N_DOWNSAMPLE_BASE = 500 # This is the initial downsample applied to the full field before slicing\n",
    "\n",
    "vmin, vmax = (-2e5, 2e5) \n",
    "VECTOR_SCALE_FACTOR = 1e-7 # Global scaling for glyphs\n",
    "\n",
    "Y_SLICE = 0.0\n",
    "THICKNESS = 0.002 # Original, thin slice thickness\n",
    "EXCLUSION_DISTANCE = 0.011\n",
    "\n",
    "FIELD_AVERAGE_RADIUS = 2e-3\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) # \n",
    "LOG_MAG_NULL_VALUE = 1.0 \n",
    "\n",
    "fieldIN = df[iteration]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 0: Load, Filter, and Downsample Data (Single Pass)\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    "\n",
    "# Apply initial filtering (z > 0 and magnitude > 0)\n",
    "initial_mask = (points[:, 2] > 0) & (magnitudes > 0)\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    "\n",
    "if N_DOWNSAMPLE_EMAG > 1:\n",
    "    print(f\"Applying downsample of {N_DOWNSAMPLE_EMAG} to E_mag data.\")\n",
    "    #  Downsample (Base Downsample)\n",
    "    points = points[::N_DOWNSAMPLE_EMAG]\n",
    "    vectors = vectors[::N_DOWNSAMPLE_EMAG]\n",
    "    magnitudes = magnitudes[::N_DOWNSAMPLE_EMAG]\n",
    "\n",
    "# Create a PyVista Point Cloud (PolyData)\n",
    "point_cloud = pv.PolyData(points)\n",
    "point_cloud[\"E_mag\"] = np.log10(magnitudes) # Store log magnitude for visualization\n",
    "point_cloud[\"E_vec\"] = vectors[:,0]             # Store vectors (E_x component)\n",
    "\n",
    "print(f\"Starting points (filtered by z > 0 & mag > 0): {len(points)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup and Slicing \n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    "\n",
    "# 1a. Load and Crop Geometry \n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    "bbox_bounds = point_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = pv_spheres.clip_box(bbox, invert=False)\n",
    "\n",
    "# 1b. Define the slice plane\n",
    "center = (point_cloud.center[0], Y_SLICE, point_cloud.center[2])\n",
    "normal = [0, 1, 0] # ZX plane (normal along Y)\n",
    "\n",
    "plane_bounds = [\n",
    "    point_cloud.bounds[0], point_cloud.bounds[1], # X bounds\n",
    "    Y_SLICE, Y_SLICE,                             # Y (fixed)\n",
    "    point_cloud.bounds[4], point_cloud.bounds[5]  # Z bounds\n",
    "]\n",
    "\n",
    "field_slice_mesh = pv.Plane(\n",
    "    center=center, \n",
    "    direction=normal,\n",
    "    j_size=bbox_bounds[1] - bbox_bounds[0], \n",
    "    i_size=bbox_bounds[5] - bbox_bounds[4], \n",
    "    i_resolution=250, \n",
    "    j_resolution=250\n",
    ")\n",
    "\n",
    "# --- MODIFIED INTERPOLATION CALL FOR NEAREST NEIGHBOR ---\n",
    "field_slice_interpolated = field_slice_mesh.interpolate(\n",
    "    point_cloud,\n",
    "    sharpness=3.0,      # High sharpness often helps with point data\n",
    "    radius=0.002, #1e-12,       # Set radius to near-zero to minimize interpolation\n",
    "    \n",
    "    # 1. Provide a float placeholder to satisfy the TypeError\n",
    "    null_value=1, \n",
    "    \n",
    "    # 2. Force the strategy to use the nearest point (Nearest Neighbor)\n",
    "    strategy='closest_point' # <--- This achieves the extrapolation you want\n",
    ")\n",
    "\n",
    "geo_slice = pv_spheres_cropped.slice(normal=normal, origin=field_slice_mesh.center)\n",
    "print(f\"Geometry and slicing preparation complete in {time.time() - start_time_geo:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## Step 2: Filter Data Based on Distance to geo_slice\n",
    "# ----------------------------------------------------\n",
    "start_time_filter = time.time()\n",
    "\n",
    "# --- 2a. Thin Slice Filtering ---\n",
    "vector_mask = np.abs(points[:, 1] - Y_SLICE) < THICKNESS\n",
    "points_slice = points[vector_mask]\n",
    "vectors_slice = vectors[vector_mask]\n",
    "magnitudes_slice = magnitudes[vector_mask]\n",
    "\n",
    "# Aggressive Downsample (Base Downsample)\n",
    "points_slice_ds = points_slice[::N_DOWNSAMPLE_BASE]\n",
    "vectors_slice_ds = vectors_slice[::N_DOWNSAMPLE_BASE]\n",
    "magnitudes_slice_ds = magnitudes_slice[::N_DOWNSAMPLE_BASE]\n",
    "\n",
    "# 2. Find the distance to the nearest neighbor in geo_slice for every point in points_ds\n",
    "# We use query with k=1 (nearest neighbor)\n",
    "tree = cKDTree(geo_slice.points)\n",
    "distance_to_geometry, _ = tree.query(points_slice_ds, k=1)\n",
    "\n",
    "# 3. Create a mask: keep only points whose distance is greater than the threshold\n",
    "keep_mask = distance_to_geometry > EXCLUSION_DISTANCE\n",
    "\n",
    "# 4. Apply the mask to the base data arrays\n",
    "points_slice_ds = points_slice_ds[keep_mask]\n",
    "vectors_slice_ds = vectors_slice_ds[keep_mask]\n",
    "magnitudes_slice_ds = magnitudes_slice_ds[keep_mask]          \n",
    "\n",
    "# 2b. Create PolyData for thin glyphs\n",
    "points_slice_ds[:,1] = Y_SLICE  # Force y-coordinate to the slice plane for visualization\n",
    "slice_mesh_vectors_noRim = pv.PolyData(points_slice_ds)\n",
    "slice_mesh_vectors_noRim['vectors'] = vectors_slice_ds\n",
    "slice_mesh_vectors_noRim['magnitude'] = magnitudes_slice_ds\n",
    "\n",
    "# 2c. Create the THIN glyphs\n",
    "arrow = pv.Arrow(tip_length=0.2, tip_radius=0.08, shaft_radius=0.01)\n",
    "glyphs = slice_mesh_vectors_noRim.glyph(\n",
    "    orient='vectors',\n",
    "    scale='magnitude',\n",
    "    factor=VECTOR_SCALE_FACTOR,\n",
    "    geom=arrow\n",
    ")\n",
    "\n",
    "print(f\"Distance filtering removed {len(keep_mask) - len(points_slice_ds)} points. New length: {len(points_slice_ds)}\")\n",
    "print(f\"Vector field preparation complete in {time.time() - start_time_filter:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## Step 3: Visualization ðŸ“Š\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"E_vec\",\n",
    "    cmap=\"viridis\",\n",
    "    opacity=0.9,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # Use the defined log range\n",
    "    scalar_bar_args={\n",
    "        # Title updated to reflect the log-magnitude plot, matching the data\n",
    "        'title': None, #r'log$_{10}$(E$_{mag}$)', \n",
    "        'vertical': False,            \n",
    "        'position_x': 0.22,           \n",
    "        'position_y': 0.16,           \n",
    "        'width': 0.6,                 \n",
    "        'height': 0.05,               \n",
    "    }\n",
    ")\n",
    "\n",
    "# Add sliced geometry \n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=2.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='white', show_scalar_bar=False)\n",
    "#pl.add_mesh(glyphs, show_scalar_bar=False, cmap='coolwarm') # color='white', show_scalar_bar=False)\n",
    "\n",
    "# Optional marker \n",
    "sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "pl.add_mesh(sphere, color=\"red\", opacity=0.5)\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "pl.view_xz()\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static') #jupyter_backend='static'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def filter_coordinates(\n",
    "    df: pd.DataFrame, \n",
    "    column_name: str, \n",
    "    x_range: Tuple[float, float], \n",
    "    y_range: Tuple[float, float], \n",
    "    z_range: Tuple[float, float]\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Filters a DataFrame Series containing coordinate strings based on specified \n",
    "    (x, y, z) ranges.\n",
    "\n",
    "    The coordinates in the column_name are expected to be formatted as strings \n",
    "    like \"(x, y, z)\".\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column_name (str): The name of the column containing the coordinate strings.\n",
    "        x_range (Tuple[float, float]): (min, max) for the x-coordinate filter.\n",
    "        y_range (Tuple[float, float]): (min, max) for the y-coordinate filter.\n",
    "        z_range (Tuple[float, float]): (min, max) for the z-coordinate filter.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A pandas Series containing only the coordinate strings that\n",
    "                   fall within all three specified ranges.\n",
    "    \"\"\"\n",
    "    \n",
    "    coords = np.vstack( df[column_name])\n",
    "    \n",
    "    # X-Filter: x is greater than x_min AND x is less than x_max\n",
    "    x_min, x_max = x_range\n",
    "    x_filter = (coords[:,0] > x_min) & (coords[:,0]  < x_max)\n",
    "\n",
    "    # Y-Filter: y is greater than y_min AND y is less than y_max\n",
    "    y_min, y_max = y_range\n",
    "    y_filter = (coords[:,1] > y_min) & (coords[:,1]  < y_max)\n",
    "\n",
    "    # Z-Filter: z is greater than z_min AND z is less than z_max\n",
    "    z_min, z_max = z_range\n",
    "    z_filter = (coords[:,2] > z_min) & (coords[:,2]  < z_max)\n",
    "\n",
    "    # --- 3. Combine the filters and apply to the original series ---\n",
    "    combined_filter = x_filter & y_filter & z_filter\n",
    "\n",
    "    # Return the original coordinate strings that match the combined filter\n",
    "    return df[column_name][combined_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(all_gamma_holes_df[\"Post_Step_Position_mm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# Define the filtering ranges based on your request\n",
    "X_RANGE = (plane_bounds[0],plane_bounds[1])\n",
    "Y_RANGE = (-THICKNESS/2, THICKNESS/2)\n",
    "Z_RANGE = (plane_bounds[4],plane_bounds[5])\n",
    "\n",
    "# Call the function with the sample data and required ranges\n",
    "filtered_series_gamma = filter_coordinates(df=all_gamma_holes_df,column_name=\"Post_Step_Position_mm\",\n",
    "    x_range=X_RANGE,y_range=Y_RANGE,z_range=Z_RANGE)\n",
    "print(f\"\\nTotal points found: {len(filtered_series_gamma)}, starting length: {len(all_gamma_holes_df)}\")\n",
    "\n",
    "# Call the function with the sample data and required ranges\n",
    "filtered_series_electron = filter_coordinates(df=all_electrons_inside_df,column_name=\"Post_Step_Position_mm\",\n",
    "    x_range=X_RANGE,y_range=Y_RANGE,z_range=Z_RANGE)\n",
    "print(f\"\\nTotal points found: {len(filtered_series_electron)}, starting length: {len(all_electrons_inside_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "## Step 3: Visualization ðŸ“Š\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"E_vec\",\n",
    "    cmap=\"viridis\",\n",
    "    opacity=0.9,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # Use the defined log range\n",
    "    scalar_bar_args={\n",
    "        # Title updated to reflect the log-magnitude plot, matching the data\n",
    "        'title': None, #r'log$_{10}$(E$_{mag}$)', \n",
    "        'vertical': False,            \n",
    "        'position_x': 0.22,           \n",
    "        'position_y': 0.16,           \n",
    "        'width': 0.6,                 \n",
    "        'height': 0.05,               \n",
    "    }\n",
    ")\n",
    "\n",
    "# Add sliced geometry \n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=2.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='white', show_scalar_bar=False)\n",
    "#pl.add_mesh(glyphs, show_scalar_bar=False, cmap='coolwarm') # color='white', show_scalar_bar=False)\n",
    "\n",
    "# Optional marker \n",
    "sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "pl.add_mesh(sphere, color=\"red\", opacity=0.5)\n",
    "\n",
    "\n",
    "# # Add the single PolyData mesh\n",
    "# pl.add_mesh(\n",
    "#     pv.PolyData(np.vstack(filtered_series_gamma)),\n",
    "#     color='red',\n",
    "#     opacity=0.9,\n",
    "#     # Key arguments for fast point visualization:\n",
    "#     render_points_as_spheres=True,  # Makes them look like spheres\n",
    "#     point_size=3,                  # Controls the size of the 'spheres'\n",
    "#     # The 'point_size' units are in screen pixels by default\n",
    "# )\n",
    "\n",
    "# Add the single PolyData mesh\n",
    "pl.add_mesh(\n",
    "    pv.PolyData(np.vstack(filtered_series_electron)),\n",
    "    color='red',\n",
    "    opacity=0.9,\n",
    "    # Key arguments for fast point visualization:\n",
    "    render_points_as_spheres=True,  # Makes them look like spheres\n",
    "    point_size=3,                  # Controls the size of the 'spheres'\n",
    "    # The 'point_size' units are in screen pixels by default\n",
    ")\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "pl.view_xz()\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static') #jupyter_backend='static'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "directory_path =  \"../build-smallerworld-initial8max0.8final12/root/\" #\"../build-adaptive-barns-fixed/root/\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*.root\"))\n",
    "\n",
    "all_gamma_holes = []\n",
    "all_electrons_inside = []\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    if iterationNUM > 5:\n",
    "        break\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"gamma_holes_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)], _ = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path),\n",
    "                                                                                                             config=configIN)\n",
    "    \n",
    "    # surf, ilm_values = plot_face_illumination(vars()[\"gamma_holes_\"+str(number_str)], stacked_spheres, vmin=0, vmax=1)\n",
    "    # # Make sure each triangle has its own unique vertices\n",
    "    # surface_edited = surface.copy()\n",
    "    # surface_edited.unmerge_vertices()\n",
    "    # surface_edited.visual.vertex_colors = None\n",
    "    # surface_edited.show()\n",
    "\n",
    "    all_gamma_holes.append(vars()[\"gamma_holes_\"+str(number_str)])\n",
    "    all_electrons_inside.append(vars()[\"electrons_inside_\"+str(number_str)])\n",
    "\n",
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gamma_holes_df[\"Post_Step_Position_mm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the field to get a sense for the overall field values before analyzing the field in detail\n",
    "for iteration, thresholdIN in zip(df.keys(), [93.6167, 101.242, 220.685]):\n",
    "    length = len(df[iteration][\"E_mag\"])\n",
    "    plt.hist(df[iteration][\"E_mag\"][df[iteration][\"E_mag\"]>0],bins=np.logspace(0,8,100),alpha=0.5,label=f\"{iteration}: total leaves ={length}\")\n",
    "    plt.axvline(x=thresholdIN*1e3, linestyle=\":\")\n",
    "#plt.hist(df2[df2[\"E_mag\"]>0][\"E_mag\"],bins=np.logspace(-10,8,100),alpha=0.2,label=\"Iteration 78\")\n",
    "#plt.axvline(x=3e2,color=\"k\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"|E| (V/m)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Temperature: 425 K\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Scene Initialization and Data Loading ---\n",
    "\n",
    "# Retrieve the field data dictionary for the current iteration (assumes 'df' is a list/dict)\n",
    "fieldIN = df[iteration]\n",
    "# Define the threshold for filtering electric field magnitude (E_mag)\n",
    "threshold = 1e3 \n",
    "# Set the maximum number of arrows to plot to maintain performance\n",
    "max_arrows = 5000\n",
    "\n",
    "# Initialize the 3D scene by plotting the geometry (e.g., detector structure)\n",
    "# 'stacked_spheres' is the geometry to plot.\n",
    "# 'edge_color' is set to black with an alpha (transparency) value of 350 (out of 511 max for trimesh visual.face_colors/edge_colors)\n",
    "scene = plot_trimesh_edges_only(stacked_spheres, edge_color=[0, 0, 0, 350]) \n",
    "\n",
    "## ----------------------------------------------------\n",
    "## --- 2. Filtering and Sampling High-Magnitude Points ---\n",
    "## ----------------------------------------------------\n",
    "\n",
    "# --- Filtering ---\n",
    "# Create a boolean mask for all points where E_mag exceeds the threshold\n",
    "large_magnitude_mask = fieldIN['E_mag'] > threshold\n",
    "\n",
    "# Apply the mask to all three arrays ('pos', 'E', 'E_mag') simultaneously\n",
    "# to create a new dictionary containing only the high-field points.\n",
    "field_largevalues_masked = {\n",
    "    'pos': fieldIN['pos'][large_magnitude_mask],\n",
    "    'E': fieldIN['E'][large_magnitude_mask],\n",
    "    'E_mag': fieldIN['E_mag'][large_magnitude_mask]\n",
    "}\n",
    "\n",
    "# Get the count of data points after initial filtering\n",
    "N_large = len(field_largevalues_masked['E_mag'])\n",
    "\n",
    "# --- Sampling ---\n",
    "if N_large > max_arrows:\n",
    "    \n",
    "    # Randomly select a subset of indices to stay below the 'max_arrows' limit.\n",
    "    # np.random.choice is the NumPy equivalent of a DataFrame's .sample() method.\n",
    "    np.random.seed(42) # Set seed for reproducible sampling\n",
    "    sample_indices = np.random.choice(\n",
    "        N_large,         # Range of indices to choose from (0 to N_large - 1)\n",
    "        size=max_arrows, # The number of indices to select\n",
    "        replace=False    # Ensure each index is chosen only once\n",
    "    )\n",
    "    \n",
    "    # Apply the sample indices to all arrays to create the final data dictionary for plotting\n",
    "    field_plot = {\n",
    "        'pos': field_largevalues_masked['pos'][sample_indices],\n",
    "        'E': field_largevalues_masked['E'][sample_indices],\n",
    "        'E_mag': field_largevalues_masked['E_mag'][sample_indices]\n",
    "    }\n",
    "    \n",
    "    print(f\"Sampled down to {max_arrows} points from {N_large} large-magnitude points.\")\n",
    "\n",
    "else:\n",
    "    # If the filtered data is small enough, use it directly without sampling\n",
    "    field_plot = field_largevalues_masked\n",
    "    print(f\"Used all {N_large} large-magnitude points for plotting.\")\n",
    "\n",
    "\n",
    "## ----------------------------------------------------\n",
    "## --- 3. Normalization and Coloring Setup ---\n",
    "## ----------------------------------------------------\n",
    "\n",
    "# Get the vector field directions and magnitudes for the sampled points\n",
    "directions = field_plot[\"E\"]\n",
    "\n",
    "# Calculate unit vectors (normalized directions)\n",
    "# Use np.where to prevent division by zero if E_mag is exactly zero (though it shouldn't be after filtering)\n",
    "directions_unit = np.where(\n",
    "    field_plot[\"E_mag\"][:, None] > 0, \n",
    "    directions / field_plot[\"E_mag\"][:, None], \n",
    "    0\n",
    ")\n",
    "\n",
    "# Use Logarithmic scaling for magnitude visualization\n",
    "# Add a small epsilon (1e-12) before log to avoid log(0), which results in -inf\n",
    "log_magnitudes = np.log10(fieldIN['E_mag'][fieldIN['E_mag'] > 1e4]) \n",
    "\n",
    "# Normalize log-magnitudes to the range [0, 1] for colormapping\n",
    "# np.ptp (peak-to-peak) is range (max - min)\n",
    "min_log = log_magnitudes.min()\n",
    "ptp_log = np.ptp(log_magnitudes)\n",
    "\n",
    "# The normalization uses the min/ptp of the *sampled* data, not the full dataset\n",
    "# A small epsilon is added to the divisor to prevent division by zero in case ptp is 0\n",
    "log_magnitudes = np.log10(field_plot['E_mag']) \n",
    "norm_magnitudes = (log_magnitudes - min_log) / (ptp_log + 1e-12)\n",
    "\n",
    "print(f\"Log(E_mag) Min: {min_log}, Range (PtP): {ptp_log}\")\n",
    "\n",
    "# Choose a Colormap (e.g., 'jet')\n",
    "cmap = plt.cm.jet\n",
    "# Map the normalized magnitudes (0 to 1) to colors (RGBA floats 0.0 to 1.0)\n",
    "colors_rgba = cmap(norm_magnitudes)\n",
    "\n",
    "# Convert the RGBA colors from floats (0.0-1.0) to 8-bit integers (0-255) for trimesh\n",
    "colors_rgba = (colors_rgba * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "## ----------------------------------------------------\n",
    "## --- 4. Arrow Creation and Visualization ---\n",
    "## ----------------------------------------------------\n",
    "\n",
    "# Define base dimensions for the visualization\n",
    "base_arrow_length = 0.05  # Base length before scaling by magnitude\n",
    "arrow_radius = 0.001       # Radius of the arrow shaft\n",
    "cone_ratio = 0.2           # Ratio of the cone length to the total arrow length\n",
    "\n",
    "# Scale the base length by the normalized magnitude for visual encoding\n",
    "scaling_factor = 0.5  # Overall factor to control arrow visibility\n",
    "scaled_lengths = base_arrow_length * scaling_factor * norm_magnitudes\n",
    "\n",
    "# Iterate over each sampled point to create and place an arrow\n",
    "for pos, dir_vec, color, magnitude_norm, scaled_length in zip(\n",
    "    field_plot[\"pos\"], \n",
    "    directions_unit, \n",
    "    colors_rgba, \n",
    "    norm_magnitudes,\n",
    "    scaled_lengths\n",
    "):\n",
    "    # Skip if the direction vector has zero magnitude (E_mag was not filtered perfectly or is near zero)\n",
    "    if np.linalg.norm(dir_vec) == 0:\n",
    "        continue\n",
    "\n",
    "    # Calculate arrow dimensions\n",
    "    arrow_length = scaled_length\n",
    "    cone_length = arrow_length * cone_ratio\n",
    "    shaft_length = arrow_length - cone_length\n",
    "\n",
    "    # Create Arrow Geometry in trimesh (built along the Z-axis by default)\n",
    "\n",
    "    # 1. Create shaft (cylinder)\n",
    "    shaft = trimesh.creation.cylinder(radius=arrow_radius, height=shaft_length, sections=12)\n",
    "    # Move the shaft so its base is at z=0\n",
    "    shaft.apply_translation([0, 0, shaft_length / 2]) \n",
    "\n",
    "    # 2. Create cone (cone)\n",
    "    cone = trimesh.creation.cone(radius=arrow_radius * 2, height=cone_length, sections=12)\n",
    "    # Position the cone base to touch the top of the shaft\n",
    "    cone.apply_translation([0, 0, shaft_length])\n",
    "\n",
    "    # 3. Combine parts into a single trimesh object\n",
    "    arrow = trimesh.util.concatenate([shaft, cone])\n",
    "\n",
    "    # Set the computed color (scaled by magnitude) for all faces of the arrow\n",
    "    arrow.visual.face_colors = np.tile(color, (arrow.faces.shape[0], 1))\n",
    "\n",
    "    # Calculate the necessary transformation to place and orient the arrow\n",
    "\n",
    "    # a. Compute rotation matrix to align the default Z-axis ([0, 0, 1]) to the direction vector (dir_vec)\n",
    "    transform = trimesh.geometry.align_vectors([0, 0, 1], dir_vec)\n",
    "    \n",
    "    # b. Set the translation part of the transformation matrix (the arrow's position)\n",
    "    transform[:3, 3] = pos\n",
    "    \n",
    "    # c. Apply the full rotation and translation\n",
    "    arrow.apply_transform(transform)\n",
    "\n",
    "    # Add the colored and positioned arrow to the visualization scene\n",
    "    scene.add_geometry(arrow)\n",
    "\n",
    "# # # Target point\n",
    "# #location = np.array([[-0.1, 0, 0.1 - 0.015 + 0.037]])\n",
    "# location = np.array([[0, -0.1, 0.1]]) \n",
    "# #location = np.array([[0, 0, 0.2- 0.015]])   \n",
    " \n",
    "# radius_mm = 10/1000 # units: um\n",
    "# positions = df[iteration][\"pos\"]\n",
    "# efield = df[1][\"E\"]\n",
    "# emag = df[1][\"E_mag\"]\n",
    "\n",
    "# # Vectorized distance computation (distance between positions and target)\n",
    "# distances = np.linalg.norm(positions - location, axis=1)\n",
    "\n",
    "# # 1. Get indices of all points within the specified radius\n",
    "# # The result is a boolean array\n",
    "# within_radius_mask = distances <= radius_mm\n",
    "\n",
    "# # Get the actual indices\n",
    "# indices_within_radius = np.where(within_radius_mask)[0]\n",
    "\n",
    "# # Count the points found\n",
    "# count = len(indices_within_radius)\n",
    "# position_points = positions[indices_within_radius]\n",
    "# #E_points = efield[indices_within_radius]\n",
    "# #E_mag_points = emag[indices_within_radius]\n",
    "# emag_values = emag[indices_within_radius]\n",
    "\n",
    "# print(count,np.mean(emag_values[emag_values>0]))\n",
    "\n",
    "# # Create small spheres at each point\n",
    "# spheres = []\n",
    "# for point in position_points:\n",
    "#     sphere = trimesh.creation.icosphere(radius=0.001)  # adjust radius for point size\n",
    "#     sphere.apply_translation(point)\n",
    "#     sphere.visual.face_colors = [255, 0, 0, 255]  # red spheres\n",
    "#     spheres.append(sphere)\n",
    "\n",
    "# # Combine all meshes into a scene\n",
    "# scene.add_geometry(spheres)\n",
    "\n",
    "# Display the final scene containing the geometry and the vector field arrows\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### read in processed fieldmaps:\n",
    "\n",
    "Processed fieldmaps have a set spherical radius around the point of interest (goal: replicate Fig. 7 from Zimmerman et al., 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output110525/processed-fieldmaps\"\n",
    "processed_data = load_h5_to_dict(f\"processed-fieldmaps/PE_425K_initial8max0.8final12_sphere50um.h5\")\n",
    "# Retrieve the field data dictionary for the current iteration (assumes 'df' is a list/dict)\n",
    "\n",
    "# takes ~2 minutes to read in 12 GB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Target point\n",
    "target_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037])\n",
    "#target_point = np.array([-0.1-0.007, 0, 0.1 - 0.015 + 0.037])\n",
    "\n",
    "Evector_atpoint_iterations = []\n",
    "\n",
    "for keyIN in processed_data.keys():\n",
    "    \n",
    "    # Extract data\n",
    "    points = processed_data[keyIN][\"pos\"] \n",
    "    vectors = processed_data[keyIN][\"E\"]\n",
    "    magnitudes = processed_data[keyIN][\"E_mag\"]\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Step 2: Build KDTree and find nearest neighbor\n",
    "    # ----------------------------------------------------\n",
    "    tree = cKDTree(points)\n",
    "    dist, idx = tree.query(target_point)\n",
    "\n",
    "    # Nearest neighbor field\n",
    "    E_vec_at_point = vectors[idx]\n",
    "    E_mag_at_point = magnitudes[idx]\n",
    "\n",
    "    #print(f\"{keyIN}: E vector (nearest neighbor) = {E_vec_at_point}, |E| = {E_mag_at_point}\")\n",
    "\n",
    "    # Store results\n",
    "    Evector_atpoint_iterations.append((int(keyIN.split(\"_\")[1]), E_vec_at_point, E_mag_at_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of tuples to arrays for easier plotting\n",
    "iterations, E_vectors, E_mag = zip(*Evector_atpoint_iterations)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.loglog(iterations, abs(np.array(E_vectors)[:, 0]), marker='.', linestyle='-', color='red') #abs(np.array(E_vectors)[:, 2])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the field data dictionary for the current iteration (assumes 'df' is a list/dict)\n",
    "fieldIN = processed_data[\"iter_1\"] \n",
    "\n",
    "plt.hist(fieldIN[\"E_mag\"][fieldIN[\"E_mag\"]>0],bins=np.logspace(0,8,100),alpha=0.5)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"|E| (V/m)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Temperature: 425 K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Varying density along a streamline\n",
    "plt.streamplot(processed_data[\"iter_1\"][\"pos\"][:,0], processed_data[\"iter_1\"][\"pos\"][:,1], \\\n",
    "               processed_data[\"iter_1\"][\"E\"][:,0], processed_data[\"iter_1\"][\"E\"][:,1], density=[0.5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Case 1: SW electrons and ions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### plot the field over each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlysolarwind\"\n",
    "\n",
    "# --- Configurations ---\n",
    "folder_path = [ \"../build-temp600K-dynamicThreshold\",\"../build-temp425K-dynamicThreshold\", \\\n",
    "               \"../build-425K-nodissipation-initial6-0.8Max-final9\", \"../build-425K-withoutdissipation\"]\n",
    "temperatures = [600,425,425,425]\n",
    "notes = [\"initial6max0.2final9(dissipation)\",\"initial6max0.2final9(dissipation)\",\"initial6max0.7final9(noDissipation)\",\"initial5max0.8final9(noDissipation)\"]\n",
    "\n",
    "# Target point (Fixed for all configurations)\n",
    "location = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) \n",
    "\n",
    "# --- Parallel Processing Worker Function ---\n",
    "\n",
    "def process_config(tempIN, folderIN, noteIN, configIN, location):\n",
    "    \"\"\"\n",
    "    Worker function to process a single configuration. \n",
    "    Returns the unique key and the calculated results.\n",
    "    \"\"\"\n",
    "    key_name = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    print(f\"--- Processing {key_name} in {folderIN} (Worker Process) ---\\n\")\n",
    "\n",
    "    # 1. Read Field Data\n",
    "    filenames = sorted(glob.glob(f\"{folderIN}/fieldmaps/*{configIN}*.txt\"))\n",
    "    fields_SW = read_data_format_efficient(filenames, scaling=True) \n",
    "    first_key = list(fields_SW.keys())[0]\n",
    "\n",
    "    # Prepare dictionary for this single result\n",
    "    config_results: Dict[str, Any] = {}\n",
    "    \n",
    "    # 2. Compute and Store Field at Target Location\n",
    "    # The function now returns a dict {'iter', 'E', 'E_mag'} for all iterations\n",
    "    config_results[\"fieldAtTarget\"] = compute_nearest_field_vector(fields_SW, target=location, start=first_key)\n",
    "    \n",
    "    # 3. Compute and Store the list of leaf lengths (number of points per iteration)\n",
    "    config_results[\"lengthLeaves\"] = [len(fields_SW[keyIN][\"pos\"]) for keyIN in fields_SW.keys()]\n",
    "    config_results[\"gradRefinements\"] = [fields_SW[keyIN][\"gradRefinements\"] for keyIN in fields_SW.keys()]\n",
    "    \n",
    "    # If the fields_PE dictionary is very large, deleting it immediately frees memory\n",
    "    del fields_SW \n",
    "    \n",
    "    # Return the key and the results to the main thread\n",
    "    return key_name, config_results\n",
    "\n",
    "# MASTER DICTIONARY to store all results securely\n",
    "results_data: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# --- Parallel Processing Loop ---\n",
    "\n",
    "# Prepare the list of arguments for the executor\n",
    "configs = zip(temperatures, folder_path, notes)\n",
    "args_list = [(temp, folder, note, configIN, location) for temp, folder, note in configs]\n",
    "\n",
    "MAX_WORKERS = len(temperatures) # Use one worker per configuration\n",
    "\n",
    "print(f\"--- Starting Parallel Processing with {MAX_WORKERS} workers ---\")\n",
    "\n",
    "# Use ProcessPoolExecutor for CPU-bound tasks\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    \n",
    "    # Submit all tasks and store the future objects\n",
    "    futures = [executor.submit(process_config, *args) for args in args_list]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            key, data = future.result()\n",
    "            results_data[key] = data\n",
    "        except Exception as exc:\n",
    "            print(f'Configuration generated an exception: {exc}')\n",
    "\n",
    "print(f\"--- Completed Parallel Processing ---\")\n",
    "\n",
    "# takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting and Analysis ---\n",
    "\n",
    "print(\"\\n--- Generating Plot for SW Comparison ---\")\n",
    "\n",
    "# 1. Load comparison data from Zimmerman\n",
    "zimmerman_SWdata = pd.read_csv(\"Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"Fig7a-PE.csv\")\n",
    "zimmerman_PEandSWdata = pd.read_csv(\"Fig7a-PE+SW.csv\")\n",
    "\n",
    "# 2. Define constants and calculate conversion factor\n",
    "# Simulation world area size in m^2 (600 um x 600 um)\n",
    "WORLD_XY_AREA_SQ_M = 600 * 600 / (1e6**2) \n",
    "\n",
    "# Number of particles (protons) injected into the active area per iteration\n",
    "PARTICLES_PER_ITERATION = 160440\n",
    "\n",
    "# Ion flux calculated from the simulation area (ions/m^2)\n",
    "FLUX_PER_ITERATION = PARTICLES_PER_ITERATION / WORLD_XY_AREA_SQ_M \n",
    "\n",
    "# Photoemission (PE) ion flux value (e/m^2/s). This factor combines \n",
    "# the effective current (4 uA/cm^2) and conversion to e/m^2/s.\n",
    "SW_ION_FLUX = 3e-7 * 6.241509e18 \n",
    "\n",
    "# Conversion factor: Time (s) per simulation iteration\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION / SW_ION_FLUX\n",
    "print(f\"Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "# 3. Define plot parameters (Colors)\n",
    "# Choose a continuous colormap and generate discrete colors based on the number of temperatures\n",
    "CMAP_NAME = 'jet' \n",
    "discrete_cmap = plt.get_cmap(CMAP_NAME, len(temperatures))\n",
    "color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, len(temperatures))]\n",
    "\n",
    "# 4. Generate Plot (Log-Log)\n",
    "plt.figure()\n",
    "\n",
    "# Plot reference data (Zimmerman)\n",
    "plt.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"], '--', label=\"SW (Zimmerman 2016)\", color=\"r\", lw=4)\n",
    "plt.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], 'g:', label=\"PE (Zimmerman 2016)\")\n",
    "plt.loglog(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"], 'b:', label=\"PE+SW (Zimmerman 2016)\")\n",
    "\n",
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.plot((results_data[key][\"fieldAtTarget\"][\"iter\"] -0.5)* CONVERT_ITERATION_PE_TIME, \n",
    "             results_data[key][\"fieldAtTarget\"][\"E\"][:,0], #results_data[key][\"fieldAtTarget\"][\"E_mag\"] \n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "# Plotting the E-field magnitude (|E|)\n",
    "plt.ylabel(r\"$|E_x$| (V/m)\") \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set axes limits\n",
    "plt.ylim(4.8e3, 2.8e5)\n",
    "plt.xlim(7.4e-2, 1.25e1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"lengthLeaves\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Total Leaf Nodes (millions)\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.title(\"SW Case\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"gradRefinements\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Gradient Refinements (millions)\")\n",
    "plt.title(\"SW Case\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "# configIN = \"onlysolarwind\"\n",
    "# directory_path = \"../build-leakage/\" # takes 12 minutes to read in with this data\n",
    "# #directory_path = \"../build-adaptive-barns-fixed/\"\n",
    "\n",
    "# filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "# fields_SW = read_data_format_efficient(filenames,scaling=True)   \n",
    "# \n",
    "# # Target point\n",
    "# location = np.array([-0.1, 0, 0.1-0.015+0.037]) \n",
    "# # return the electric field at that location\n",
    "# Efield_SW_location = compute_nearest_field_vector(fields_SW, target=location, start=1)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### check the minimum distance between point in field map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "directory_path = \"../build-425K-nodissipation-initial6-0.8Max-final9/\"\n",
    "filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "fields_SW = read_data_format_efficient(filenames,scaling=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fields_PE' is your list/dictionary structure and \n",
    "# fields_PE[1]['pos'] is a NumPy array of shape (N, 3), where N is the number of points.\n",
    "# Example data (replace this with your actual data):\n",
    "data_points = fields_SW[1]['pos'] \n",
    "# data_points = np.array([\n",
    "#     [1.0, 1.0, 1.0],\n",
    "#     [1.001, 1.0, 1.0],  # Very close point\n",
    "#     [2.0, 2.0, 2.0],\n",
    "#     [5.0, 5.0, 5.0]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# 1. Build the KD-Tree\n",
    "# This organizes the points in a spatial structure for efficient nearest neighbor search.\n",
    "tree = cKDTree(data_points)\n",
    "\n",
    "# 2. Query for the 2 nearest neighbors of every point\n",
    "# The 'k=2' parameter tells the query to find the distance to the 2 nearest neighbors:\n",
    "# - The 1st neighbor (k=1) is always the point itself (distance = 0.0).\n",
    "# - The 2nd neighbor (k=2) is the closest *other* point.\n",
    "distances, indices = tree.query(data_points, k=2)\n",
    "\n",
    "# 3. Extract the minimum non-zero distance\n",
    "# The minimum distance between any unique pair of points is the minimum value \n",
    "# in the array of distances to the second nearest neighbor (distances[:, 1]).\n",
    "min_distance = np.min(distances[:, 1])*1000\n",
    "\n",
    "print(f\"The total number of points (voxels) is: {len(data_points)}\")\n",
    "print(f\"The minimum distance between any two unique voxels is: {min_distance} um\")\n",
    "\n",
    "# takes around ~10 seconds to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### calculate # of iterations for direct comparison with Zimmerman:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory_path = \"../build-adaptive-barns/root/\"\n",
    "directory_path = \"../build-leakage-425K-finerbinning/root/\"\n",
    "configIN = \"solarwind\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*num500000.root\"))\n",
    "\n",
    "all_incident_protons_inside, all_incident_electrons_inside = [],[]\n",
    "\n",
    "for fileIN in filelist:\n",
    "\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"protons_inside_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)] = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path), \n",
    "                                                                                                             config=configIN)\n",
    "    all_incident_protons_inside.append(vars()[\"protons_inside_\"+str(number_str)])\n",
    "    all_incident_electrons_inside.append(vars()[\"electrons_inside_\"+str(number_str)])\n",
    "    print(78*\"-\")\n",
    "\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all iterations into single DataFrames\n",
    "all_incident_protons_inside_df = pd.concat(all_incident_protons_inside, ignore_index=True)\n",
    "all_incident_electrons_inside_df = pd.concat(all_incident_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf, ilm_values,_ = plot_face_illumination(electrons_inside_stackediteration0, stacked_spheres, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ilm_values, bins=np.logspace(-1,3,100))\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"# of Photons hitting each Voxel\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Distribution Stats: mean {np.mean(ilm_values[ilm_values!=0])}, median {np.median(ilm_values[ilm_values!=0])}, max {np.max(ilm_values)}\")\n",
    "\n",
    "# our area is a factor of 4 smaller than their area \n",
    "#print(f\"for one iteration, mean # of particles in each equivalently sized voxel is {np.mean(ilm_values[ilm_values!=0])}\") <- no longer needed, made voxels similar sizes\n",
    "voxel_area = 0.0004/(1000)**2 # rough area approximated from python (0.4 micron2)\n",
    "zimmerman_charge = 1*(1e-6) # C/m2\n",
    "zimmerman_electronnum = (zimmerman_charge/1.60217663e-19)*voxel_area\n",
    "print(f\"will take {zimmerman_electronnum/(np.mean(ilm_values[ilm_values!=0]))} iterations at this rate to get to the photoemission flux ranges shown at 3 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### make a movie of all surface potential for each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"figures/solarwind/\"\n",
    "\n",
    "for num in range(0, iterationNUM+1):\n",
    "    surface, _, facecolors = plot_surface_potential_fornegativepositive_charge(\n",
    "        vars()[\"electrons_inside_stackediteration\"+str(num)], \n",
    "        vars()[\"protons_inside_stackediteration\"+str(num)], \n",
    "        stacked_spheres, \n",
    "        vmin=-1.8, vmax=1.8\n",
    "    )\n",
    "\n",
    "    surface_edited = surface.copy()\n",
    "    surface_edited.unmerge_vertices()  # Ensure unique vertices per face\n",
    "\n",
    "    # Crop bounding box\n",
    "    bbox_min = np.array([-0.2, -0.3, 0])\n",
    "    bbox_max = np.array([ 0.2,  0.1, 100])\n",
    "\n",
    "    in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                    (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "    face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "    cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "    cropped_colors = facecolors[face_mask]/255  # Crop facecolors to match cropped mesh\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    mesh = Poly3DCollection(cropped.triangles, alpha=1.0)\n",
    "    mesh.set_facecolor(cropped_colors)  # Apply correct colors\n",
    "    mesh.set_edgecolor('k')          # edge color\n",
    "    mesh.set_linewidths(0.1)         # edge line width\n",
    "\n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_title(f\"Iteration {num}\")\n",
    "    ax.title.set_position((0.5, 0.1))  # manually control title position\n",
    "\n",
    "    # Scale\n",
    "    scale = surface_edited.bounds.flatten()\n",
    "    ax.auto_scale_xyz(scale, scale, scale)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    filename = f\"{directory}iteration_{num}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    if num%10==0:\n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "print(f\"All plots saved to {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"figures/solarwind/\"\n",
    "\n",
    "for num in range(0, iterationNUM + 1):\n",
    "    # Get face illumination for electrons and protons\n",
    "    surface_e, _, facecolors_e = plot_face_illumination(\n",
    "        vars()[\"electrons_inside_stackediteration\" + str(num)],\n",
    "        stacked_spheres, vmin=0, vmax=100\n",
    "    )\n",
    "\n",
    "    surface_p, _, facecolors_p = plot_face_illumination(\n",
    "        vars()[\"protons_inside_stackediteration\" + str(num)],\n",
    "        stacked_spheres, vmin=0, vmax=100\n",
    "    )\n",
    "\n",
    "    def crop_and_prepare(surface, facecolors):\n",
    "        surface = surface.copy()\n",
    "        surface.unmerge_vertices()\n",
    "\n",
    "        # Crop bounding box\n",
    "        bbox_min = np.array([-0.2, -0.3, 0])\n",
    "        bbox_max = np.array([ 0.2,  0.1, 100])\n",
    "\n",
    "        in_box = np.all((surface.vertices >= bbox_min) & \n",
    "                        (surface.vertices <= bbox_max), axis=1)\n",
    "\n",
    "        face_mask = np.all(in_box[surface.faces], axis=1)\n",
    "        cropped = surface.submesh([face_mask], only_watertight=False, append=True)\n",
    "        cropped_colors = facecolors[face_mask] / 255.0\n",
    "        return cropped, cropped_colors\n",
    "\n",
    "    cropped_e, colors_e = crop_and_prepare(surface_e, facecolors_e)\n",
    "    cropped_p, colors_p = crop_and_prepare(surface_p, facecolors_p)\n",
    "\n",
    "    # Plotting both side by side\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "    for i, (cropped, colors, title) in enumerate([\n",
    "        (cropped_e, colors_e, \"Electron Illumination\"),\n",
    "        (cropped_p, colors_p, \"Proton Illumination\")\n",
    "    ]):\n",
    "        ax = fig.add_subplot(1, 2, i + 1, projection='3d')\n",
    "        mesh = Poly3DCollection(cropped.triangles, alpha=1.0)\n",
    "        mesh.set_facecolor(colors)\n",
    "        mesh.set_edgecolor('k')\n",
    "        mesh.set_linewidths(0.1)\n",
    "        ax.add_collection3d(mesh)\n",
    "\n",
    "        scale = cropped.bounds.flatten()\n",
    "        ax.auto_scale_xyz(scale, scale, scale)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(f\"{title}\\nIteration {num}\", pad=5)\n",
    "\n",
    "    filename = f\"{directory}iteration_{num}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    if num % 10 == 0:\n",
    "        print(f\"Saved: {filename}\")\n",
    "        \n",
    "print(f\"âœ… All plots saved to {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterationIN=0\n",
    "\n",
    "directory = \"figures/solarwind/\"\n",
    "\n",
    "for num in range(0,iterationNUM):\n",
    "    surface,_ = plot_surface_potential_fornegativepositive_charge(vars()[\"electrons_inside_stackediteration\"+str(num)], vars()[\"protons_inside_stackediteration\"+str(num)], stacked_spheres, vmin=-10,vmax=10)\n",
    "\n",
    "    # plt.hist(facolors[facolors!=0],bins=50)\n",
    "    # plt.show()\n",
    "\n",
    "    # Make sure each triangle has its own unique vertices\n",
    "    surface_edited = surface.copy()\n",
    "    surface_edited.unmerge_vertices()\n",
    "    surface_edited.visual.vertex_colors = None\n",
    "    surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap and normalization\n",
    "cmap = plt.cm.OrRd #seismic\n",
    "norm = Normalize(vmin=0, vmax=100)\n",
    "\n",
    "# Create a figure and a single axis for the colorbar\n",
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "# Create the colorbar\n",
    "cb = ColorbarBase(ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "cb.set_label('# of Particles / face')  # Optional label\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.25, -0.31, 0])\n",
    "bbox_max = np.array([ 0.15,  0.1,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Case 2: Photoemission (incident gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### plot the field over each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output110525/analysis/processed-fieldmaps\"\n",
    "processed_data = load_h5_to_dict(f\"{directory}/PE_425K_initial8max0.8final12_sphere50um.h5\")\n",
    "# Retrieve the field data dictionary for the current iteration (assumes 'df' is a list/dict)\n",
    "\n",
    "# takes ~2 minutes to read in 12 GB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target point\n",
    "#target_point = np.array([-0.1-0.0073, 0, 0.1 - 0.015 + 0.035-0.00073]) # point on the sphere (field starts to flatten)\n",
    "target_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.036])\n",
    "\n",
    "Evector_atpoint_iterations = []\n",
    "\n",
    "radius = 5e-3 #FIELD_AVERAGE_RADIUS #3e-3\n",
    "\n",
    "print(\"Target point:\", target_point, \" ,Average Radius: \", radius)\n",
    "\n",
    "for keyIN in processed_data.keys():\n",
    "    \n",
    "    points = processed_data[keyIN][\"pos\"] \n",
    "    vectors = processed_data[keyIN][\"E\"]\n",
    "    magnitudes = processed_data[keyIN][\"E_mag\"]\n",
    "\n",
    "    distances = np.linalg.norm(points - target_point, axis=1)\n",
    "    within_radius_mask = distances <= radius\n",
    "\n",
    "    # Average over points within radius\n",
    "    avg_position = np.mean(points[within_radius_mask], axis=0)\n",
    "    E_vec_at_point = np.mean(vectors[within_radius_mask], axis=0)\n",
    "    E_mag_at_point = np.mean(magnitudes[within_radius_mask], axis=0)\n",
    "\n",
    "    point_error = np.abs(avg_position - target_point)\n",
    "\n",
    "    Evector_atpoint_iterations.append(\n",
    "        (int(keyIN.split(\"_\")[1]), E_vec_at_point, E_mag_at_point, point_error)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting and Analysis ---\n",
    "\n",
    "print(\"\\n--- Generating Plot for PE Comparison ---\")\n",
    "\n",
    "# 1. Load comparison data from Zimmerman\n",
    "zimmerman_SWdata = pd.read_csv(\"literature-data/Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"literature-data/Fig7a-PE.csv\")\n",
    "zimmerman_PEandSWdata = pd.read_csv(\"literature-data/Fig7a-PE+SW.csv\")\n",
    "\n",
    "# 2. Define constants and calculate conversion factor\n",
    "# Simulation world area size in microns\n",
    "WORLD_XY_AREA_SQ_M = 300 * 300 / (1e6**2) \n",
    "\n",
    "# Number of particles injected into the active area per iteration\n",
    "PARTICLES_PER_ITERATION = 81775 #80559 # for the old geometry with smaller voxels #424975 # for 600by600 world\n",
    "\n",
    "# Ion flux calculated from the simulation area (ions/m^2)\n",
    "FLUX_PER_ITERATION = PARTICLES_PER_ITERATION / WORLD_XY_AREA_SQ_M \n",
    "\n",
    "# Photoemission (PE) ion flux value (e/m^2/s). This factor combines \n",
    "# the effective current (4 uA/cm^2) and conversion to e/m^2/s.\n",
    "PE_ION_FLUX = 4e-6 * 6.241509e18 \n",
    "\n",
    "# Conversion factor: Time (s) per simulation iteration\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION / PE_ION_FLUX\n",
    "print(f\"Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "# # 3. Define plot parameters (Colors)\n",
    "# # Choose a continuous colormap and generate discrete colors based on the number of temperatures\n",
    "# CMAP_NAME = 'jet' \n",
    "# discrete_cmap = plt.get_cmap(CMAP_NAME, len(config_results.keys()) + 1)\n",
    "# color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, len(config_results.keys()) + 1)]\n",
    "\n",
    "# 4. Generate Plot (Log-Log)\n",
    "plt.figure()\n",
    "\n",
    "# Plot reference data (Zimmerman)\n",
    "plt.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"], '--', color=\"r\", lw=4) #, label=\"SW (Zimmerman 2016)\"\n",
    "plt.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], 'g:') #, label=\"PE (Zimmerman 2016)\"\n",
    "plt.plot(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"], 'b:') #, label=\"PE+SW (Zimmerman 2016)\"\n",
    "\n",
    "# Convert list of tuples to arrays for easier plotting\n",
    "iterations, E_vectors, E_mag, _ = zip(*Evector_atpoint_iterations)\n",
    "\n",
    "# Plot\n",
    "plt.loglog(np.array(iterations) * CONVERT_ITERATION_PE_TIME, abs(np.array(E_vectors)[:, 0]), marker='.', linestyle='-', color='green',label=\"Geant4: PE Case\") #abs(np.array(E_vectors)[:, 2])\n",
    "\n",
    "# # Plot simulation results\n",
    "# for key, colorIN in zip(config_results.keys(),color_list_rgba):    \n",
    "#     tempIN = key.split(\"_\")[1]\n",
    "#     noteIN = key.split(\"_\")[2]\n",
    "    \n",
    "    # # Plot E-field magnitude at target location\n",
    "    # plt.plot(np.array(config_results[key][\"fieldAtTarget\"][\"iter\"])* CONVERT_ITERATION_PE_TIME, \n",
    "    #          abs(np.array(results_data[key][\"fieldAtTarget\"][\"E\"])[:,0]), #np.array(config_results[key][\"fieldAtTarget\"][\"E_mag\"])[:,0], #abs(np.array(results_data[key][\"fieldAtTarget\"][\"E\"])[:,0]), #np.array(config_results[key][\"fieldAtTarget\"][\"E_mag\"])[:,0], #abs(results_data[key][\"fieldAtTarget\"][\"E\"][:,0]), #results_data[key][\"fieldAtTarget\"][\"E_mag\"]\n",
    "    #          '.-',\n",
    "    #          label=f\"{tempIN}: {noteIN}\",\n",
    "    #          lw=0.5, \n",
    "    #          color=colorIN)\n",
    "    \n",
    "    # # Plot E-field magnitude at target location\n",
    "    # plt.errorbar((results_data[key][\"fieldAtTarget\"][\"iter\"])* CONVERT_ITERATION_PE_TIME, \n",
    "    #           results_data[key][\"fieldAtTarget\"][\"E_mag\"][:,0], yerr = results_data[key][\"fieldAtTarget\"][\"E_mag\"][:,1]/2, #abs(results_data[key][\"fieldAtTarget\"][\"E\"][:,0]), #results_data[key][\"fieldAtTarget\"][\"E_mag\"]\n",
    "    #          label=f\"{tempIN} K: {noteIN}\",\n",
    "    #          lw=0.5, \n",
    "    #          color=colorIN)\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "# Plotting the E-field magnitude (|E|)\n",
    "plt.axvline(x=65*CONVERT_ITERATION_PE_TIME)\n",
    "plt.ylabel(r\"$|E_x$| (V/m)\") \n",
    "#plt.ylabel(r\"$|E$| (V/m)\") \n",
    "plt.legend() #bbox_to_anchor=(1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "# Set axes limits\n",
    "#plt.ylim(4.8e3, 2.8e5)\n",
    "#plt.xlim(7.4e-1, 1.25e1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "23890015 - 24198886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"lengthLeaves\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Total Leaf Nodes (millions)\")\n",
    "plt.title(\"PE Case\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"gradRefinements\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Gradient Refinements (millions)\")\n",
    "plt.title(\"PE Case\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "# configIN = \"onlyphotoemission\"\n",
    "# directory_path = \"../build-disspate-charge/\"\n",
    "\n",
    "# filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "# fields_PE = read_data_format_efficient(filenames,scaling=True) \n",
    "\n",
    "# # Target point\n",
    "# location = np.array([-0.1, 0, 0.1-0.015+0.037]) \n",
    "# # return the electric field at that location\n",
    "# Efield_PE_location = compute_nearest_field_vector(fields_PE, target=location, start=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process different sets of iterations (not in parallel) ##\n",
    "\n",
    "# configIN = \"onlyphotoemission\"\n",
    "# folder_path = [\"../build-temp425K-dynamicThreshold\",  \"../build-temp600K-dynamicThreshold\", \"../build-425K-initial6-0.9Max-0.05um-final10\"]\n",
    "# temperatures = [425,600,425]\n",
    "# notes = [\"initial6max0.2final9\", \"initial6max0.2final9\", \"initial6max0.6final10\"]\n",
    "\n",
    "# # Target point\n",
    "# location = np.array([-0.1, 0, 0.1-0.015+0.037]) \n",
    "\n",
    "# # MASTER DICTIONARY to store all results securely\n",
    "# results_data: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# # --- Processing Loop ---\n",
    "\n",
    "# # Use enumerate for index 'j' and zip the parameter lists together\n",
    "# for j, (tempIN, folderIN, noteIN) in enumerate(zip(temperatures, folder_path, notes)):\n",
    "\n",
    "#     # Create a unique key for storing results\n",
    "#     key_name = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "#     print(f\"--- Processing {j}: {key_name} from {folderIN} ---\")\n",
    "\n",
    "#     # 1. Read Field Data\n",
    "#     filenames = sorted(glob.glob(f\"{folderIN}/fieldmaps/*{configIN}*.txt\"))\n",
    "#     fields_PE = read_data_format_efficient(filenames, scaling=True) \n",
    "#     first_key = list(fields_PE.keys())[0]\n",
    "    \n",
    "#     # 2. Initialize entry in the results dictionary\n",
    "#     results_data[key_name] = {}\n",
    "    \n",
    "#     # 3. Compute and Store Field at Target Location\n",
    "#     results_data[key_name][\"fieldAtTarget\"] = compute_nearest_field_vector(fields_PE, target=location, start=first_key)\n",
    "    \n",
    "#     # 4. Compute and Store the list of leaf lengths (number of points per iteration)\n",
    "#     results_data[key_name][\"lengthLeaves\"] = np.array([len(fields_PE[keyIN][\"pos\"]) for keyIN in fields_PE.keys()])\n",
    "    \n",
    "#     # If the fields_PE dictionary is very large, deleting it immediately frees memory\n",
    "#     #del fields_PE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### check the minimum distance between point in field map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "directory_path = \"../build-smallerworld-initial8max0.8final13/\"\n",
    "filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "fields_PE = read_data_format_efficient(filenames,scaling=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fields_PE' is your list/dictionary structure and \n",
    "# fields_PE[1]['pos'] is a NumPy array of shape (N, 3), where N is the number of points.\n",
    "# Example data (replace this with your actual data):\n",
    "data_points = fields_PE[1]['pos'] \n",
    "# data_points = np.array([\n",
    "#     [1.0, 1.0, 1.0],\n",
    "#     [1.001, 1.0, 1.0],  # Very close point\n",
    "#     [2.0, 2.0, 2.0],\n",
    "#     [5.0, 5.0, 5.0]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# 1. Build the KD-Tree\n",
    "# This organizes the points in a spatial structure for efficient nearest neighbor search.\n",
    "tree = cKDTree(data_points)\n",
    "\n",
    "# 2. Query for the 2 nearest neighbors of every point\n",
    "# The 'k=2' parameter tells the query to find the distance to the 2 nearest neighbors:\n",
    "# - The 1st neighbor (k=1) is always the point itself (distance = 0.0).\n",
    "# - The 2nd neighbor (k=2) is the closest *other* point.\n",
    "distances, indices = tree.query(data_points, k=2)\n",
    "\n",
    "# 3. Extract the minimum non-zero distance\n",
    "# The minimum distance between any unique pair of points is the minimum value \n",
    "# in the array of distances to the second nearest neighbor (distances[:, 1]).\n",
    "min_distance = np.min(distances[:, 1])*1000\n",
    "\n",
    "print(f\"The total number of points (voxels) is: {len(data_points)}\")\n",
    "print(f\"The minimum distance between any two unique voxels is: {min_distance} um\")\n",
    "\n",
    "# takes around ~10 seconds to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### calculate temperature change over all iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Worker Function for Parallel Execution ---\n",
    "def process_root_file(fileIN: str, directory_path: str, target_volume = \"SiO2\") -> Tuple[int, float]:\n",
    "    \"\"\"Reads a single ROOT file, performs event analysis, and returns the index and calculated total energy.\"\"\"\n",
    "\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    try:\n",
    "        # Read data for the current iteration\n",
    "        df = read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path)\n",
    "\n",
    "    except Exception:\n",
    "        # Catch errors like missing keys or file corruption during read_rootfile\n",
    "        print(f\"-> ERROR: Skipping {fileIN.split('/')[-1]} due to failed file read\")\n",
    "        return iterationNUM, 0.0 # Return 0 energy and the index to maintain order\n",
    "\n",
    "    # 1. Get all incident gamma events (Particle_Type=\"gamma\", Parent_ID=0.0)\n",
    "    incident_gamma = df[(df[\"Particle_Type\"] == \"gamma\") & (df[\"Parent_ID\"] == 0.0)].drop_duplicates(subset=\"Event_Number\", keep=\"first\")\n",
    "\n",
    "    # 2. Get all unique event numbers that resulted in an electron creation\n",
    "    last_e_event = df[(df[\"Particle_Type\"] == \"e-\") & (df[\"Parent_ID\"] > 0.0)].drop_duplicates(subset=\"Event_Number\", keep=\"last\")\n",
    "    event_numbers_with_e_creation = last_e_event[\"Event_Number\"].unique()\n",
    "\n",
    "    # 3. All unique incident gamma event numbers\n",
    "    incident_gamma_event_numbers = incident_gamma[\"Event_Number\"].unique()\n",
    "\n",
    "    # 4. Events where NO electron was created (incident gamma events - events with e- creation)\n",
    "    events_without_photoelectric_e = np.setdiff1d(incident_gamma_event_numbers, event_numbers_with_e_creation)\n",
    "\n",
    "    # 5. Filter the main DataFrame to contain only data from the non-interacting events\n",
    "    events_without_photoelectric_e_df = df[df[\"Event_Number\"].isin(events_without_photoelectric_e)]\n",
    "\n",
    "    # 6. Calculate total energy deposited by gammas that *did not* result in a photoelectric electron\n",
    "    totalEnergy = np.sum(events_without_photoelectric_e_df[\n",
    "        (events_without_photoelectric_e_df[\"Particle_Type\"] == \"gamma\") & \n",
    "        (events_without_photoelectric_e_df[\"Volume_Name_Post\"] == target_volume)\n",
    "    ][\"Kinetic_Energy_Diff_eV\"])\n",
    "\n",
    "    print(f\"-> PROCESSED #{iterationNUM}: {fileIN.split('/')[-1]}\")\n",
    "\n",
    "    return iterationNUM, totalEnergy\n",
    "\n",
    "# --- Configuration ---\n",
    "configIN = \"onlyphotoemission\"\n",
    "directory_path =  \"../build-temp425K-dynamicThreshold/root/\" \n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*.root\"))\n",
    "\n",
    "# --- Main Parallel Execution ---\n",
    "\n",
    "# List to hold the (index, totalEnergy) tuples from parallel processes\n",
    "NUM_FILES = len(filelist)\n",
    "photoEnergyDepositionsforIterations = np.empty(NUM_FILES, dtype=np.float64)\n",
    "\n",
    "print(f\"--- Starting Parallel Processing of {NUM_FILES} files ---\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=NUM_FILES) as executor:\n",
    "    \n",
    "    # Submit tasks, passing the index to ensure results are ordered correctly later\n",
    "    futures = [executor.submit(process_root_file, fileIN, directory_path) for fileIN in filelist]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        index, totalEnergy = future.result()\n",
    "        photoEnergyDepositionsforIterations[index]= totalEnergy\n",
    "\n",
    "# Final assignment to the NumPy array\n",
    "#photoEnergyDepositionsforIterations = np.array([r[1] for r in all_results], dtype=np.float64)\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "initialT = 425\n",
    "heat_capacity = 670+1e3*((initialT-250)/530.6)-1e3*((initialT-250)/498.7)**2 # for lunar regolith\n",
    "density = 2.2/1000 #kg/cm3\n",
    "radius = 5 # assuming that all of the energy is deposited in a 10 um area!!\n",
    "volume = 4/2*np.pi*(radius*1e-4)**3 # cm3\n",
    "mass = volume*density # mass of material\n",
    "# this radius and volume is not true, just calculated as an extreme to see if we need to dynamically adjust the temperature!!\n",
    "\n",
    "print(\"--- Over {NUM_FILES} Iterations ---\")\n",
    "print(f\"Mean Temperature Increase : {np.mean(photoEnergyDepositionsforIterations*1.60218e-19/heat_capacity/mass*100)} K\")\n",
    "print(f\"Total Temperature Increase: {np.sum(photoEnergyDepositionsforIterations*1.60218e-19/heat_capacity/mass*100)} K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### create plots of the face ilumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "#directory_path =  \"../build-temp425K-dynamicThreshold/root/\" #\"../build-adaptive-barns-fixed/root/\"\n",
    "directory_path = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output110525/smallerworld-initial8max0.8final12/\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*.root\"))\n",
    "\n",
    "all_gamma_holes = []\n",
    "all_electrons_inside = []\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"gamma_holes_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)], _ = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path),\n",
    "                                                                                                             config=configIN)\n",
    "    \n",
    "    surf, ilm_values = plot_face_illumination(vars()[\"gamma_holes_\"+str(number_str)], stacked_spheres, vmin=0, vmax=1)\n",
    "    # Make sure each triangle has its own unique vertices\n",
    "    surface_edited = surface.copy()\n",
    "    surface_edited.unmerge_vertices()\n",
    "    surface_edited.visual.vertex_colors = None\n",
    "    surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf, ilm_values = plot_face_illumination(gamma_holes_stackediteration0, stacked_spheres, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "directory_path = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output110525/smallerworld-initial8max0.8final12/root/\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*{configIN}*_num100000.root\"))\n",
    "\n",
    "all_gamma_holes = []\n",
    "all_electrons_inside = []\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    gamma_holes_df, electron_inside_df, _ = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path),\n",
    "                                                                                                             config=configIN)\n",
    "    all_gamma_holes.append(gamma_holes_df)\n",
    "    all_electrons_inside.append(electron_inside_df)\n",
    "    print(78*\"-\")\n",
    "\n",
    "    #break\n",
    "\n",
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"/storage/coda1/p-zjiang33/0/shared/avira7/root_files/stacked-sphere/processed-files\" \n",
    "all_gamma_holes_df.to_pickle(f'{save_directory}/PE_gammaholes_locations.pkl') # only got through 43 iterations\n",
    "all_electrons_inside_df.to_pickle(f'{save_directory}/PE_electronsinside_locations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    \"\"\"\n",
    "    Loads data from a pickle file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Successfully loaded data from {filepath}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pickle file {filepath}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_directory = \"/storage/coda1/p-zjiang33/0/shared/avira7/root_files/stacked-sphere/processed-files\" \n",
    "all_gamma_holes_df=load_pickle(f'{save_directory}/PE_gammaholes_locations.pkl') # only got through 43 iterations\n",
    "all_electrons_inside_df=load_pickle(f'{save_directory}/PE_electronsinside_locations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 43\")\n",
    "# input order: gammas, photoelectrons, protons, electrons, convex_combined,\n",
    "surface,facecolors,_ = plot_electric_pressure_from_charge_density(all_electrons_inside_df, all_gamma_holes_df, stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gamma_holes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_saturation(t, a, b, tau):\n",
    "    return a - b * np.exp(-t / tau)\n",
    "\n",
    "\n",
    "# Initial parameter guess: a, b, tau\n",
    "initial_guess = [4.0, 2.0, 1.0]\n",
    "popt, pcov = curve_fit(exp_saturation, 10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], p0=initial_guess)\n",
    "\n",
    "# Plot\n",
    "plt.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"],'--',label=\"SW\",color=\"r\",lw=4)\n",
    "plt.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"],'g:',label=\"PE\")\n",
    "plt.plot(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"],'b:',label=\"PE+SW\")\n",
    "plt.plot((efield_PE[\"iter\"] - 101)*convert_iteration_PEtime, abs(mag_values_PE), 'k.-',label=\"Geant4: PE (different incident)\",lw=0.5)\n",
    "#plt.semilogx((efield_PE_noholes[\"iter\"] - 101)*convert_iteration_PEtime2, abs(mag_values_PE_noholes), 'b.-',label=\"Geant4: PE (no holes)\",lw=0.5)\n",
    "#plt.plot((efield_PE_normal[\"iter\"] - 101)*convert_iteration_PEtime, abs(mag_values_PE_normal), 'g.-',label=\"Geant4: PE\",lw=0.5)\n",
    "#plt.plot((efield_SW[\"iter\"] - 1)*convert_iteration_SWtime, mag_values_SW, 'k.-',label=\"Geant4: SW\",lw=0.5)\n",
    "\n",
    "# xdata_zimmerman = np.linspace(1e-2,10,100)\n",
    "# y_fit_zimmerman = exp_saturation(xdata_zimmerman, *popt)\n",
    "# #plt.plot(xdata_zimmerman, y_fit_zimmerman, 'r-',lw=0.2)\n",
    "\n",
    "# # Initial parameter guess: a, b, tau\n",
    "# initial_guess = [4.0, 2.0, 1.0]\n",
    "# popt, pcov = curve_fit(exp_saturation,(efield_PE[\"iter\"] - 101)*convert_iteration_PEtime, abs(mag_values_PE), p0=popt)\n",
    "\n",
    "# xdata = np.linspace(1e-1,5,100)\n",
    "# y_fit = exp_saturation(xdata, *popt)\n",
    "# plt.plot(xdata, y_fit, 'g-',lw=0.2, label=\"prediction\")\n",
    "# #plt.plot(xdata_zimmerman, y_fit_zimmerman-3.8e4, 'r-',lw=0.2)\n",
    "# plt.axvline(x=4)\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(r\"|E| (V/m)\")\n",
    "#plt.ylabel(r\"E$_x$ (V/m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ilm_values, bins=np.logspace(-1,3,100))\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"# of Photons hitting each Voxel\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Distribution Stats: mean {np.mean(ilm_values[ilm_values!=0])}, median {np.median(ilm_values[ilm_values!=0])}, max {np.max(ilm_values)}\")\n",
    "\n",
    "# our area is a factor of 4 smaller than their area \n",
    "print(f\"for one iteration, median # of particles in each equivalently sized voxel is {np.median(ilm_values[ilm_values!=0])/4}\")\n",
    "voxel_area = 0.0004/(1000)**2 # rough area approximated from python (0.4 micron2)\n",
    "zimmerman_charge = 0.5*(1e-6) # C/m2\n",
    "zimmerman_electronnum = (zimmerman_charge/1.60217663e-19)*voxel_area\n",
    "print(f\"will take {zimmerman_electronnum/(np.max(ilm_values[ilm_values!=0])/4)} iterations at this rate to get to the photoemission flux ranges shown at 3 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 33\")\n",
    "surface,pot = plot_surface_potential_fornegativepositive_charge(all_electrons_inside_df, all_gamma_holes_df, stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "print(min(pot),max(pot))\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.2, -0.2, 0])\n",
    "bbox_max = np.array([ 0.2,  0.2,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((stacked_spheres.vertices >= bbox_min) & \n",
    "                (stacked_spheres.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[stacked_spheres.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = stacked_spheres.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap and normalization\n",
    "cmap = plt.cm.seismic\n",
    "norm = Normalize(vmin=-0.2, vmax=0.2)\n",
    "\n",
    "# Create a figure and a single axis for the colorbar\n",
    "fig, ax = plt.subplots(figsize=(4, 0.5))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "# Create the colorbar\n",
    "cb = ColorbarBase(ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "cb.set_label('Surface Potential (mV)')  # Optional label\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "## Case 3: all particles (incident e-, protons, gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"../build-sphere-charging/root/\"\n",
    "configIN = \"allparticles\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*stackediteration*{configIN}*num5000.root\"))\n",
    "\n",
    "all_gamma_holes,all_photoemission_electrons,all_protons_inside,all_electrons_inside = [],[],[],[]\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"gamma_holes_\"+str(number_str)], vars()[\"photoemission_electrons_inside_\"+str(number_str)], \\\n",
    "        vars()[\"protons_inside_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)] = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path), \\\n",
    "                                                                                                             config=configIN)\n",
    "    all_gamma_holes.append(vars()[\"gamma_holes_\"+str(number_str)])\n",
    "    all_photoemission_electrons.append(vars()[\"photoemission_electrons_inside_\"+str(number_str)])\n",
    "    all_protons_inside.append(vars()[\"protons_inside_\"+str(number_str)])\n",
    "    all_electrons_inside.append(vars()[\"electrons_inside_\"+str(number_str)])\n",
    "    print(78*\"-\")\n",
    "\n",
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_photoemission_electrons_df = pd.concat(all_photoemission_electrons, ignore_index=True)\n",
    "all_protons_inside_df = pd.concat(all_protons_inside, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_photoemission_electrons_df = pd.concat(all_photoemission_electrons, ignore_index=True)\n",
    "all_protons_inside_df = pd.concat(all_protons_inside, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 28\")\n",
    "# input order: gammas, photoelectrons, protons, electrons, convex_combined,\n",
    "surface,facecolors = plot_electric_pressure_from_charge_density(all_gamma_holes_df, all_photoemission_electrons_df, all_protons_inside_df, all_electrons_inside_df, \n",
    "                                                      stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.2, -0.3, 0])\n",
    "bbox_max = np.array([ 0.2,  0.1,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap and normalization\n",
    "cmap = plt.cm.seismic\n",
    "norm = Normalize(vmin=-0.2, vmax=0.2)\n",
    "\n",
    "# Create a figure and a single axis for the colorbar\n",
    "fig, ax = plt.subplots(figsize=(4, 0.5))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "# Create the colorbar\n",
    "cb = ColorbarBase(ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "cb.set_label('Surface Potential (mV)')  # Optional label\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 19: only SW ions\")\n",
    "surface,facecolors = plot_surface_potential_fornegativepositive_charge(all_electrons_inside_df, all_protons_inside_df, stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 22: only photons\")\n",
    "surface,facecolors = plot_surface_potential_fornegativepositive_charge(all_photoemission_electrons_df, all_gamma_holes_df, stacked_spheres, vmin=-0.5,vmax=0.5)\n",
    "\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.2, -0.3, 0])\n",
    "bbox_max = np.array([ 0.2,  0.1,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
