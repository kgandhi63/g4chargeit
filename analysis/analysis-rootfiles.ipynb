{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# import pyvista as pv\n",
    "# pv.set_jupyter_backend('trame')  # or 'panel' if using panel\n",
    "\n",
    "from scipy.constants import epsilon_0, e as q_e\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.collections import LineCollection\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import matplotlib as mpl\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "import trimesh\n",
    "import h5py\n",
    "from trimesh.points import PointCloud\n",
    "\n",
    "from common_functions import *\n",
    " \n",
    "# Enable LaTeX rendering\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Set the global font size\n",
    "mpl.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN STACKED SPHERES GEOMETRY ## \n",
    "\n",
    "stacked_spheres = trimesh.load_mesh('../sphere-charging/geometry/stacked_spheres_frompython_cropped.stl') \n",
    "#stacked_spheres = trimesh.load_mesh('../sphere-charging/geometry/isolated_grains_interpolated.stl') \n",
    "\n",
    "# Visualize with Trimesh\n",
    "stacked_spheres.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Visual Representation of the Electric Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### read raw fieldmap from build folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 30\n",
    "\n",
    "configIN = \"onlyphotoemission\"\n",
    "directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output111025/processed-fieldmaps\"\n",
    "#directory = \"../build-dissipationRefinedGrid-initial8max0.8final12\"\n",
    "\n",
    "if iteration <10 :\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*00{iteration}*{configIN}*.txt\")) #{iteration}\n",
    "else:\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*{iteration}*{configIN}*.txt\")) #{iteration}\n",
    "print(filenames)\n",
    "\n",
    "df  = read_data_format_efficient(filenames,scaling=True)\n",
    "\n",
    "# check to make sure this matches the total nodes in outputlogs\n",
    "#len(df[iteration][\"E_mag\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS HERE ARE OPTIMIZED FOR ITERATION 86 ##\n",
    "\n",
    "fieldIN = df[iteration]\n",
    "\n",
    "N_DOWNSAMPLE_EMAG = 1\n",
    "ARROW_VOXEL_SPACING = 0.02 \n",
    "Y_SLICE = 0.0\n",
    "THICKNESS = 0.001\n",
    "VECTOR_SCALE_FACTOR = 9e-7 #2e-7 #2e-3 #5e-6 #2e-3 #5e-6 # Global scaling for glyphs\n",
    "FIELD_AVERAGE_RADIUS = 2.8e-3 #2e-3\n",
    "\n",
    "vmin, vmax = (-2e5, 2e5) # in log(E_mag) units\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) # \n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Voxel Downsampling Helper Function\n",
    "# Ensures uniform spatial distribution of points in the slice\n",
    "# ----------------------------------------------------\n",
    "def voxel_downsample_points(points, spacing):\n",
    "    \"\"\"\n",
    "    Selects one point per voxel defined by the spacing.\n",
    "    Assumes points are 3D, but only uses X and Z for 2D density control.\n",
    "    \"\"\"\n",
    "    # 1. Normalize coordinates to voxel indices (focus on X and Z for the 2D slice)\n",
    "    min_x, _, min_z = points.min(axis=0)\n",
    "    \n",
    "    # Calculate bin indices for the points\n",
    "    # We use X (column 0) and Z (column 2)\n",
    "    x_indices = np.floor((points[:, 0] - min_x) / spacing).astype(int)\n",
    "    z_indices = np.floor((points[:, 2] - min_z) / spacing).astype(int)\n",
    "    \n",
    "    # Combine X and Z indices into a unique hash/key\n",
    "    max_x_index = x_indices.max() + 1\n",
    "    voxel_keys = z_indices * max_x_index + x_indices\n",
    "\n",
    "    # 2. Find the unique keys and their first occurrence\n",
    "    # `return_index=True` gives the index of the first occurrence of each unique key\n",
    "    unique_keys, unique_indices = np.unique(voxel_keys, return_index=True)\n",
    "    \n",
    "    return unique_indices\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 0: Load, Filter, and Downsample Data (Single Pass)\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    "\n",
    "# Apply initial filtering (z > 0 and magnitude > 0)\n",
    "initial_mask = (points[:, 2] > 0) & (magnitudes > 0)\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    "\n",
    "# Aggressive Downsample (for point cloud, typically N_DOWNSAMPLE_EMAG=1 is best)\n",
    "points_ds = points[::N_DOWNSAMPLE_EMAG]\n",
    "vectors_ds = vectors[::N_DOWNSAMPLE_EMAG]\n",
    "magnitudes_ds = magnitudes[::N_DOWNSAMPLE_EMAG]\n",
    "\n",
    "# Create a PyVista Point Cloud (PolyData)\n",
    "point_cloud = pv.PolyData(points_ds)\n",
    "point_cloud[\"E_mag\"] = magnitudes_ds   # Store log magnitude for visualization\n",
    "point_cloud[\"Ex_val\"] = vectors_ds[:,0] # Store vectors\n",
    "point_cloud[\"Ez_val\"] = vectors_ds[:,2] # Store vectors\n",
    "\n",
    "print(f\"Starting points (filtered by z > 0 & mag > 0): {len(points)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup and Slicing\n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    "\n",
    "# 1a. Load and Crop Geometry\n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    "\n",
    "# Define bounding box based on the downsampled field data\n",
    "bbox_bounds = point_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = pv_spheres.clip_box(bbox, invert=False)\n",
    "\n",
    "# 1b. Define the slice plane (ZX plane, normal along Y)\n",
    "center = (point_cloud.center[0], Y_SLICE, point_cloud.center[2])\n",
    "normal = [0, 1, 0] # ZX plane (normal along Y)\n",
    "\n",
    "# Create a plane mesh for interpolation (this will be the magnitude slice)\n",
    "plane_bounds = [\n",
    "    point_cloud.bounds[0], point_cloud.bounds[1], # X bounds\n",
    "    Y_SLICE, Y_SLICE,                             # Y (fixed)\n",
    "    point_cloud.bounds[4], point_cloud.bounds[5]  # Z bounds\n",
    "]\n",
    "\n",
    "field_slice_mesh = pv.Plane(\n",
    "    center=center, \n",
    "    direction=normal,\n",
    "    j_size=bbox_bounds[1] - bbox_bounds[0], # X span\n",
    "    i_size=bbox_bounds[5] - bbox_bounds[4], # Z span\n",
    "    i_resolution=250, \n",
    "    j_resolution=250\n",
    ")\n",
    "\n",
    "# --- MODIFIED INTERPOLATION CALL FOR NEAREST NEIGHBOR ---\n",
    "field_slice_interpolated = field_slice_mesh.interpolate(\n",
    "    point_cloud,\n",
    "    sharpness=3.0,      # High sharpness often helps with point data\n",
    "    radius=0.001, #1e-12,       # Set radius to near-zero to minimize interpolation\n",
    "    \n",
    "    # 1. Provide a float placeholder to satisfy the TypeError\n",
    "    null_value=1, \n",
    "    \n",
    "    # 2. Force the strategy to use the nearest point (Nearest Neighbor)\n",
    "    strategy='closest_point' # <--- This achieves the extrapolation you want\n",
    ")\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "# Slice the geometry using the plane (more precise than slice_orthogonal)\n",
    "geo_slice = pv_spheres_cropped.slice(normal=normal, origin=field_slice_mesh.center)\n",
    "print(f\"Geometry and slicing preparation complete in {time.time() - start_time_geo:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 2: Vector Field Glyphs (Arrows)\n",
    "# ----------------------------------------------------\n",
    "start_time_vectors = time.time()\n",
    "\n",
    "# 2a. Filter the downsampled points again to extract only those in the slice volume\n",
    "# We use NumPy masking directly on the downsampled data (points_ds)\n",
    "vector_mask = np.abs(points_ds[:, 1] - Y_SLICE) < THICKNESS\n",
    "points_slice_full = points_ds[vector_mask]\n",
    "vectors_slice_full = vectors_ds[vector_mask]\n",
    "magnitudes_slice_full = magnitudes_ds[vector_mask]\n",
    "\n",
    "# 2b. Apply Voxel Downsampling to achieve uniform density\n",
    "unique_indices = voxel_downsample_points(points_slice_full, ARROW_VOXEL_SPACING)\n",
    "\n",
    "points_slice = points_slice_full[unique_indices]\n",
    "vectors_slice = vectors_slice_full[unique_indices]\n",
    "magnitudes_slice = magnitudes_slice_full[unique_indices]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# MODIFICATION: Calculate Clamping Limit and Apply Clamping\n",
    "# ----------------------------------------------------\n",
    "# The maximum allowed length of an arrow is ARROW_VOXEL_SPACING.\n",
    "# The glyph length = magnitude * VECTOR_SCALE_FACTOR * arrow_length_in_geom (which is 1.0 for pv.Arrow).\n",
    "# To ensure: glyph_length <= ARROW_VOXEL_SPACING\n",
    "# We need: magnitude * VECTOR_SCALE_FACTOR <= ARROW_VOXEL_SPACING\n",
    "# Therefore: magnitude_clamped <= ARROW_VOXEL_SPACING / VECTOR_SCALE_FACTOR\n",
    "\n",
    "# Define the maximum magnitude allowed\n",
    "MAGNITUDE_MAX_CLAMP = ARROW_VOXEL_SPACING / VECTOR_SCALE_FACTOR /2\n",
    "\n",
    "# Apply the clamping (upper bound) to the magnitude array\n",
    "magnitudes_slice_clamped = np.clip(magnitudes_slice, a_min=None, a_max=MAGNITUDE_MAX_CLAMP)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# 2c. Create a PolyData object for glyphs\n",
    "points_slice[:,1] = Y_SLICE - 2*THICKNESS# Force y-coordinate to the slice plane for visualization\n",
    "vectors_slice[:,1] = 0.0 - 2* THICKNESS# Zero out Y component for 2D slice visualization\n",
    "slice_mesh_vectors = pv.PolyData(points_slice)\n",
    "slice_mesh_vectors['vectors'] = vectors_slice\n",
    "# Use the CLAMPED magnitude array for scaling\n",
    "slice_mesh_vectors['magnitude'] = magnitudes_slice_clamped\n",
    "#slice_mesh_vectors['magnitude'] = np.log10(magnitudes_slice)\n",
    "\n",
    "# # 2c. Create a PolyData object for glyphs\n",
    "# points_slice[:,1] = Y_SLICE - 2*THICKNESS# Force y-coordinate to the slice plane for visualization\n",
    "# vectors_slice[:,1] = 0.0 - 2* THICKNESS# Zero out Y component for 2D slice visualization\n",
    "# slice_mesh_vectors = pv.PolyData(points_slice)\n",
    "# slice_mesh_vectors['vectors'] = vectors_slice\n",
    "# #slice_mesh_vectors['magnitude'] = np.log10(magnitudes_slice)\n",
    "# slice_mesh_vectors['magnitude'] = magnitudes_slice\n",
    "\n",
    "print(f\"Points in vector slice (after density control): {len(points_slice)}, old length: {len(points_slice_full)}...\")\n",
    "\n",
    "# 2d. Create the glyphs\n",
    "arrow = pv.Arrow(tip_length=0.3, tip_radius=0.2, shaft_radius=0.04)\n",
    "glyphs = slice_mesh_vectors.glyph(\n",
    "    orient='vectors',\n",
    "    scale='magnitude',\n",
    "    factor=VECTOR_SCALE_FACTOR,\n",
    "    geom=arrow\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 3: Visualization\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"Ex_val\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    opacity=1,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # <-- set fixed color range here\n",
    "    # --- COLORBAR POSITIONING FIX ---\n",
    "    scalar_bar_args={\n",
    "        'title':None, # r'log$_{10}$(E$_{mag}$)', # Updated title format\n",
    "        'vertical': False,            # Make it horizontal\n",
    "        'position_x': 0.20,           # User-specified start position\n",
    "        'position_y': 0.12,           # User-specified vertical position\n",
    "        'width': 0.6,                 # User-specified width\n",
    "        'height': 0.05,               # User-specified height\n",
    "    }\n",
    "    # -------------------------------\n",
    ")\n",
    "\n",
    "# Add sliced geometry (outline only)\n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=5,opacity=0.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='black', show_scalar_bar=False, line_width=4,opacity=1)\n",
    "\n",
    "# # Optional marker\n",
    "sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "pl.add_mesh(sphere, color=\"red\", opacity=0.5)\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "\n",
    "# Align camera perpendicular to the slice\n",
    "pl.view_xz() \n",
    "\n",
    "# --- ADD THIS LINE BEFORE pl.show() ---\n",
    "pl.screenshot(f'figures/fieldvectors_iteration{iteration}.jpeg', scale=4)\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static') #jupyter_backend='static'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldIN = df[iteration]\n",
    "\n",
    "FIELD_AVERAGE_RADIUS = 2e-3\n",
    "vmin, vmax = (-0.1, 0.1) # in log(E_mag) units\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) # \n",
    "red_point = np.array([0.1, 0., 0.1 - 0.015 + 0.037]) # \n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 0: Load, Filter, and Downsample Data (Single Pass)\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    "\n",
    "# Apply initial filtering (z > 0 and magnitude > 0)\n",
    "initial_mask = (points[:, 2] > 0) & (magnitudes > 0)\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    "\n",
    "# --- Step: Interpolate Electric Field onto Surface and Compute Pressure ---\n",
    "epsilon_0 = 8.854187817e-12  # Vacuum permittivity\n",
    " \n",
    "# Interpolate full vector field onto surface geometry\n",
    "field_cloud = pv.PolyData(points)\n",
    "field_cloud[\"E_x\"] = vectors[:, 0]\n",
    "field_cloud[\"E_y\"] = vectors[:, 1]\n",
    "field_cloud[\"E_z\"] = vectors[:, 2]\n",
    "\n",
    "print(f\"Starting points (filtered by z > 0 & mag > 0): {len(points)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup and Slicing \n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    "\n",
    "# 1a. Load and Crop Geometry \n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    "bbox_bounds = field_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = (\n",
    "    pv_spheres\n",
    "    .clip_box(bbox, invert=False)\n",
    "    .extract_surface()\n",
    "    .compute_normals(point_normals=True, cell_normals=True, inplace=False)\n",
    ")\n",
    "\n",
    "# Interpolate onto surface mesh\n",
    "pv_spheres_interp = pv_spheres_cropped.interpolate(\n",
    "    field_cloud,\n",
    "    radius=0.002,\n",
    "    strategy='closest_point',\n",
    "    sharpness=3.0,\n",
    "    null_value=0.0\n",
    ")\n",
    " \n",
    "# Extract interpolated field and surface normals\n",
    "E_vec = np.stack([pv_spheres_interp[\"E_x\"], pv_spheres_interp[\"E_y\"], pv_spheres_interp[\"E_z\"]], axis=1)\n",
    "#E_vec = np.stack([pv_spheres_interp[\"E_x\"], np.array([0]*len(pv_spheres_interp[\"E_x\"])),np.array([0]*len(pv_spheres_interp[\"E_x\"]))], axis=1)\n",
    " \n",
    "# Compute dot product and magnitude squared\n",
    "E_dot_n = np.einsum('ij,ij->i', E_vec, pv_spheres_interp.point_normals)\n",
    "E_mag_sq = np.einsum('ij,ij->i', E_vec, E_vec)\n",
    " \n",
    "# Maxwell stress tensor projection: normal pressure\n",
    "electric_pressure = epsilon_0 * (E_dot_n**2 - 0.5 * E_mag_sq)\n",
    "#electric_pressure = epsilon_0 * E_mag_sq # (E_dot_n**2) # - 0.5 * E_mag_sq)\n",
    "pv_spheres_interp[\"electric_pressure\"] = electric_pressure\n",
    "\n",
    "print(f\"Geometry and slicing preparation complete in {time.time() - start_time_geo:.2f}s\")\n",
    "\n",
    "\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    pv_spheres_interp,\n",
    "    scalars=\"electric_pressure\",\n",
    "    cmap=\"seismic\",\n",
    "    opacity=1,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # Use the defined log range\n",
    "    interpolate_before_map=False,\n",
    "    scalar_bar_args={\n",
    "        # Title updated to reflect the log-magnitude plot, matching the data\n",
    "        'title': None, #r'log$_{10}$(E$_{mag}$)', \n",
    "        'vertical': False,            \n",
    "        'position_x': 0.22,           \n",
    "        'position_y': 0.16,           \n",
    "        'width': 0.6,                 \n",
    "        'height': 0.05,               \n",
    "    }\n",
    ")\n",
    "\n",
    "# Define camera: (position, focal_point, view_up)\n",
    "pl.camera_position = [(-0.12166342033912077, -0.3800808894298424, 0.29897113376370393),\n",
    " (0.0, 0.0, 0.09238091282895766),\n",
    " (-0.08325442944763095, 0.4963396747392208, 0.8641270897600156)]\n",
    "pl.camera_position =[(0.09973232154642885, -0.3416433894224343, 0.3806928562326606),\n",
    "  (0.0, 0.0, 0.09238091282895766),\n",
    "  (-0.13524196040858827, 0.6156566616046678, 0.776322411866769)]\n",
    "\n",
    "# # Optional marker\n",
    "sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "pl.add_mesh(sphere, color=\"black\", opacity=0.5)\n",
    "\n",
    "#pl.view_isometric()  # nice angled 3D view\n",
    "# or:\n",
    "#pl.view_xy()  # top-down (Z up)\n",
    "#pl.view_yz()  # side view\n",
    "#pl.view_xz()  # front view\n",
    "\n",
    "pl.show(jupyter_backend='static') #jupyter_backend='static'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Convert\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    pv_spheres_interp,\n",
    "    scalars=\"electric_pressure\",\n",
    "    cmap=\"seismic\",\n",
    "    opacity=1,\n",
    "    clim=[-0.1, 0.1],   # Use the defined log range\n",
    "    interpolate_before_map=False,\n",
    "    show_edges=False,\n",
    "    scalar_bar_args={\n",
    "        # Title updated to reflect the log-magnitude plot, matching the data\n",
    "        'title': None, #r'log$_{10}$(E$_{mag}$)', \n",
    "        'vertical': False,            \n",
    "        'position_x': 0.22,           \n",
    "        'position_y': 0.16,           \n",
    "        'width': 0.6,                 \n",
    "        'height': 0.05,               \n",
    "    }\n",
    ")\n",
    "\n",
    "# Define camera: (position, focal_point, view_up)\n",
    "\n",
    "# pl.camera_position =[(0.09973232154642885, -0.3416433894224343, 0.3806928562326606),\n",
    "#  (0.0, 0.0, 0.09238091282895766),\n",
    "#  (-0.13524196040858827, 0.6156566616046678, 0.776322411866769)]\n",
    "\n",
    "pl.camera_position = [(0.1363737593585825, -0.11683469032939907, 0.47247186847252504),\n",
    " (0.0, 0.0, 0.09238091282895766),\n",
    " (-0.14259264054382748, 0.9305696518359464, 0.3372053705750612)]\n",
    "\n",
    "# pl.camera_position= [(-0.12799331102842687, -0.2754039931467276, 0.29686110267708105),\n",
    "#  (0.0, 0.0, 0.09238091282895766),\n",
    "#  (-0.021089755556589956, 0.6022928135020578, 0.797996609650905)]\n",
    "\n",
    "# # Optional marker\n",
    "# sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "# pl.add_mesh(sphere, color=\"black\", opacity=0.5)\n",
    "\n",
    "#pl.view_isometric()  # nice angled 3D view\n",
    "# or:\n",
    "#pl.view_xy()  # top-down (Z up)\n",
    "#pl.view_yz()  # side view\n",
    "#pl.view_xz()  # front view\n",
    "\n",
    "pl.show(jupyter_backend='static') #jupyter_backend='static'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Enable LaTeX rendering\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Set the global font size\n",
    "mpl.rcParams.update({'font.size': 12})\n",
    "\n",
    "cmap = plt.cm.YlGnBu \n",
    "vmin, vmax = (-2e5, 2e5) # in log(E_mag) units\n",
    "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 0.1))\n",
    "\n",
    "# --- 1. Create and Configure the ScalarFormatter ---\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "\n",
    "formatter.set_useOffset(False) \n",
    "formatter.set_powerlimits((0, 0)) \n",
    "\n",
    "# --- 2. Create the Colorbar and apply the Formatter ---\n",
    "cb = mpl.colorbar.ColorbarBase(\n",
    "    ax, \n",
    "    cmap=cmap, \n",
    "    norm=norm, \n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Apply the formatter to the colorbar's x-axis\n",
    "cb.ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# --- 3. Display the Plot ---\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS HERE ARE OPTIMIZED FOR ITERATION 1 ##\n",
    "\n",
    "N_DOWNSAMPLE_EMAG = 1\n",
    "N_DOWNSAMPLE_BASE = 500 # This is the initial downsample applied to the full field before slicing\n",
    "\n",
    "vmin, vmax = (-2e5, 2e5) #(-2e4, 2e4) \n",
    "VECTOR_SCALE_FACTOR = 1e-7 #2e-6 #5e-6 # Global scaling for glyphs\n",
    "\n",
    "Y_SLICE = 0.0\n",
    "THICKNESS = 0.002 # Original, thin slice thickness\n",
    "EXCLUSION_DISTANCE = 0.011\n",
    "\n",
    "FIELD_AVERAGE_RADIUS = 2e-3\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015+ 0.039]) #  \n",
    "LOG_MAG_NULL_VALUE = 1.0 \n",
    "\n",
    "fieldIN = df[iteration]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 0: Load, Filter, and Downsample Data (Single Pass)\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    "\n",
    "# Apply initial filtering (z > 0 and magnitude > 0)\n",
    "initial_mask = (points[:, 2] > 0) & (magnitudes > 0)\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    "\n",
    "if N_DOWNSAMPLE_EMAG > 1:\n",
    "    print(f\"Applying downsample of {N_DOWNSAMPLE_EMAG} to E_mag data.\")\n",
    "    #  Downsample (Base Downsample)\n",
    "    points = points[::N_DOWNSAMPLE_EMAG]\n",
    "    vectors = vectors[::N_DOWNSAMPLE_EMAG]\n",
    "    magnitudes = magnitudes[::N_DOWNSAMPLE_EMAG]\n",
    "\n",
    "# Create a PyVista Point Cloud (PolyData)\n",
    "point_cloud = pv.PolyData(points)\n",
    "point_cloud[\"E_mag\"] = np.log10(magnitudes) # Store log magnitude for visualization\n",
    "point_cloud[\"E_vec\"] = vectors[:,0]             # Store vectors (E_x component)\n",
    "\n",
    "print(f\"Starting points (filtered by z > 0 & mag > 0): {len(points)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup and Slicing \n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    "\n",
    "# 1a. Load and Crop Geometry \n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    "bbox_bounds = point_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = pv_spheres.clip_box(bbox, invert=False)\n",
    "\n",
    "# 1b. Define the slice plane\n",
    "center = (point_cloud.center[0], Y_SLICE, point_cloud.center[2])\n",
    "normal = [0, 1, 0] # ZX plane (normal along Y)\n",
    "\n",
    "plane_bounds = [\n",
    "    point_cloud.bounds[0], point_cloud.bounds[1], # X bounds\n",
    "    Y_SLICE, Y_SLICE,                             # Y (fixed)\n",
    "    point_cloud.bounds[4], point_cloud.bounds[5]  # Z bounds\n",
    "]\n",
    "\n",
    "field_slice_mesh = pv.Plane(\n",
    "    center=center, \n",
    "    direction=normal,\n",
    "    j_size=bbox_bounds[1] - bbox_bounds[0], \n",
    "    i_size=bbox_bounds[5] - bbox_bounds[4], \n",
    "    i_resolution=250, \n",
    "    j_resolution=250\n",
    ")\n",
    "\n",
    "# --- MODIFIED INTERPOLATION CALL FOR NEAREST NEIGHBOR ---\n",
    "field_slice_interpolated = field_slice_mesh.interpolate(\n",
    "    point_cloud,\n",
    "    sharpness=3.0,      # High sharpness often helps with point data\n",
    "    radius=0.002, #1e-12,       # Set radius to near-zero to minimize interpolation\n",
    "    \n",
    "    # 1. Provide a float placeholder to satisfy the TypeError\n",
    "    null_value=1, \n",
    "    \n",
    "    # 2. Force the strategy to use the nearest point (Nearest Neighbor)\n",
    "    strategy='closest_point' # <--- This achieves the extrapolation you want\n",
    ")\n",
    "\n",
    "geo_slice = pv_spheres_cropped.slice(normal=normal, origin=field_slice_mesh.center)\n",
    "print(f\"Geometry and slicing preparation complete in {time.time() - start_time_geo:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## Step 2: Filter Data Based on Distance to geo_slice\n",
    "# ----------------------------------------------------\n",
    "start_time_filter = time.time()\n",
    "\n",
    "# --- 2a. Thin Slice Filtering ---\n",
    "vector_mask = np.abs(points[:, 1] - Y_SLICE) < THICKNESS\n",
    "points_slice = points[vector_mask]\n",
    "vectors_slice = vectors[vector_mask]\n",
    "magnitudes_slice = magnitudes[vector_mask]\n",
    "\n",
    "# Aggressive Downsample (Base Downsample)\n",
    "points_slice_ds = points_slice[::N_DOWNSAMPLE_BASE]\n",
    "vectors_slice_ds = vectors_slice[::N_DOWNSAMPLE_BASE]\n",
    "magnitudes_slice_ds = magnitudes_slice[::N_DOWNSAMPLE_BASE]\n",
    "\n",
    "# 2. Find the distance to the nearest neighbor in geo_slice for every point in points_ds\n",
    "# We use query with k=1 (nearest neighbor)\n",
    "tree = cKDTree(geo_slice.points)\n",
    "distance_to_geometry, _ = tree.query(points_slice_ds, k=1)\n",
    "\n",
    "# 3. Create a mask: keep only points whose distance is greater than the threshold\n",
    "keep_mask = distance_to_geometry > EXCLUSION_DISTANCE\n",
    "\n",
    "# 4. Apply the mask to the base data arrays\n",
    "points_slice_ds = points_slice_ds[keep_mask]\n",
    "vectors_slice_ds = vectors_slice_ds[keep_mask]\n",
    "magnitudes_slice_ds = magnitudes_slice_ds[keep_mask]          \n",
    "\n",
    "# 2b. Create PolyData for thin glyphs\n",
    "#points_slice_ds[:,1] = Y_SLICE  -THICKNESS*2# Force y-coordinate to the slice plane for visualization\n",
    "slice_mesh_vectors_noRim = pv.PolyData(points_slice_ds)\n",
    "slice_mesh_vectors_noRim['vectors'] = vectors_slice_ds\n",
    "slice_mesh_vectors_noRim['magnitude'] = magnitudes_slice_ds\n",
    "\n",
    "# 2c. Create the THIN glyphs\n",
    "arrow = pv.Arrow(tip_length=0.2, tip_radius=0.08, shaft_radius=0.02)\n",
    "glyphs = slice_mesh_vectors_noRim.glyph(\n",
    "    orient='vectors',\n",
    "    scale='magnitude',\n",
    "    factor=VECTOR_SCALE_FACTOR,\n",
    "    geom=arrow\n",
    ")\n",
    "\n",
    "print(f\"Distance filtering removed {len(keep_mask) - len(points_slice_ds)} points. New length: {len(points_slice_ds)}\")\n",
    "print(f\"Vector field preparation complete in {time.time() - start_time_filter:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## Step 3: Visualization ðŸ“Š\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"E_vec\",\n",
    "    cmap=\"viridis\",\n",
    "    opacity=0.9,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # Use the defined log range\n",
    "    scalar_bar_args={\n",
    "        # Title updated to reflect the log-magnitude plot, matching the data\n",
    "        'title': None, #r'log$_{10}$(E$_{mag}$)', \n",
    "        'vertical': False,            \n",
    "        'position_x': 0.22,           \n",
    "        'position_y': 0.16,           \n",
    "        'width': 0.6,                 \n",
    "        'height': 0.05,               \n",
    "    }\n",
    ")\n",
    "\n",
    "# Add sliced geometry \n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=2.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='white', show_scalar_bar=False)\n",
    "#pl.add_mesh(glyphs, show_scalar_bar=False, cmap='coolwarm') # color='white', show_scalar_bar=False)\n",
    "\n",
    "# Optional marker \n",
    "#sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "#pl.add_mesh(sphere, color=\"red\", opacity=0.5)\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "pl.view_xz()\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS HERE ARE OPTIMIZED FOR ITERATION 86 ##\n",
    "\n",
    "N_DOWNSAMPLE_EMAG = 1\n",
    "N_DOWNSAMPLE_BASE = 500 # This is the initial downsample applied to the full field before slicing\n",
    "\n",
    "vmin, vmax = (-2e5, 2e5) \n",
    "VECTOR_SCALE_FACTOR = 1e-7 # Global scaling for glyphs\n",
    "\n",
    "Y_SLICE = 0.0\n",
    "THICKNESS = 0.002 # Original, thin slice thickness\n",
    "EXCLUSION_DISTANCE = 0.011\n",
    "\n",
    "FIELD_AVERAGE_RADIUS = 2e-3\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) # \n",
    "LOG_MAG_NULL_VALUE = 1.0 \n",
    "\n",
    "fieldIN = df[iteration]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 0: Load, Filter, and Downsample Data (Single Pass)\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    "\n",
    "# Apply initial filtering (z > 0 and magnitude > 0)\n",
    "initial_mask = (points[:, 2] > 0) & (magnitudes > 0)\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    "\n",
    "if N_DOWNSAMPLE_EMAG > 1:\n",
    "    print(f\"Applying downsample of {N_DOWNSAMPLE_EMAG} to E_mag data.\")\n",
    "    #  Downsample (Base Downsample)\n",
    "    points = points[::N_DOWNSAMPLE_EMAG]\n",
    "    vectors = vectors[::N_DOWNSAMPLE_EMAG]\n",
    "    magnitudes = magnitudes[::N_DOWNSAMPLE_EMAG]\n",
    "\n",
    "# Create a PyVista Point Cloud (PolyData)\n",
    "point_cloud = pv.PolyData(points)\n",
    "point_cloud[\"E_mag\"] = np.log10(magnitudes) # Store log magnitude for visualization\n",
    "point_cloud[\"E_vec\"] = vectors[:,0]             # Store vectors (E_x component)\n",
    "\n",
    "print(f\"Starting points (filtered by z > 0 & mag > 0): {len(points)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup and Slicing \n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    "\n",
    "# 1a. Load and Crop Geometry \n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    "bbox_bounds = point_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = pv_spheres.clip_box(bbox, invert=False)\n",
    "\n",
    "# 1b. Define the slice plane\n",
    "center = (point_cloud.center[0], Y_SLICE, point_cloud.center[2])\n",
    "normal = [0, 1, 0] # ZX plane (normal along Y)\n",
    "\n",
    "plane_bounds = [\n",
    "    point_cloud.bounds[0], point_cloud.bounds[1], # X bounds\n",
    "    Y_SLICE, Y_SLICE,                             # Y (fixed)\n",
    "    point_cloud.bounds[4], point_cloud.bounds[5]  # Z bounds\n",
    "]\n",
    "\n",
    "field_slice_mesh = pv.Plane(\n",
    "    center=center, \n",
    "    direction=normal,\n",
    "    j_size=bbox_bounds[1] - bbox_bounds[0], \n",
    "    i_size=bbox_bounds[5] - bbox_bounds[4], \n",
    "    i_resolution=250, \n",
    "    j_resolution=250\n",
    ")\n",
    "\n",
    "# --- MODIFIED INTERPOLATION CALL FOR NEAREST NEIGHBOR ---\n",
    "field_slice_interpolated = field_slice_mesh.interpolate(\n",
    "    point_cloud,\n",
    "    sharpness=3.0,      # High sharpness often helps with point data\n",
    "    radius=0.002, #1e-12,       # Set radius to near-zero to minimize interpolation\n",
    "    \n",
    "    # 1. Provide a float placeholder to satisfy the TypeError\n",
    "    null_value=1, \n",
    "    \n",
    "    # 2. Force the strategy to use the nearest point (Nearest Neighbor)\n",
    "    strategy='closest_point' # <--- This achieves the extrapolation you want\n",
    ")\n",
    "\n",
    "geo_slice = pv_spheres_cropped.slice(normal=normal, origin=field_slice_mesh.center)\n",
    "print(f\"Geometry and slicing preparation complete in {time.time() - start_time_geo:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## Step 2: Filter Data Based on Distance to geo_slice\n",
    "# ----------------------------------------------------\n",
    "start_time_filter = time.time()\n",
    "\n",
    "# --- 2a. Thin Slice Filtering ---\n",
    "vector_mask = np.abs(points[:, 1] - Y_SLICE) < THICKNESS\n",
    "points_slice = points[vector_mask]\n",
    "vectors_slice = vectors[vector_mask]\n",
    "magnitudes_slice = magnitudes[vector_mask]\n",
    "\n",
    "# Aggressive Downsample (Base Downsample)\n",
    "points_slice_ds = points_slice[::N_DOWNSAMPLE_BASE]\n",
    "vectors_slice_ds = vectors_slice[::N_DOWNSAMPLE_BASE]\n",
    "magnitudes_slice_ds = magnitudes_slice[::N_DOWNSAMPLE_BASE]\n",
    "\n",
    "# 2. Find the distance to the nearest neighbor in geo_slice for every point in points_ds\n",
    "# We use query with k=1 (nearest neighbor)\n",
    "tree = cKDTree(geo_slice.points)\n",
    "distance_to_geometry, _ = tree.query(points_slice_ds, k=1)\n",
    "\n",
    "# 3. Create a mask: keep only points whose distance is greater than the threshold\n",
    "keep_mask = distance_to_geometry > EXCLUSION_DISTANCE\n",
    "\n",
    "# 4. Apply the mask to the base data arrays\n",
    "points_slice_ds = points_slice_ds[keep_mask]\n",
    "vectors_slice_ds = vectors_slice_ds[keep_mask]\n",
    "magnitudes_slice_ds = magnitudes_slice_ds[keep_mask]          \n",
    "\n",
    "# 2b. Create PolyData for thin glyphs\n",
    "points_slice_ds[:,1] = Y_SLICE  # Force y-coordinate to the slice plane for visualization\n",
    "slice_mesh_vectors_noRim = pv.PolyData(points_slice_ds)\n",
    "slice_mesh_vectors_noRim['vectors'] = vectors_slice_ds\n",
    "slice_mesh_vectors_noRim['magnitude'] = magnitudes_slice_ds\n",
    "\n",
    "# 2c. Create the THIN glyphs\n",
    "arrow = pv.Arrow(tip_length=0.2, tip_radius=0.08, shaft_radius=0.01)\n",
    "glyphs = slice_mesh_vectors_noRim.glyph(\n",
    "    orient='vectors',\n",
    "    scale='magnitude',\n",
    "    factor=VECTOR_SCALE_FACTOR,\n",
    "    geom=arrow\n",
    ")\n",
    "\n",
    "print(f\"Distance filtering removed {len(keep_mask) - len(points_slice_ds)} points. New length: {len(points_slice_ds)}\")\n",
    "print(f\"Vector field preparation complete in {time.time() - start_time_filter:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## Step 3: Visualization ðŸ“Š\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"E_vec\",\n",
    "    cmap=\"viridis\",\n",
    "    opacity=0.9,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # Use the defined log range\n",
    "    scalar_bar_args={\n",
    "        # Title updated to reflect the log-magnitude plot, matching the data\n",
    "        'title': None, #r'log$_{10}$(E$_{mag}$)', \n",
    "        'vertical': False,            \n",
    "        'position_x': 0.22,           \n",
    "        'position_y': 0.16,           \n",
    "        'width': 0.6,                 \n",
    "        'height': 0.05,               \n",
    "    }\n",
    ")\n",
    "\n",
    "# Add sliced geometry \n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=2.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='white', show_scalar_bar=False)\n",
    "#pl.add_mesh(glyphs, show_scalar_bar=False, cmap='coolwarm') # color='white', show_scalar_bar=False)\n",
    "\n",
    "# Optional marker \n",
    "sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "pl.add_mesh(sphere, color=\"red\", opacity=0.5)\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "pl.view_xz()\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static') #jupyter_backend='static'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def filter_coordinates(\n",
    "    df: pd.DataFrame, \n",
    "    column_name: str, \n",
    "    x_range: Tuple[float, float], \n",
    "    y_range: Tuple[float, float], \n",
    "    z_range: Tuple[float, float]\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Filters a DataFrame Series containing coordinate strings based on specified \n",
    "    (x, y, z) ranges.\n",
    "\n",
    "    The coordinates in the column_name are expected to be formatted as strings \n",
    "    like \"(x, y, z)\".\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column_name (str): The name of the column containing the coordinate strings.\n",
    "        x_range (Tuple[float, float]): (min, max) for the x-coordinate filter.\n",
    "        y_range (Tuple[float, float]): (min, max) for the y-coordinate filter.\n",
    "        z_range (Tuple[float, float]): (min, max) for the z-coordinate filter.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A pandas Series containing only the coordinate strings that\n",
    "                   fall within all three specified ranges.\n",
    "    \"\"\"\n",
    "    \n",
    "    coords = np.vstack( df[column_name])\n",
    "    \n",
    "    # X-Filter: x is greater than x_min AND x is less than x_max\n",
    "    x_min, x_max = x_range\n",
    "    x_filter = (coords[:,0] > x_min) & (coords[:,0]  < x_max)\n",
    "\n",
    "    # Y-Filter: y is greater than y_min AND y is less than y_max\n",
    "    y_min, y_max = y_range\n",
    "    y_filter = (coords[:,1] > y_min) & (coords[:,1]  < y_max)\n",
    "\n",
    "    # Z-Filter: z is greater than z_min AND z is less than z_max\n",
    "    z_min, z_max = z_range\n",
    "    z_filter = (coords[:,2] > z_min) & (coords[:,2]  < z_max)\n",
    "\n",
    "    # --- 3. Combine the filters and apply to the original series ---\n",
    "    combined_filter = x_filter & y_filter & z_filter\n",
    "\n",
    "    # Return the original coordinate strings that match the combined filter\n",
    "    return df[column_name][combined_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(all_gamma_holes_df[\"Post_Step_Position_mm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# Define the filtering ranges based on your request\n",
    "X_RANGE = (plane_bounds[0],plane_bounds[1])\n",
    "Y_RANGE = (-THICKNESS/2, THICKNESS/2)\n",
    "Z_RANGE = (plane_bounds[4],plane_bounds[5])\n",
    "\n",
    "# Call the function with the sample data and required ranges\n",
    "filtered_series_gamma = filter_coordinates(df=all_gamma_holes_df,column_name=\"Post_Step_Position_mm\",\n",
    "    x_range=X_RANGE,y_range=Y_RANGE,z_range=Z_RANGE)\n",
    "print(f\"\\nTotal points found: {len(filtered_series_gamma)}, starting length: {len(all_gamma_holes_df)}\")\n",
    "\n",
    "# Call the function with the sample data and required ranges\n",
    "filtered_series_electron = filter_coordinates(df=all_electrons_inside_df,column_name=\"Post_Step_Position_mm\",\n",
    "    x_range=X_RANGE,y_range=Y_RANGE,z_range=Z_RANGE)\n",
    "print(f\"\\nTotal points found: {len(filtered_series_electron)}, starting length: {len(all_electrons_inside_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "## Step 3: Visualization ðŸ“Š\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"E_vec\",\n",
    "    cmap=\"viridis\",\n",
    "    opacity=0.9,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # Use the defined log range\n",
    "    scalar_bar_args={\n",
    "        # Title updated to reflect the log-magnitude plot, matching the data\n",
    "        'title': None, #r'log$_{10}$(E$_{mag}$)', \n",
    "        'vertical': False,            \n",
    "        'position_x': 0.22,           \n",
    "        'position_y': 0.16,           \n",
    "        'width': 0.6,                 \n",
    "        'height': 0.05,               \n",
    "    }\n",
    ")\n",
    "\n",
    "# Add sliced geometry \n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=2.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='white', show_scalar_bar=False)\n",
    "#pl.add_mesh(glyphs, show_scalar_bar=False, cmap='coolwarm') # color='white', show_scalar_bar=False)\n",
    "\n",
    "# Optional marker \n",
    "sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "pl.add_mesh(sphere, color=\"red\", opacity=0.5)\n",
    "\n",
    "\n",
    "# # Add the single PolyData mesh\n",
    "# pl.add_mesh(\n",
    "#     pv.PolyData(np.vstack(filtered_series_gamma)),\n",
    "#     color='red',\n",
    "#     opacity=0.9,\n",
    "#     # Key arguments for fast point visualization:\n",
    "#     render_points_as_spheres=True,  # Makes them look like spheres\n",
    "#     point_size=3,                  # Controls the size of the 'spheres'\n",
    "#     # The 'point_size' units are in screen pixels by default\n",
    "# )\n",
    "\n",
    "# Add the single PolyData mesh\n",
    "pl.add_mesh(\n",
    "    pv.PolyData(np.vstack(filtered_series_electron)),\n",
    "    color='red',\n",
    "    opacity=0.9,\n",
    "    # Key arguments for fast point visualization:\n",
    "    render_points_as_spheres=True,  # Makes them look like spheres\n",
    "    point_size=3,                  # Controls the size of the 'spheres'\n",
    "    # The 'point_size' units are in screen pixels by default\n",
    ")\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "pl.view_xz()\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static') #jupyter_backend='static'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "directory_path =  \"../build-smallerworld-initial8max0.8final12/root/\" #\"../build-adaptive-barns-fixed/root/\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*.root\"))\n",
    "\n",
    "all_gamma_holes = []\n",
    "all_electrons_inside = []\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    if iterationNUM > 5:\n",
    "        break\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"gamma_holes_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)], _ = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path),\n",
    "                                                                                                             config=configIN)\n",
    "    \n",
    "    # surf, ilm_values = plot_face_illumination(vars()[\"gamma_holes_\"+str(number_str)], stacked_spheres, vmin=0, vmax=1)\n",
    "    # # Make sure each triangle has its own unique vertices\n",
    "    # surface_edited = surface.copy()\n",
    "    # surface_edited.unmerge_vertices()\n",
    "    # surface_edited.visual.vertex_colors = None\n",
    "    # surface_edited.show()\n",
    "\n",
    "    all_gamma_holes.append(vars()[\"gamma_holes_\"+str(number_str)])\n",
    "    all_electrons_inside.append(vars()[\"electrons_inside_\"+str(number_str)])\n",
    "\n",
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gamma_holes_df[\"Post_Step_Position_mm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the field to get a sense for the overall field values before analyzing the field in detail\n",
    "for iteration, thresholdIN in zip(df.keys(), [93.6167, 101.242, 220.685]):\n",
    "    length = len(df[iteration][\"E_mag\"])\n",
    "    plt.hist(df[iteration][\"E_mag\"][df[iteration][\"E_mag\"]>0],bins=np.logspace(0,8,100),alpha=0.5,label=f\"{iteration}: total leaves ={length}\")\n",
    "    plt.axvline(x=thresholdIN*1e3, linestyle=\":\")\n",
    "#plt.hist(df2[df2[\"E_mag\"]>0][\"E_mag\"],bins=np.logspace(-10,8,100),alpha=0.2,label=\"Iteration 78\")\n",
    "#plt.axvline(x=3e2,color=\"k\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"|E| (V/m)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Temperature: 425 K\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Scene Initialization and Data Loading ---\n",
    "\n",
    "# Retrieve the field data dictionary for the current iteration (assumes 'df' is a list/dict)\n",
    "fieldIN = df[iteration]\n",
    "# Define the threshold for filtering electric field magnitude (E_mag)\n",
    "threshold = 1e3 \n",
    "# Set the maximum number of arrows to plot to maintain performance\n",
    "max_arrows = 5000\n",
    "\n",
    "# Initialize the 3D scene by plotting the geometry (e.g., detector structure)\n",
    "# 'stacked_spheres' is the geometry to plot.\n",
    "# 'edge_color' is set to black with an alpha (transparency) value of 350 (out of 511 max for trimesh visual.face_colors/edge_colors)\n",
    "scene = plot_trimesh_edges_only(stacked_spheres, edge_color=[0, 0, 0, 350]) \n",
    "\n",
    "## ----------------------------------------------------\n",
    "## --- 2. Filtering and Sampling High-Magnitude Points ---\n",
    "## ----------------------------------------------------\n",
    "\n",
    "# --- Filtering ---\n",
    "# Create a boolean mask for all points where E_mag exceeds the threshold\n",
    "large_magnitude_mask = fieldIN['E_mag'] > threshold\n",
    "\n",
    "# Apply the mask to all three arrays ('pos', 'E', 'E_mag') simultaneously\n",
    "# to create a new dictionary containing only the high-field points.\n",
    "field_largevalues_masked = {\n",
    "    'pos': fieldIN['pos'][large_magnitude_mask],\n",
    "    'E': fieldIN['E'][large_magnitude_mask],\n",
    "    'E_mag': fieldIN['E_mag'][large_magnitude_mask]\n",
    "}\n",
    "\n",
    "# Get the count of data points after initial filtering\n",
    "N_large = len(field_largevalues_masked['E_mag'])\n",
    "\n",
    "# --- Sampling ---\n",
    "if N_large > max_arrows:\n",
    "    \n",
    "    # Randomly select a subset of indices to stay below the 'max_arrows' limit.\n",
    "    # np.random.choice is the NumPy equivalent of a DataFrame's .sample() method.\n",
    "    np.random.seed(42) # Set seed for reproducible sampling\n",
    "    sample_indices = np.random.choice(\n",
    "        N_large,         # Range of indices to choose from (0 to N_large - 1)\n",
    "        size=max_arrows, # The number of indices to select\n",
    "        replace=False    # Ensure each index is chosen only once\n",
    "    )\n",
    "    \n",
    "    # Apply the sample indices to all arrays to create the final data dictionary for plotting\n",
    "    field_plot = {\n",
    "        'pos': field_largevalues_masked['pos'][sample_indices],\n",
    "        'E': field_largevalues_masked['E'][sample_indices],\n",
    "        'E_mag': field_largevalues_masked['E_mag'][sample_indices]\n",
    "    }\n",
    "    \n",
    "    print(f\"Sampled down to {max_arrows} points from {N_large} large-magnitude points.\")\n",
    "\n",
    "else:\n",
    "    # If the filtered data is small enough, use it directly without sampling\n",
    "    field_plot = field_largevalues_masked\n",
    "    print(f\"Used all {N_large} large-magnitude points for plotting.\")\n",
    "\n",
    "\n",
    "## ----------------------------------------------------\n",
    "## --- 3. Normalization and Coloring Setup ---\n",
    "## ----------------------------------------------------\n",
    "\n",
    "# Get the vector field directions and magnitudes for the sampled points\n",
    "directions = field_plot[\"E\"]\n",
    "\n",
    "# Calculate unit vectors (normalized directions)\n",
    "# Use np.where to prevent division by zero if E_mag is exactly zero (though it shouldn't be after filtering)\n",
    "directions_unit = np.where(\n",
    "    field_plot[\"E_mag\"][:, None] > 0, \n",
    "    directions / field_plot[\"E_mag\"][:, None], \n",
    "    0\n",
    ")\n",
    "\n",
    "# Use Logarithmic scaling for magnitude visualization\n",
    "# Add a small epsilon (1e-12) before log to avoid log(0), which results in -inf\n",
    "log_magnitudes = np.log10(fieldIN['E_mag'][fieldIN['E_mag'] > 1e4]) \n",
    "\n",
    "# Normalize log-magnitudes to the range [0, 1] for colormapping\n",
    "# np.ptp (peak-to-peak) is range (max - min)\n",
    "min_log = log_magnitudes.min()\n",
    "ptp_log = np.ptp(log_magnitudes)\n",
    "\n",
    "# The normalization uses the min/ptp of the *sampled* data, not the full dataset\n",
    "# A small epsilon is added to the divisor to prevent division by zero in case ptp is 0\n",
    "log_magnitudes = np.log10(field_plot['E_mag']) \n",
    "norm_magnitudes = (log_magnitudes - min_log) / (ptp_log + 1e-12)\n",
    "\n",
    "print(f\"Log(E_mag) Min: {min_log}, Range (PtP): {ptp_log}\")\n",
    "\n",
    "# Choose a Colormap (e.g., 'jet')\n",
    "cmap = plt.cm.jet\n",
    "# Map the normalized magnitudes (0 to 1) to colors (RGBA floats 0.0 to 1.0)\n",
    "colors_rgba = cmap(norm_magnitudes)\n",
    "\n",
    "# Convert the RGBA colors from floats (0.0-1.0) to 8-bit integers (0-255) for trimesh\n",
    "colors_rgba = (colors_rgba * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "## ----------------------------------------------------\n",
    "## --- 4. Arrow Creation and Visualization ---\n",
    "## ----------------------------------------------------\n",
    "\n",
    "# Define base dimensions for the visualization\n",
    "base_arrow_length = 0.05  # Base length before scaling by magnitude\n",
    "arrow_radius = 0.001       # Radius of the arrow shaft\n",
    "cone_ratio = 0.2           # Ratio of the cone length to the total arrow length\n",
    "\n",
    "# Scale the base length by the normalized magnitude for visual encoding\n",
    "scaling_factor = 0.5  # Overall factor to control arrow visibility\n",
    "scaled_lengths = base_arrow_length * scaling_factor * norm_magnitudes\n",
    "\n",
    "# Iterate over each sampled point to create and place an arrow\n",
    "for pos, dir_vec, color, magnitude_norm, scaled_length in zip(\n",
    "    field_plot[\"pos\"], \n",
    "    directions_unit, \n",
    "    colors_rgba, \n",
    "    norm_magnitudes,\n",
    "    scaled_lengths\n",
    "):\n",
    "    # Skip if the direction vector has zero magnitude (E_mag was not filtered perfectly or is near zero)\n",
    "    if np.linalg.norm(dir_vec) == 0:\n",
    "        continue\n",
    "\n",
    "    # Calculate arrow dimensions\n",
    "    arrow_length = scaled_length\n",
    "    cone_length = arrow_length * cone_ratio\n",
    "    shaft_length = arrow_length - cone_length\n",
    "\n",
    "    # Create Arrow Geometry in trimesh (built along the Z-axis by default)\n",
    "\n",
    "    # 1. Create shaft (cylinder)\n",
    "    shaft = trimesh.creation.cylinder(radius=arrow_radius, height=shaft_length, sections=12)\n",
    "    # Move the shaft so its base is at z=0\n",
    "    shaft.apply_translation([0, 0, shaft_length / 2]) \n",
    "\n",
    "    # 2. Create cone (cone)\n",
    "    cone = trimesh.creation.cone(radius=arrow_radius * 2, height=cone_length, sections=12)\n",
    "    # Position the cone base to touch the top of the shaft\n",
    "    cone.apply_translation([0, 0, shaft_length])\n",
    "\n",
    "    # 3. Combine parts into a single trimesh object\n",
    "    arrow = trimesh.util.concatenate([shaft, cone])\n",
    "\n",
    "    # Set the computed color (scaled by magnitude) for all faces of the arrow\n",
    "    arrow.visual.face_colors = np.tile(color, (arrow.faces.shape[0], 1))\n",
    "\n",
    "    # Calculate the necessary transformation to place and orient the arrow\n",
    "\n",
    "    # a. Compute rotation matrix to align the default Z-axis ([0, 0, 1]) to the direction vector (dir_vec)\n",
    "    transform = trimesh.geometry.align_vectors([0, 0, 1], dir_vec)\n",
    "    \n",
    "    # b. Set the translation part of the transformation matrix (the arrow's position)\n",
    "    transform[:3, 3] = pos\n",
    "    \n",
    "    # c. Apply the full rotation and translation\n",
    "    arrow.apply_transform(transform)\n",
    "\n",
    "    # Add the colored and positioned arrow to the visualization scene\n",
    "    scene.add_geometry(arrow)\n",
    "\n",
    "# # # Target point\n",
    "# #location = np.array([[-0.1, 0, 0.1 - 0.015 + 0.037]])\n",
    "# location = np.array([[0, -0.1, 0.1]]) \n",
    "# #location = np.array([[0, 0, 0.2- 0.015]])   \n",
    " \n",
    "# radius_mm = 10/1000 # units: um\n",
    "# positions = df[iteration][\"pos\"]\n",
    "# efield = df[1][\"E\"]\n",
    "# emag = df[1][\"E_mag\"]\n",
    "\n",
    "# # Vectorized distance computation (distance between positions and target)\n",
    "# distances = np.linalg.norm(positions - location, axis=1)\n",
    "\n",
    "# # 1. Get indices of all points within the specified radius\n",
    "# # The result is a boolean array\n",
    "# within_radius_mask = distances <= radius_mm\n",
    "\n",
    "# # Get the actual indices\n",
    "# indices_within_radius = np.where(within_radius_mask)[0]\n",
    "\n",
    "# # Count the points found\n",
    "# count = len(indices_within_radius)\n",
    "# position_points = positions[indices_within_radius]\n",
    "# #E_points = efield[indices_within_radius]\n",
    "# #E_mag_points = emag[indices_within_radius]\n",
    "# emag_values = emag[indices_within_radius]\n",
    "\n",
    "# print(count,np.mean(emag_values[emag_values>0]))\n",
    "\n",
    "# # Create small spheres at each point\n",
    "# spheres = []\n",
    "# for point in position_points:\n",
    "#     sphere = trimesh.creation.icosphere(radius=0.001)  # adjust radius for point size\n",
    "#     sphere.apply_translation(point)\n",
    "#     sphere.visual.face_colors = [255, 0, 0, 255]  # red spheres\n",
    "#     spheres.append(sphere)\n",
    "\n",
    "# # Combine all meshes into a scene\n",
    "# scene.add_geometry(spheres)\n",
    "\n",
    "# Display the final scene containing the geometry and the vector field arrows\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### read in processed fieldmaps:\n",
    "\n",
    "Processed fieldmaps have a set spherical radius around the point of interest (goal: replicate Fig. 7 from Zimmerman et al., 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037])\n",
    "\n",
    "configIN = \"onlyphotoemission\"\n",
    "directory = \"../build-dissipationRefinedGrid-initial8max0.8final12-500000\"\n",
    "\n",
    "filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*{configIN}*.txt\")) #{iteration}\n",
    "print(filenames)\n",
    "\n",
    "Evector_atpoint_iterations = []\n",
    "\n",
    "df  = read_data_format_efficient(filenames,scaling=True)\n",
    "\n",
    "for keyIN in df.keys():\n",
    "\n",
    "    print(keyIN)\n",
    "\n",
    "    # Extract data\n",
    "    points = df[keyIN][\"pos\"] \n",
    "    vectors = df[keyIN][\"E\"]\n",
    "    magnitudes = df[keyIN][\"E_mag\"]\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Step 2: Build KDTree and find nearest neighbor\n",
    "    # ----------------------------------------------------\n",
    "    tree = cKDTree(points)\n",
    "    dist, idx = tree.query(target_point)\n",
    "\n",
    "    # Nearest neighbor field\n",
    "    E_vec_at_point = vectors[idx]\n",
    "    E_mag_at_point = magnitudes[idx]\n",
    "\n",
    "    # Store results\n",
    "    Evector_atpoint_iterations.append((keyIN, E_vec_at_point, E_mag_at_point))\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evector_atpoint_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output110525/processed-fieldmaps\"\n",
    "processed_data = load_h5_to_dict(f\"processed-fieldmaps/PE_425K_initial8max0.8final12_sphere50um.h5\")\n",
    "# Retrieve the field data dictionary for the current iteration (assumes 'df' is a list/dict)\n",
    "\n",
    "# takes ~2 minutes to read in 12 GB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Target point\n",
    "target_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037])\n",
    "#target_point = np.array([-0.1-0.007, 0, 0.1 - 0.015 + 0.037])\n",
    "\n",
    "Evector_atpoint_iterations = []\n",
    "\n",
    "for keyIN in processed_data.keys():\n",
    "    \n",
    "    # Extract data\n",
    "    points = processed_data[keyIN][\"pos\"] \n",
    "    vectors = processed_data[keyIN][\"E\"]\n",
    "    magnitudes = processed_data[keyIN][\"E_mag\"]\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Step 2: Build KDTree and find nearest neighbor\n",
    "    # ----------------------------------------------------\n",
    "    tree = cKDTree(points)\n",
    "    dist, idx = tree.query(target_point)\n",
    "\n",
    "    # Nearest neighbor field\n",
    "    E_vec_at_point = vectors[idx]\n",
    "    E_mag_at_point = magnitudes[idx]\n",
    "\n",
    "    #print(f\"{keyIN}: E vector (nearest neighbor) = {E_vec_at_point}, |E| = {E_mag_at_point}\")\n",
    "\n",
    "    # Store results\n",
    "    Evector_atpoint_iterations.append((int(keyIN.split(\"_\")[1]), E_vec_at_point, E_mag_at_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of tuples to arrays for easier plotting\n",
    "iterations, E_vectors, E_mag = zip(*Evector_atpoint_iterations)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.loglog(iterations, abs(np.array(E_vectors)[:, 0]), marker='.', linestyle='-', color='red') #abs(np.array(E_vectors)[:, 2])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the field data dictionary for the current iteration (assumes 'df' is a list/dict)\n",
    "fieldIN = processed_data[\"iter_1\"] \n",
    "\n",
    "plt.hist(fieldIN[\"E_mag\"][fieldIN[\"E_mag\"]>0],bins=np.logspace(0,8,100),alpha=0.5)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"|E| (V/m)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Temperature: 425 K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Varying density along a streamline\n",
    "plt.streamplot(processed_data[\"iter_1\"][\"pos\"][:,0], processed_data[\"iter_1\"][\"pos\"][:,1], \\\n",
    "               processed_data[\"iter_1\"][\"E\"][:,0], processed_data[\"iter_1\"][\"E\"][:,1], density=[0.5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Case 1: SW electrons and ions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### plot the field over each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlysolarwind\"\n",
    "\n",
    "# --- Configurations ---\n",
    "folder_path = [ \"../build-temp600K-dynamicThreshold\",\"../build-temp425K-dynamicThreshold\", \\\n",
    "               \"../build-425K-nodissipation-initial6-0.8Max-final9\", \"../build-425K-withoutdissipation\"]\n",
    "temperatures = [600,425,425,425]\n",
    "notes = [\"initial6max0.2final9(dissipation)\",\"initial6max0.2final9(dissipation)\",\"initial6max0.7final9(noDissipation)\",\"initial5max0.8final9(noDissipation)\"]\n",
    "\n",
    "# Target point (Fixed for all configurations)\n",
    "location = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) \n",
    "\n",
    "# --- Parallel Processing Worker Function ---\n",
    "\n",
    "def process_config(tempIN, folderIN, noteIN, configIN, location):\n",
    "    \"\"\"\n",
    "    Worker function to process a single configuration. \n",
    "    Returns the unique key and the calculated results.\n",
    "    \"\"\"\n",
    "    key_name = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    print(f\"--- Processing {key_name} in {folderIN} (Worker Process) ---\\n\")\n",
    "\n",
    "    # 1. Read Field Data\n",
    "    filenames = sorted(glob.glob(f\"{folderIN}/fieldmaps/*{configIN}*.txt\"))\n",
    "    fields_SW = read_data_format_efficient(filenames, scaling=True) \n",
    "    first_key = list(fields_SW.keys())[0]\n",
    "\n",
    "    # Prepare dictionary for this single result\n",
    "    config_results: Dict[str, Any] = {}\n",
    "    \n",
    "    # 2. Compute and Store Field at Target Location\n",
    "    # The function now returns a dict {'iter', 'E', 'E_mag'} for all iterations\n",
    "    config_results[\"fieldAtTarget\"] = compute_nearest_field_vector(fields_SW, target=location, start=first_key)\n",
    "    \n",
    "    # 3. Compute and Store the list of leaf lengths (number of points per iteration)\n",
    "    config_results[\"lengthLeaves\"] = [len(fields_SW[keyIN][\"pos\"]) for keyIN in fields_SW.keys()]\n",
    "    config_results[\"gradRefinements\"] = [fields_SW[keyIN][\"gradRefinements\"] for keyIN in fields_SW.keys()]\n",
    "    \n",
    "    # If the fields_PE dictionary is very large, deleting it immediately frees memory\n",
    "    del fields_SW \n",
    "    \n",
    "    # Return the key and the results to the main thread\n",
    "    return key_name, config_results\n",
    "\n",
    "# MASTER DICTIONARY to store all results securely\n",
    "results_data: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# --- Parallel Processing Loop ---\n",
    "\n",
    "# Prepare the list of arguments for the executor\n",
    "configs = zip(temperatures, folder_path, notes)\n",
    "args_list = [(temp, folder, note, configIN, location) for temp, folder, note in configs]\n",
    "\n",
    "MAX_WORKERS = len(temperatures) # Use one worker per configuration\n",
    "\n",
    "print(f\"--- Starting Parallel Processing with {MAX_WORKERS} workers ---\")\n",
    "\n",
    "# Use ProcessPoolExecutor for CPU-bound tasks\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    \n",
    "    # Submit all tasks and store the future objects\n",
    "    futures = [executor.submit(process_config, *args) for args in args_list]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            key, data = future.result()\n",
    "            results_data[key] = data\n",
    "        except Exception as exc:\n",
    "            print(f'Configuration generated an exception: {exc}')\n",
    "\n",
    "print(f\"--- Completed Parallel Processing ---\")\n",
    "\n",
    "# takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting and Analysis ---\n",
    "\n",
    "print(\"\\n--- Generating Plot for SW Comparison ---\")\n",
    "\n",
    "# 1. Load comparison data from Zimmerman\n",
    "zimmerman_SWdata = pd.read_csv(\"Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"Fig7a-PE.csv\")\n",
    "zimmerman_PEandSWdata = pd.read_csv(\"Fig7a-PE+SW.csv\")\n",
    "\n",
    "# 2. Define constants and calculate conversion factor\n",
    "# Simulation world area size in m^2 (600 um x 600 um)\n",
    "WORLD_XY_AREA_SQ_M = 600 * 600 / (1e6**2) \n",
    "\n",
    "# Number of particles (protons) injected into the active area per iteration\n",
    "PARTICLES_PER_ITERATION = 160440\n",
    "\n",
    "# Ion flux calculated from the simulation area (ions/m^2)\n",
    "FLUX_PER_ITERATION = PARTICLES_PER_ITERATION / WORLD_XY_AREA_SQ_M \n",
    "\n",
    "# Photoemission (PE) ion flux value (e/m^2/s). This factor combines \n",
    "# the effective current (4 uA/cm^2) and conversion to e/m^2/s.\n",
    "SW_ION_FLUX = 3e-7 * 6.241509e18 \n",
    "\n",
    "# Conversion factor: Time (s) per simulation iteration\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION / SW_ION_FLUX\n",
    "print(f\"Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "# 3. Define plot parameters (Colors)\n",
    "# Choose a continuous colormap and generate discrete colors based on the number of temperatures\n",
    "CMAP_NAME = 'jet' \n",
    "discrete_cmap = plt.get_cmap(CMAP_NAME, len(temperatures))\n",
    "color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, len(temperatures))]\n",
    "\n",
    "# 4. Generate Plot (Log-Log)\n",
    "plt.figure()\n",
    "\n",
    "# Plot reference data (Zimmerman)\n",
    "plt.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"], '--', label=\"SW (Zimmerman 2016)\", color=\"r\", lw=4)\n",
    "plt.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], 'g:', label=\"PE (Zimmerman 2016)\")\n",
    "plt.loglog(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"], 'b:', label=\"PE+SW (Zimmerman 2016)\")\n",
    "\n",
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.plot((results_data[key][\"fieldAtTarget\"][\"iter\"] -0.5)* CONVERT_ITERATION_PE_TIME, \n",
    "             results_data[key][\"fieldAtTarget\"][\"E\"][:,0], #results_data[key][\"fieldAtTarget\"][\"E_mag\"] \n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "# Plotting the E-field magnitude (|E|)\n",
    "plt.ylabel(r\"$|E_x$| (V/m)\") \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set axes limits\n",
    "plt.ylim(4.8e3, 2.8e5)\n",
    "plt.xlim(7.4e-2, 1.25e1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"lengthLeaves\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Total Leaf Nodes (millions)\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.title(\"SW Case\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"gradRefinements\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Gradient Refinements (millions)\")\n",
    "plt.title(\"SW Case\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "# configIN = \"onlysolarwind\"\n",
    "# directory_path = \"../build-leakage/\" # takes 12 minutes to read in with this data\n",
    "# #directory_path = \"../build-adaptive-barns-fixed/\"\n",
    "\n",
    "# filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "# fields_SW = read_data_format_efficient(filenames,scaling=True)   \n",
    "# \n",
    "# # Target point\n",
    "# location = np.array([-0.1, 0, 0.1-0.015+0.037]) \n",
    "# # return the electric field at that location\n",
    "# Efield_SW_location = compute_nearest_field_vector(fields_SW, target=location, start=1)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### check the minimum distance between point in field map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "directory_path = \"../build-425K-nodissipation-initial6-0.8Max-final9/\"\n",
    "filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "fields_SW = read_data_format_efficient(filenames,scaling=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fields_PE' is your list/dictionary structure and \n",
    "# fields_PE[1]['pos'] is a NumPy array of shape (N, 3), where N is the number of points.\n",
    "# Example data (replace this with your actual data):\n",
    "data_points = fields_SW[1]['pos'] \n",
    "# data_points = np.array([\n",
    "#     [1.0, 1.0, 1.0],\n",
    "#     [1.001, 1.0, 1.0],  # Very close point\n",
    "#     [2.0, 2.0, 2.0],\n",
    "#     [5.0, 5.0, 5.0]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# 1. Build the KD-Tree\n",
    "# This organizes the points in a spatial structure for efficient nearest neighbor search.\n",
    "tree = cKDTree(data_points)\n",
    "\n",
    "# 2. Query for the 2 nearest neighbors of every point\n",
    "# The 'k=2' parameter tells the query to find the distance to the 2 nearest neighbors:\n",
    "# - The 1st neighbor (k=1) is always the point itself (distance = 0.0).\n",
    "# - The 2nd neighbor (k=2) is the closest *other* point.\n",
    "distances, indices = tree.query(data_points, k=2)\n",
    "\n",
    "# 3. Extract the minimum non-zero distance\n",
    "# The minimum distance between any unique pair of points is the minimum value \n",
    "# in the array of distances to the second nearest neighbor (distances[:, 1]).\n",
    "min_distance = np.min(distances[:, 1])*1000\n",
    "\n",
    "print(f\"The total number of points (voxels) is: {len(data_points)}\")\n",
    "print(f\"The minimum distance between any two unique voxels is: {min_distance} um\")\n",
    "\n",
    "# takes around ~10 seconds to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### calculate # of iterations for direct comparison with Zimmerman:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"../build-wang-comparison/root/\"\n",
    "#directory_path = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/build-dissipationRefinedGrid-initial8max0.8final12/root\"\n",
    "configIN = \"solarwind\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*num50000.root\"))\n",
    "\n",
    "all_incident_protons_inside, all_incident_electrons_inside = [],[]\n",
    "\n",
    "for fileIN in filelist:\n",
    "\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"protons_inside_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)] = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path), \n",
    "                                                                                                             config=configIN)\n",
    "    all_incident_protons_inside.append(vars()[\"protons_inside_\"+str(number_str)])\n",
    "    all_incident_electrons_inside.append(vars()[\"electrons_inside_\"+str(number_str)])\n",
    "    print(78*\"-\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all iterations into single DataFrames\n",
    "all_incident_protons_inside_df = pd.concat(all_incident_protons_inside, ignore_index=True)\n",
    "all_incident_electrons_inside_df = pd.concat(all_incident_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf, ilm_values,_ = plot_face_illumination(electrons_inside_iteration0, stacked_spheres, vmin=0, vmax=10)\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surf.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "protons_inside_iteration50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ilm_values, bins=np.logspace(-1,3,100))\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"# of Photons hitting each Voxel\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Distribution Stats: mean {np.mean(ilm_values[ilm_values!=0])}, median {np.median(ilm_values[ilm_values!=0])}, max {np.max(ilm_values)}\")\n",
    "\n",
    "# our area is a factor of 4 smaller than their area \n",
    "#print(f\"for one iteration, mean # of particles in each equivalently sized voxel is {np.mean(ilm_values[ilm_values!=0])}\") <- no longer needed, made voxels similar sizes\n",
    "voxel_area = 0.0004/(1000)**2 # rough area approximated from python (0.4 micron2)\n",
    "zimmerman_charge = 1*(1e-6) # C/m2\n",
    "zimmerman_electronnum = (zimmerman_charge/1.60217663e-19)*voxel_area\n",
    "print(f\"will take {zimmerman_electronnum/(np.mean(ilm_values[ilm_values!=0]))} iterations at this rate to get to the photoemission flux ranges shown at 3 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### make a movie of all surface potential for each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"figures/solarwind/\"\n",
    "\n",
    "for num in range(0, iterationNUM+1):\n",
    "    surface, _, facecolors = plot_surface_potential_fornegativepositive_charge(\n",
    "        vars()[\"electrons_inside_stackediteration\"+str(num)], \n",
    "        vars()[\"protons_inside_stackediteration\"+str(num)], \n",
    "        stacked_spheres, \n",
    "        vmin=-1.8, vmax=1.8\n",
    "    )\n",
    "\n",
    "    surface_edited = surface.copy()\n",
    "    surface_edited.unmerge_vertices()  # Ensure unique vertices per face\n",
    "\n",
    "    # Crop bounding box\n",
    "    bbox_min = np.array([-0.2, -0.3, 0])\n",
    "    bbox_max = np.array([ 0.2,  0.1, 100])\n",
    "\n",
    "    in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                    (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "    face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "    cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "    cropped_colors = facecolors[face_mask]/255  # Crop facecolors to match cropped mesh\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    mesh = Poly3DCollection(cropped.triangles, alpha=1.0)\n",
    "    mesh.set_facecolor(cropped_colors)  # Apply correct colors\n",
    "    mesh.set_edgecolor('k')          # edge color\n",
    "    mesh.set_linewidths(0.1)         # edge line width\n",
    "\n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_title(f\"Iteration {num}\")\n",
    "    ax.title.set_position((0.5, 0.1))  # manually control title position\n",
    "\n",
    "    # Scale\n",
    "    scale = surface_edited.bounds.flatten()\n",
    "    ax.auto_scale_xyz(scale, scale, scale)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    filename = f\"{directory}iteration_{num}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    if num%10==0:\n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "print(f\"All plots saved to {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"figures/solarwind/\"\n",
    "\n",
    "for num in range(0, iterationNUM + 1):\n",
    "    # Get face illumination for electrons and protons\n",
    "    surface_e, _, facecolors_e = plot_face_illumination(\n",
    "        vars()[\"electrons_inside_stackediteration\" + str(num)],\n",
    "        stacked_spheres, vmin=0, vmax=100\n",
    "    )\n",
    "\n",
    "    surface_p, _, facecolors_p = plot_face_illumination(\n",
    "        vars()[\"protons_inside_stackediteration\" + str(num)],\n",
    "        stacked_spheres, vmin=0, vmax=100\n",
    "    )\n",
    "\n",
    "    def crop_and_prepare(surface, facecolors):\n",
    "        surface = surface.copy()\n",
    "        surface.unmerge_vertices()\n",
    "\n",
    "        # Crop bounding box\n",
    "        bbox_min = np.array([-0.2, -0.3, 0])\n",
    "        bbox_max = np.array([ 0.2,  0.1, 100])\n",
    "\n",
    "        in_box = np.all((surface.vertices >= bbox_min) & \n",
    "                        (surface.vertices <= bbox_max), axis=1)\n",
    "\n",
    "        face_mask = np.all(in_box[surface.faces], axis=1)\n",
    "        cropped = surface.submesh([face_mask], only_watertight=False, append=True)\n",
    "        cropped_colors = facecolors[face_mask] / 255.0\n",
    "        return cropped, cropped_colors\n",
    "\n",
    "    cropped_e, colors_e = crop_and_prepare(surface_e, facecolors_e)\n",
    "    cropped_p, colors_p = crop_and_prepare(surface_p, facecolors_p)\n",
    "\n",
    "    # Plotting both side by side\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "    for i, (cropped, colors, title) in enumerate([\n",
    "        (cropped_e, colors_e, \"Electron Illumination\"),\n",
    "        (cropped_p, colors_p, \"Proton Illumination\")\n",
    "    ]):\n",
    "        ax = fig.add_subplot(1, 2, i + 1, projection='3d')\n",
    "        mesh = Poly3DCollection(cropped.triangles, alpha=1.0)\n",
    "        mesh.set_facecolor(colors)\n",
    "        mesh.set_edgecolor('k')\n",
    "        mesh.set_linewidths(0.1)\n",
    "        ax.add_collection3d(mesh)\n",
    "\n",
    "        scale = cropped.bounds.flatten()\n",
    "        ax.auto_scale_xyz(scale, scale, scale)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(f\"{title}\\nIteration {num}\", pad=5)\n",
    "\n",
    "    filename = f\"{directory}iteration_{num}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    if num % 10 == 0:\n",
    "        print(f\"Saved: {filename}\")\n",
    "        \n",
    "print(f\"âœ… All plots saved to {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterationIN=0\n",
    "\n",
    "directory = \"figures/solarwind/\"\n",
    "\n",
    "for num in range(0,iterationNUM):\n",
    "    surface,_ = plot_surface_potential_fornegativepositive_charge(vars()[\"electrons_inside_stackediteration\"+str(num)], vars()[\"protons_inside_stackediteration\"+str(num)], stacked_spheres, vmin=-10,vmax=10)\n",
    "\n",
    "    # plt.hist(facolors[facolors!=0],bins=50)\n",
    "    # plt.show()\n",
    "\n",
    "    # Make sure each triangle has its own unique vertices\n",
    "    surface_edited = surface.copy()\n",
    "    surface_edited.unmerge_vertices()\n",
    "    surface_edited.visual.vertex_colors = None\n",
    "    surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap and normalization\n",
    "cmap = plt.cm.OrRd #seismic\n",
    "norm = Normalize(vmin=0, vmax=100)\n",
    "\n",
    "# Create a figure and a single axis for the colorbar\n",
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "# Create the colorbar\n",
    "cb = ColorbarBase(ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "cb.set_label('# of Particles / face')  # Optional label\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.25, -0.31, 0])\n",
    "bbox_max = np.array([ 0.15,  0.1,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Case 2: Photoemission (incident gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### plot the field over each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output111025/processed-fieldmaps\"\n",
    "# processedResults = load_h5_to_dict(f\"{directory}/PE_425K_initial8max0.8final12_noDissipation_sphere50um-throughXX.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- configuration ---\n",
    "directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output111025/processed-fieldmaps\"\n",
    "h5_filenames = glob.glob(f\"{directory}/*.h5\")\n",
    "\n",
    "# --- helper function for one key ---\n",
    "def process_key(args):\n",
    "\n",
    "    keyIN, val, target_point = args\n",
    "\n",
    "    points = val[\"pos\"]\n",
    "    vectors = val[\"E\"]\n",
    "    magnitudes = val[\"E_mag\"]\n",
    "\n",
    "    # radius = 6e-3  # spherical averaging radius\n",
    "    radius = 2.5e-3  # spherical averaging radius\n",
    "    \n",
    "    mask = np.sum((points - target_point)**2, axis=1) <= radius**2\n",
    "    if not np.any(mask):\n",
    "        # Return placeholders if no points in sphere\n",
    "        return -1, np.full(3, np.nan), np.nan, np.full(3, np.nan), 0\n",
    "\n",
    "    avg_position = points[mask].mean(axis=0)\n",
    "    E_vec = vectors[mask].mean(axis=0)\n",
    "    E_mag = magnitudes[mask].mean(axis=0)\n",
    "    E_vec_errors = vectors[mask].std(axis=0) /np.sqrt(len(magnitudes[mask]))\n",
    "    #point_err = np.abs(avg_position - target_point)\n",
    "    return int(keyIN.split(\"_\")[1]), E_vec, E_mag, E_vec_errors,len(magnitudes[mask]) \n",
    "\n",
    "    # tree = cKDTree(points)\n",
    "    # dist, idx = tree.query(target_point)\n",
    "\n",
    "    # # Nearest neighbor field\n",
    "    # E_vec_at_point = vectors[idx]\n",
    "    # E_mag_at_point = magnitudes[idx]\n",
    "\n",
    "    # return int(keyIN.split(\"_\")[1]), E_vec_at_point, E_mag_at_point, dist \n",
    "\n",
    "# --- extract metadata from filename ---\n",
    "def parse_filename_metadata(filename):\n",
    "    \"\"\"\n",
    "    Only processes files with:\n",
    "      - Temperature: 425 K\n",
    "      - pos_value: -0.1\n",
    "    Example:\n",
    "    PE_425K_initial8max0.8final12_RefinedGridDissipation_500000particles_Sphere20um_pos-0.1_through26.h5\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    base = os.path.basename(filename)\n",
    "\n",
    "    case = base.split(\"_\")[0]\n",
    "\n",
    "    # Temperature\n",
    "    temp_match = re.search(r'_(\\d+)K_', base)\n",
    "    temperature = int(temp_match.group(1)) if temp_match else np.nan\n",
    "\n",
    "    # Position\n",
    "    pos_match = re.search(r'_pos([-+]?\\d*\\.?\\d+)_through', base)\n",
    "    pos_value = float(pos_match.group(1)) if pos_match else np.nan\n",
    "\n",
    "    # Sphere size (if any)\n",
    "    sphere_match = re.search(r'_sphere(\\d+)um', base, re.IGNORECASE)\n",
    "    sphere_um = int(sphere_match.group(1)) if sphere_match else np.nan\n",
    "\n",
    "    # Octree parameters: initial, grad threshold, final\n",
    "    octree_match = re.search(r'_initial(\\d+)max([-+]?\\d*\\.?\\d+)final(\\d+)', base)\n",
    "    if octree_match:\n",
    "        octree_params = {\n",
    "            \"initial_depth\": int(octree_match.group(1)),\n",
    "            \"percent_gradThreshold\": float(octree_match.group(2)),\n",
    "            \"final_depth\": int(octree_match.group(3))\n",
    "        }\n",
    "    else:\n",
    "        octree_params = {\"initial_depth\": np.nan, \"grad_threshold\": np.nan, \"final_depth\": np.nan}\n",
    "\n",
    "    # # Number of particles (e.g., \"_500000particles_\")\n",
    "    # particle_match = re.search(r'_(\\d+)particles', base, re.IGNORECASE)\n",
    "    # num_particles = int(particle_match.group(1)) if particle_match else 100000\n",
    "\n",
    "    # # --- filtering condition ---\n",
    "    # if not (temperature == 425 and np.isclose(pos_value, -0.1)) and (num_particles == 500000):\n",
    "    #     # Skip file if criteria not met\n",
    "    #     return None\n",
    "\n",
    "    # Target point\n",
    "    #target_point = np.array([pos_value - 0.0073, 0, 0.1 - 0.015 + 0.037 - 0.00073])\n",
    "    target_point = np.array([pos_value, 0, 0.1 - 0.015 + 0.037])\n",
    "\n",
    "    metadata = {\n",
    "        \"filename\": base,\n",
    "        \"case\": case,\n",
    "        \"temperature\": temperature,\n",
    "        \"target_point\": target_point,\n",
    "        \"sphere_um\": sphere_um,\n",
    "        \"octree\": octree_params,\n",
    "       # \"num_particles\": num_particles\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# --- worker for one file ---\n",
    "def process_file(fileIN):\n",
    "    metadata = parse_filename_metadata(fileIN)\n",
    "    \n",
    "    # # Filter: skip files with 'through' files that have processed < 40 iterations\n",
    "    # through_idx = int(re.search(r'_through(\\d+)\\.h5', fileIN).group(1))\n",
    "    # if through_idx < 40:\n",
    "    #     return None\n",
    "\n",
    "    if metadata is None:\n",
    "        return None\n",
    "\n",
    "    print(f\"â†’ Started {os.path.basename(fileIN)}\\n\", flush=True)\n",
    "    processedResults = load_h5_to_dict(fileIN)\n",
    "    key_prefix = os.path.basename(fileIN).split('_through')[0]\n",
    "\n",
    "    keys = list(processedResults.keys())\n",
    "    n_keys = len(keys)\n",
    "\n",
    "    # --- inner parallelization across keys ---\n",
    "    args_list = [(k, processedResults[k], metadata[\"target_point\"]) for k in keys]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as tpool:  # inner parallel threads\n",
    "        results = list(tpool.map(process_key, args_list))\n",
    "\n",
    "    ids, E_vecs, E_mags, standard_errors, N = zip(*results)\n",
    "    ids = np.array(ids)\n",
    "    E_vecs = np.array(E_vecs)\n",
    "    E_mags = np.array(E_mags)\n",
    "    standard_errors = np.array(standard_errors)\n",
    "    num_points = np.array(N)\n",
    "\n",
    "    print(f\"âœ“ Finished {os.path.basename(fileIN)}\", flush=True)\n",
    "    return key_prefix, {\n",
    "        \"iter\": ids,\"E_vecs\": E_vecs,\"E_mags\": E_mags,\"point_errors\": standard_errors, \"N\":num_points, \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "# --- parallel execution across files ---\n",
    "num_cores = 2\n",
    "all_processed = {}\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "    futures = {executor.submit(process_file, f): f for f in h5_filenames}\n",
    "    for fut in as_completed(futures):\n",
    "        fileIN = futures[fut]\n",
    "        try:\n",
    "            result = fut.result()\n",
    "            if result is not None:\n",
    "                key_prefix, data = result\n",
    "                all_processed[key_prefix] = data\n",
    "                print(f\"âœ” Processed {os.path.basename(fileIN)}\\n\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in {os.path.basename(fileIN)}: {e}\\n\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.optimize import curve_fit\n",
    "# Import interpolate for numerical method\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# --- 1. CONFIGURATION AND DATA LOADING ---\n",
    "print(\"\\n--- Starting Data Processing and Plot Generation ---\")\n",
    "\n",
    "# Define Data Keys for Fitting and Subtraction\n",
    "KEY_FIT = 'PE_425K_initial8max0.8final12_noDissipation_sphere50um_pos-0.1'\n",
    "KEY_TARGET = 'PE_425K_initial8max0.8final12_RefinedGridDissipation_sphere20um_pos-0.1'\n",
    "\n",
    "# Load external literature data\n",
    "zimmerman_SWdata = pd.read_csv(\"literature-data/Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"literature-data/Fig7a-PE.csv\")\n",
    "zimmerman_PEandSWdata = pd.read_csv(\"literature-data/Fig7a-PE+SW.csv\")\n",
    "\n",
    "# Simulation Parameters (used for time conversion)\n",
    "WORLD_XY_AREA_SQ_M = 300 * 300 / (1e6**2) # World area (m^2)\n",
    "\n",
    "# PE Conversion Factor\n",
    "PARTICLES_PER_ITERATION_PE = 81775\n",
    "FLUX_PER_ITERATION_PE = PARTICLES_PER_ITERATION_PE / WORLD_XY_AREA_SQ_M\n",
    "PE_ION_FLUX = 4e-6 * 6.241509e18\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION_PE / PE_ION_FLUX\n",
    "print(f\"PE Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "# SW Conversion Factor\n",
    "PARTICLES_PER_ITERATION_SW = 30601\n",
    "FLUX_PER_ITERATION_SW = PARTICLES_PER_ITERATION_SW / WORLD_XY_AREA_SQ_M\n",
    "SW_ION_FLUX = 3e-7 * 6.241509e18\n",
    "CONVERT_ITERATION_SW_TIME = FLUX_PER_ITERATION_SW / SW_ION_FLUX\n",
    "print(f\"SW Conversion Factor (s/iteration): {CONVERT_ITERATION_SW_TIME:.3e}\")\n",
    "\n",
    "# Color Map Setup\n",
    "CMAP_NAME = 'Dark2'\n",
    "discrete_cmap = plt.get_cmap(CMAP_NAME, len(all_processed.keys()) + 1)\n",
    "color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, len(all_processed.keys()) + 1)]\n",
    "\n",
    "# --- 2. CURVE FITTING AND EXTRAPOLATION ---\n",
    "\n",
    "# Define the new fitting function (Polynomial of Order 3)\n",
    "def poly_curve(t, a, b, c, d):\n",
    "    \"\"\"\n",
    "    Function: a*t^3 + b*t^2 + c*t + d (Polynomial of Order 3)\n",
    "    \"\"\"\n",
    "    return a*t**3 + b*t**2 + c*t + d\n",
    "\n",
    "\n",
    "# Prepare data for fitting (PE_425K)\n",
    "data_fit = all_processed[KEY_FIT]\n",
    "x_fit = np.array(data_fit[\"iter\"] - 1) * CONVERT_ITERATION_PE_TIME\n",
    "y_fit = abs(data_fit[\"E_vecs\"][:, 0])\n",
    "y_fit_errors = data_fit[\"point_errors\"][:, 0] # Get errors for KEY_FIT (50um)\n",
    "\n",
    "# Prepare Target Data (PE_600K range)\n",
    "data_target = all_processed[KEY_TARGET]\n",
    "x_extrapolate = np.array(data_target[\"iter\"] - 1) * CONVERT_ITERATION_PE_TIME\n",
    "y_target = abs(data_target[\"E_vecs\"][:, 0])\n",
    "y_target_errors = data_target[\"point_errors\"][:, 0] # Get errors for KEY_TARGET (20um)\n",
    "\n",
    "# --- Choose Fitting/Extrapolation Method ---\n",
    "\n",
    "# Method 1: Basic Polynomial Fit (Order 3) - CURRENT DEFAULT\n",
    "method_label = f\"Fit of {KEY_FIT.split('_')[1]} (Poly Order 3)\"\n",
    "try:\n",
    "    # Using y_fit_errors as sigma for weighted fit\n",
    "    popt, pcov = curve_fit(poly_curve, x_fit, y_fit, \n",
    "                           p0=[0, 0, 0, 1e4], sigma=y_fit_errors, absolute_sigma=True) # Added sigma\n",
    "    \n",
    "    A_fit, B_fit, C_fit, D_fit = popt\n",
    "    print(f\"\\nFit Parameters for {KEY_FIT.split('_')[1]} (Poly): a={A_fit:.2e}, b={B_fit:.2e}, c={C_fit:.2e}, d={D_fit:.2e}\")\n",
    "\n",
    "    # Extrapolate and Calculate Subtraction\n",
    "    y_extrapolated = poly_curve(x_extrapolate, *popt)\n",
    "    y_subtraction = y_extrapolated - y_target\n",
    "\n",
    "    # Estimate error in extrapolated fit for error propagation\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    # NOTE: J calculation assumes the x_extrapolate points are the basis for the estimated error\n",
    "    J = np.array([3*x_extrapolate**2, 2*x_extrapolate, np.ones_like(x_extrapolate), np.zeros_like(x_extrapolate)]).T\n",
    "    y_extrapolated_errors = np.sqrt(np.diag(J @ pcov @ J.T))\n",
    "\n",
    "\n",
    "except RuntimeError:\n",
    "    print(\"\\nâš ï¸ Warning: Curve fitting failed. Check initial guess (p0) or fitting range.\")\n",
    "    y_extrapolated = np.zeros_like(x_fit)\n",
    "    y_subtraction = np.zeros_like(x_fit)\n",
    "    x_extrapolate = x_fit\n",
    "    method_label = f\"Fit of {KEY_FIT.split('_')[1]} (Failed, showing Zeros)\"\n",
    "    y_extrapolated_errors = np.zeros_like(x_extrapolate) # Set errors to zero if fit fails\n",
    "\n",
    "\n",
    "# --- Calculate Percent Difference and its Error ---\n",
    "y_denominator = np.where(y_extrapolated == 0, 1e-10, y_extrapolated)\n",
    "y_percent_diff = (y_subtraction / y_denominator) * 100\n",
    "\n",
    "# Error propagation for the difference: sqrt(error_fit^2 + error_target^2)\n",
    "# Direct assignment works because y_target_errors has the same length as x_extrapolate\n",
    "y_target_errors_interp = y_target_errors \n",
    "\n",
    "y_subtraction_errors = np.sqrt(y_extrapolated_errors**2 + y_target_errors_interp**2)\n",
    "\n",
    "# Error propagation for the percentage\n",
    "y_percent_diff_errors = (y_subtraction_errors / np.abs(y_denominator)) * 100 \n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "# --- 3. PLOTTING SETUP (MAIN + SUBPLOT) ---\n",
    "\n",
    "# Set up figure and grid layout (5:1 height ratio for main plot vs. residual plot)\n",
    "fig = plt.figure(figsize=(8.01, 3.22))\n",
    "gs = gridspec.GridSpec(2, 1, hspace=0.09, height_ratios=[5, 1])\n",
    "\n",
    "# Main Plot (Top)\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "# Subtraction Plot (Bottom), sharing the x-axis\n",
    "ax_sub = fig.add_subplot(gs[1], sharex=ax_main)\n",
    "\n",
    "# --- 4. MAIN PLOT GENERATION (ax_main) ---\n",
    "\n",
    "# Plot reference data (Zimmerman)\n",
    "ax_main.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"], '-', color=\"k\", lw=3, alpha=0.3, label=\"Zimmerman SW/PE/PE+SW Ref.\")\n",
    "ax_main.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], '-', color=\"k\", lw=3, alpha=0.3)\n",
    "ax_main.plot(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"], '--', color=\"k\", lw=3, alpha=0.3)\n",
    "\n",
    "# Plot simulation data\n",
    "color_list = [color_list_rgba[2],color_list_rgba[3],color_list_rgba[6]]\n",
    "i=0\n",
    "\n",
    "# Plot the fitted/extrapolated curve\n",
    "ax_main.plot(x_extrapolate, y_extrapolated, '-', color=color_list[-1], lw=1.5, \n",
    "             label=f\"{method_label} (Extrapolated)\")\n",
    "\n",
    "for keyIN, colorIN in zip(all_processed.keys(), color_list_rgba):\n",
    "    \n",
    "    # Define plotting variables outside of loop to use them later\n",
    "    case = all_processed[keyIN][\"metadata\"][\"case\"]\n",
    "    factor = CONVERT_ITERATION_PE_TIME if case == \"PE\" else CONVERT_ITERATION_SW_TIME\n",
    "    tempIN = all_processed[keyIN][\"metadata\"][\"temperature\"]\n",
    "    targetIN = all_processed[keyIN][\"metadata\"][\"target_point\"]\n",
    "    \n",
    "    x_data = np.array(all_processed[keyIN][\"iter\"] - 1) * factor\n",
    "    y_data = abs(all_processed[keyIN][\"E_vecs\"][:, 0])\n",
    "    y_err = all_processed[keyIN][\"point_errors\"][:, 0]\n",
    "    \n",
    "    tempIN = all_processed[keyIN][\"metadata\"][\"temperature\"]\n",
    "    targetIN = all_processed[keyIN][\"metadata\"][\"target_point\"]\n",
    "    # noteIN = keyIN.split(\"_\")[2] # Not used in label for brevity\n",
    "\n",
    "    # Filter plotting to only the relevant cases (e.g., specific position and T=425)\n",
    "    if (targetIN[0] < 0) & (tempIN == 425) & (\"Total\" not in keyIN.split(\"_\")[3]):\n",
    "        \n",
    "        print(keyIN)\n",
    "\n",
    "        plot_color = color_list[i]\n",
    "\n",
    "        # if keyIN == KEY_FIT:\n",
    "        #     # Plot the data line\n",
    "        #     ax_main.plot(x_data, y_data, '-', color=plot_color, lw=3)\n",
    "        # else:\n",
    "        # Plot the data line\n",
    "        ax_main.plot(x_data, y_data, '-', color=plot_color, lw=1.5)\n",
    "        \n",
    "        # Use fill_between for the error region (Replaces errorbars)\n",
    "        ax_main.fill_between(x_data, y_data - y_err, y_data + y_err, \n",
    "                            color=plot_color, alpha=0.15, \n",
    "                            label=None) # Set label=None to avoid extra legend entry\n",
    "        i+=1\n",
    "\n",
    "# Add original uncommented features back to ax_main\n",
    "#ax_main.axvline(x=65 * CONVERT_ITERATION_PE_TIME, color='gray', linestyle='-.', lw=1, alpha=0.7, label=\"Vertical Marker\")\n",
    "ax_main.set_ylabel(r\"$|E_x|$ (V/m)\")\n",
    "ax_main.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "#ax_main.set_yscale('log') # Use log scale for better visualization of power-law decay\n",
    "ax_main.set_ylim(0,2e5) # Uncommented limits\n",
    "# ax_main.set_xlim(7.4e-1, 1.25e1) # Uncommented limits\n",
    "\n",
    "# Clean up main plot\n",
    "# ax_main.grid(True, linestyle=':', alpha=0.5)\n",
    "# ax_main.legend(loc='lower left', fontsize=8, ncol=2)\n",
    "# Remove X-tick labels from the main plot\n",
    "plt.setp(ax_main.get_xticklabels(), visible=False) \n",
    "\n",
    "# --- 5. SUBTRACTION PLOT GENERATION (ax_sub) ---\n",
    "\n",
    "# PLOT PERCENT DIFFERENCE WITH SHADED ERROR REGION\n",
    "ax_sub.plot(x_extrapolate, y_percent_diff, '-', color=color_list_rgba[3], lw=2,\n",
    "            label=r\"Relative Error: $\\frac{|E_{Fit}| - |E_{Target}|}{|E_{Fit}|}$\")\n",
    "# Shaded region\n",
    "# ax_sub.fill_between(x_extrapolate, y_percent_diff - y_percent_diff_errors, \n",
    "#                     y_percent_diff + y_percent_diff_errors, \n",
    "#                     color=color_list_rgba[2], alpha=0.2, label=\"Error Region\")\n",
    "ax_sub.set_xlabel(\"Time [s]\")\n",
    "#ax_sub.set_ylabel(r\"% Diff\")\n",
    "# ax_sub.ticklabel_format(axis='y', style='sci', scilimits=(0, 0)) # Removed, % difference is typically not sci notation\n",
    "# ax_sub.grid(True, linestyle=':', alpha=0.6)\n",
    "# ax_sub.legend(loc='upper right', fontsize=8)\n",
    "ax_sub.set_xlim(0,6) # Uncommented limit check (if sharing xlim works)\n",
    "ax_sub.set_ylim(0,10) # Uncommented limit check (if sharing xlim works)\n",
    "ax_sub.set_yticks([0,4,8])\n",
    "\n",
    "# --- 6. SAVE AND SHOW ---\n",
    "plt.savefig(\"figures/zimmerman_benchmark_summary.jpeg\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 10\n",
    "print(f\"PE equivalent time for iteration#{iteration}: {(iteration-1)*CONVERT_ITERATION_PE_TIME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting and Analysis ---\n",
    "\n",
    "print(\"\\n--- Generating Plot for PE Comparison ---\")\n",
    "\n",
    "# 1. Load comparison data from Zimmerman\n",
    "zimmerman_SWdata = pd.read_csv(\"literature-data/Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"literature-data/Fig7a-PE.csv\")\n",
    "zimmerman_PEandSWdata = pd.read_csv(\"literature-data/Fig7a-PE+SW.csv\")\n",
    "\n",
    "# 2. Define constants and calculate conversion factor\n",
    "# Simulation world area size in microns\n",
    "WORLD_XY_AREA_SQ_M = 300 * 300 / (1e6**2) \n",
    "\n",
    "# Number of particles injected into the active area per iteration\n",
    "PARTICLES_PER_ITERATION = 81775 #80559 # for the old geometry with smaller voxels #424975 # for 600by600 world\n",
    "\n",
    "# Ion flux calculated from the simulation area (ions/m^2)\n",
    "FLUX_PER_ITERATION = PARTICLES_PER_ITERATION / WORLD_XY_AREA_SQ_M \n",
    "\n",
    "# Photoemission (PE) ion flux value (e/m^2/s). This factor combines \n",
    "# the effective current (4 uA/cm^2) and conversion to e/m^2/s.\n",
    "PE_ION_FLUX = 4e-6 * 6.241509e18 \n",
    "\n",
    "# Conversion factor: Time (s) per simulation iteration\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION / PE_ION_FLUX\n",
    "print(f\"PE Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "\n",
    "# Number of particles (protons) injected into the active area per iteration\n",
    "PARTICLES_PER_ITERATION = 30601 #160440\n",
    "\n",
    "# Ion flux calculated from the simulation area (ions/m^2)\n",
    "FLUX_PER_ITERATION = PARTICLES_PER_ITERATION / WORLD_XY_AREA_SQ_M \n",
    "\n",
    "# Photoemission (PE) ion flux value (e/m^2/s). This factor combines \n",
    "# the effective current (4 uA/cm^2) and conversion to e/m^2/s.\n",
    "SW_ION_FLUX = 3e-7 * 6.241509e18 \n",
    "\n",
    "# Conversion factor: Time (s) per simulation iteration\n",
    "CONVERT_ITERATION_SW_TIME = FLUX_PER_ITERATION / SW_ION_FLUX\n",
    "print(f\"SW Conversion Factor (s/iteration): {CONVERT_ITERATION_SW_TIME:.3e}\")\n",
    "\n",
    "# 3. Define plot parameters (Colors)\n",
    "# Choose a continuous colormap and generate discrete colors based on the number of temperatures\n",
    "CMAP_NAME = 'jet' \n",
    "discrete_cmap = plt.get_cmap(CMAP_NAME, len(all_processed.keys()) + 1)\n",
    "color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, len(all_processed.keys()) + 1)]\n",
    "\n",
    "# 4. Generate Plot (Log-Log)\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "# Plot reference data (Zimmerman)\n",
    "plt.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"], '--', color=\"r\", lw=4) #, label=\"SW (Zimmerman 2016)\"\n",
    "plt.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], 'g:') #, label=\"PE (Zimmerman 2016)\"\n",
    "plt.plot(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"], 'b:') #, label=\"PE+SW (Zimmerman 2016)\"\n",
    "\n",
    "for keyIN, colorIN in zip(all_processed.keys(),color_list_rgba):\n",
    "    if all_processed[keyIN][\"metadata\"][\"case\"] == \"SW\":\n",
    "        FACTOR= CONVERT_ITERATION_SW_TIME\n",
    "    else:\n",
    "        FACTOR = CONVERT_ITERATION_PE_TIME\n",
    "\n",
    "    tempIN = all_processed[keyIN][\"metadata\"][\"temperature\"]\n",
    "    targetIN = all_processed[keyIN][\"metadata\"][\"target_point\"]\n",
    "    noteIN = keyIN.split(\"_\")[2]\n",
    "\n",
    "    if targetIN[0] == 0.1:\n",
    "        plt.plot(np.array(all_processed[keyIN][\"iter\"]) * FACTOR, \\\n",
    "                abs(all_processed[keyIN][\"E_vecs\"][:,0]), marker='.', linestyle='-', color=colorIN,lw=0.5,\\\n",
    "                label=f\"{keyIN}\") #{tempIN}: {noteIN}, pos: {targetIN}\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "# Plotting the E-field magnitude (|E|)\n",
    "plt.axvline(x=65*CONVERT_ITERATION_PE_TIME)\n",
    "plt.ylabel(r\"$|E_x$| (V/m)\") \n",
    "#plt.ylabel(r\"$|E$| (V/m)\") \n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.grid(True)\n",
    "\n",
    "# Set axes limits\n",
    "#plt.ylim(4.8e3, 2.8e5)\n",
    "#plt.xlim(7.4e-1, 1.25e1)\n",
    "plt.xlim(0,4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"lengthLeaves\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Total Leaf Nodes (millions)\")\n",
    "plt.title(\"PE Case\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"gradRefinements\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Gradient Refinements (millions)\")\n",
    "plt.title(\"PE Case\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "# configIN = \"onlyphotoemission\"\n",
    "# directory_path = \"../build-disspate-charge/\"\n",
    "\n",
    "# filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "# fields_PE = read_data_format_efficient(filenames,scaling=True) \n",
    "\n",
    "# # Target point\n",
    "# location = np.array([-0.1, 0, 0.1-0.015+0.037]) \n",
    "# # return the electric field at that location\n",
    "# Efield_PE_location = compute_nearest_field_vector(fields_PE, target=location, start=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process different sets of iterations (not in parallel) ##\n",
    "\n",
    "# configIN = \"onlyphotoemission\"\n",
    "# folder_path = [\"../build-temp425K-dynamicThreshold\",  \"../build-temp600K-dynamicThreshold\", \"../build-425K-initial6-0.9Max-0.05um-final10\"]\n",
    "# temperatures = [425,600,425]\n",
    "# notes = [\"initial6max0.2final9\", \"initial6max0.2final9\", \"initial6max0.6final10\"]\n",
    "\n",
    "# # Target point\n",
    "# location = np.array([-0.1, 0, 0.1-0.015+0.037]) \n",
    "\n",
    "# # MASTER DICTIONARY to store all results securely\n",
    "# results_data: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# # --- Processing Loop ---\n",
    "\n",
    "# # Use enumerate for index 'j' and zip the parameter lists together\n",
    "# for j, (tempIN, folderIN, noteIN) in enumerate(zip(temperatures, folder_path, notes)):\n",
    "\n",
    "#     # Create a unique key for storing results\n",
    "#     key_name = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "#     print(f\"--- Processing {j}: {key_name} from {folderIN} ---\")\n",
    "\n",
    "#     # 1. Read Field Data\n",
    "#     filenames = sorted(glob.glob(f\"{folderIN}/fieldmaps/*{configIN}*.txt\"))\n",
    "#     fields_PE = read_data_format_efficient(filenames, scaling=True) \n",
    "#     first_key = list(fields_PE.keys())[0]\n",
    "    \n",
    "#     # 2. Initialize entry in the results dictionary\n",
    "#     results_data[key_name] = {}\n",
    "    \n",
    "#     # 3. Compute and Store Field at Target Location\n",
    "#     results_data[key_name][\"fieldAtTarget\"] = compute_nearest_field_vector(fields_PE, target=location, start=first_key)\n",
    "    \n",
    "#     # 4. Compute and Store the list of leaf lengths (number of points per iteration)\n",
    "#     results_data[key_name][\"lengthLeaves\"] = np.array([len(fields_PE[keyIN][\"pos\"]) for keyIN in fields_PE.keys()])\n",
    "    \n",
    "#     # If the fields_PE dictionary is very large, deleting it immediately frees memory\n",
    "#     #del fields_PE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### check the minimum distance between point in field map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "directory_path = \"../build-smallerworld-initial8max0.8final13/\"\n",
    "filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "fields_PE = read_data_format_efficient(filenames,scaling=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fields_PE' is your list/dictionary structure and \n",
    "# fields_PE[1]['pos'] is a NumPy array of shape (N, 3), where N is the number of points.\n",
    "# Example data (replace this with your actual data):\n",
    "data_points = fields_PE[1]['pos'] \n",
    "# data_points = np.array([\n",
    "#     [1.0, 1.0, 1.0],\n",
    "#     [1.001, 1.0, 1.0],  # Very close point\n",
    "#     [2.0, 2.0, 2.0],\n",
    "#     [5.0, 5.0, 5.0]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# 1. Build the KD-Tree\n",
    "# This organizes the points in a spatial structure for efficient nearest neighbor search.\n",
    "tree = cKDTree(data_points)\n",
    "\n",
    "# 2. Query for the 2 nearest neighbors of every point\n",
    "# The 'k=2' parameter tells the query to find the distance to the 2 nearest neighbors:\n",
    "# - The 1st neighbor (k=1) is always the point itself (distance = 0.0).\n",
    "# - The 2nd neighbor (k=2) is the closest *other* point.\n",
    "distances, indices = tree.query(data_points, k=2)\n",
    "\n",
    "# 3. Extract the minimum non-zero distance\n",
    "# The minimum distance between any unique pair of points is the minimum value \n",
    "# in the array of distances to the second nearest neighbor (distances[:, 1]).\n",
    "min_distance = np.min(distances[:, 1])*1000\n",
    "\n",
    "print(f\"The total number of points (voxels) is: {len(data_points)}\")\n",
    "print(f\"The minimum distance between any two unique voxels is: {min_distance} um\")\n",
    "\n",
    "# takes around ~10 seconds to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### calculate temperature change over all iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Worker Function for Parallel Execution ---\n",
    "def process_root_file(fileIN: str, directory_path: str, target_volume = \"SiO2\") -> Tuple[int, float]:\n",
    "    \"\"\"Reads a single ROOT file, performs event analysis, and returns the index and calculated total energy.\"\"\"\n",
    "\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    try:\n",
    "        # Read data for the current iteration\n",
    "        df = read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path)\n",
    "\n",
    "    except Exception:\n",
    "        # Catch errors like missing keys or file corruption during read_rootfile\n",
    "        print(f\"-> ERROR: Skipping {fileIN.split('/')[-1]} due to failed file read\")\n",
    "        return iterationNUM, 0.0 # Return 0 energy and the index to maintain order\n",
    "\n",
    "    # 1. Get all incident gamma events (Particle_Type=\"gamma\", Parent_ID=0.0)\n",
    "    incident_gamma = df[(df[\"Particle_Type\"] == \"gamma\") & (df[\"Parent_ID\"] == 0.0)].drop_duplicates(subset=\"Event_Number\", keep=\"first\")\n",
    "\n",
    "    # 2. Get all unique event numbers that resulted in an electron creation\n",
    "    last_e_event = df[(df[\"Particle_Type\"] == \"e-\") & (df[\"Parent_ID\"] > 0.0)].drop_duplicates(subset=\"Event_Number\", keep=\"last\")\n",
    "    event_numbers_with_e_creation = last_e_event[\"Event_Number\"].unique()\n",
    "\n",
    "    # 3. All unique incident gamma event numbers\n",
    "    incident_gamma_event_numbers = incident_gamma[\"Event_Number\"].unique()\n",
    "\n",
    "    # 4. Events where NO electron was created (incident gamma events - events with e- creation)\n",
    "    events_without_photoelectric_e = np.setdiff1d(incident_gamma_event_numbers, event_numbers_with_e_creation)\n",
    "\n",
    "    # 5. Filter the main DataFrame to contain only data from the non-interacting events\n",
    "    events_without_photoelectric_e_df = df[df[\"Event_Number\"].isin(events_without_photoelectric_e)]\n",
    "\n",
    "    # 6. Calculate total energy deposited by gammas that *did not* result in a photoelectric electron\n",
    "    totalEnergy = np.sum(events_without_photoelectric_e_df[\n",
    "        (events_without_photoelectric_e_df[\"Particle_Type\"] == \"gamma\") & \n",
    "        (events_without_photoelectric_e_df[\"Volume_Name_Post\"] == target_volume)\n",
    "    ][\"Kinetic_Energy_Diff_eV\"])\n",
    "\n",
    "    print(f\"-> PROCESSED #{iterationNUM}: {fileIN.split('/')[-1]}\")\n",
    "\n",
    "    return iterationNUM, totalEnergy\n",
    "\n",
    "# --- Configuration ---\n",
    "configIN = \"onlyphotoemission\"\n",
    "directory_path =  \"../build-temp425K-dynamicThreshold/root/\" \n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*.root\"))\n",
    "\n",
    "# --- Main Parallel Execution ---\n",
    "\n",
    "# List to hold the (index, totalEnergy) tuples from parallel processes\n",
    "NUM_FILES = len(filelist)\n",
    "photoEnergyDepositionsforIterations = np.empty(NUM_FILES, dtype=np.float64)\n",
    "\n",
    "print(f\"--- Starting Parallel Processing of {NUM_FILES} files ---\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=NUM_FILES) as executor:\n",
    "    \n",
    "    # Submit tasks, passing the index to ensure results are ordered correctly later\n",
    "    futures = [executor.submit(process_root_file, fileIN, directory_path) for fileIN in filelist]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        index, totalEnergy = future.result()\n",
    "        photoEnergyDepositionsforIterations[index]= totalEnergy\n",
    "\n",
    "# Final assignment to the NumPy array\n",
    "#photoEnergyDepositionsforIterations = np.array([r[1] for r in all_results], dtype=np.float64)\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "initialT = 425\n",
    "heat_capacity = 670+1e3*((initialT-250)/530.6)-1e3*((initialT-250)/498.7)**2 # for lunar regolith\n",
    "density = 2.2/1000 #kg/cm3\n",
    "radius = 5 # assuming that all of the energy is deposited in a 10 um area!!\n",
    "volume = 4/2*np.pi*(radius*1e-4)**3 # cm3\n",
    "mass = volume*density # mass of material\n",
    "# this radius and volume is not true, just calculated as an extreme to see if we need to dynamically adjust the temperature!!\n",
    "\n",
    "print(\"--- Over {NUM_FILES} Iterations ---\")\n",
    "print(f\"Mean Temperature Increase : {np.mean(photoEnergyDepositionsforIterations*1.60218e-19/heat_capacity/mass*100)} K\")\n",
    "print(f\"Total Temperature Increase: {np.sum(photoEnergyDepositionsforIterations*1.60218e-19/heat_capacity/mass*100)} K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### create plots of the face ilumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlysolarwind\"\n",
    "#directory_path =  \"../build-temp425K-dynamicThreshold/root/\" #\"../build-adaptive-barns-fixed/root/\"\n",
    "#directory_path = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output110525/smallerworld-initial8max0.8final12/\"\n",
    "directory_path = \"../build-initial8max0.7final11-wang/root/\"\n",
    "\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*_num50000.root\"))\n",
    "\n",
    "all_protons_inside = []\n",
    "all_electrons_inside = []\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    if iterationNUM > 20:\n",
    "        break\n",
    "\n",
    "    # read data from different iterations\n",
    "    electrons_df, protons_df = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path), config=configIN)\n",
    "\n",
    "    all_protons_inside.append(protons_df)\n",
    "    all_electrons_inside.append(electrons_df)\n",
    "    print(78*\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all iterations into single DataFrames\n",
    "all_protons_inside_df = pd.concat(all_protons_inside, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"/storage/coda1/p-zjiang33/0/shared/avira7/root_files/stacked-sphere/processed-files\" \n",
    "all_protons_inside_df.to_pickle(f'{save_directory}/SW_protons_locations_until20.pkl') # only got through 43 iterations\n",
    "all_electrons_inside_df.to_pickle(f'{save_directory}/SW_electrons_locations_until20.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf, ilm_values,_ = plot_face_illumination(gamma_holes_iteration0, stacked_spheres, vmin=0, vmax=5)\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surf.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "directory_path = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output110525/smallerworld-initial8max0.8final12/root/\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*{configIN}*_num100000.root\"))\n",
    "\n",
    "all_gamma_holes = []\n",
    "all_electrons_inside = []\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    if iterationNUM < 80:\n",
    "        continue\n",
    "\n",
    "    # read data from different iterations\n",
    "    gamma_holes_df, electron_inside_df, _ = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path),\n",
    "                                                                                                             config=configIN)\n",
    "    all_gamma_holes.append(gamma_holes_df)\n",
    "    all_electrons_inside.append(electron_inside_df)\n",
    "    print(78*\"-\")\n",
    "\n",
    "\n",
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"/storage/coda1/p-zjiang33/0/shared/avira7/root_files/stacked-sphere/processed-files\" \n",
    "all_gamma_holes_df.to_pickle(f'{save_directory}/PE_gammaholes_locations.pkl') # only got through 43 iterations\n",
    "all_electrons_inside_df.to_pickle(f'{save_directory}/PE_electronsinside_locations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    \"\"\"\n",
    "    Loads data from a pickle file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Successfully loaded data from {filepath}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pickle file {filepath}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_directory = \"/storage/coda1/p-zjiang33/0/shared/avira7/root_files/stacked-sphere/processed-files\" \n",
    "all_gamma_holes_df=load_pickle(f'{save_directory}/PE_gammaholes_locations.pkl') # only got through 43 iterations\n",
    "all_electrons_inside_df=load_pickle(f'{save_directory}/PE_electronsinside_locations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import trimesh\n",
    " \n",
    "# --- Define physical constants in SI units ---\n",
    "epsilon_0 = 8.854187817e-12  # F/m (Permittivity of free space)\n",
    "q_electron = -1.602176634e-19 # C\n",
    "q_proton = +1.602176634e-19   # C\n",
    " \n",
    "def plot_surface_Txx_from_charge_density(electrons, protons, convex_combined, vmin=-0.01, vmax=0.01):\n",
    "    \"\"\"\n",
    "    Calculates and plots the xx-component of the Maxwell Stress Tensor\n",
    "    T_xx = (sigma^2 / epsilon_0) * (n_x^2 - 0.5)\n",
    "    by first binning particles to faces to find sigma.\n",
    " \n",
    "    Args:\n",
    "        electrons (pd.DataFrame): DataFrame with 'Pre_Step_Position_mm'.\n",
    "        protons (pd.DataFrame): DataFrame with 'Pre_Step_Position_mm'.\n",
    "        convex_combined (trimesh.Trimesh): The mesh object (assumed to be in mm).\n",
    "        vmin (float): Minimum stress (in Pascals) for the color map.\n",
    "        vmax (float): Maximum stress (in Pascals) for the color map.\n",
    " \n",
    "    Returns:\n",
    "        (trimesh.Trimesh, np.array, np.array):\n",
    "        The mesh object with face_colors set,\n",
    "        the 1D array of T_xx per face (in Pascals),\n",
    "        the RGBA color array.\n",
    "    \"\"\"\n",
    " \n",
    "    # --- 1. Initialize Charge and Area Arrays ---\n",
    "    num_faces = len(convex_combined.faces)\n",
    "    # This array will hold the total charge (Q) on each face in Coulombs\n",
    "    face_charges = np.zeros(num_faces)\n",
    "    # Get face areas (in mm^2) and convert to m^2\n",
    "    # (1 mm)^2 = (1e-3 m)^2 = 1e-6 m^2\n",
    "    face_areas_m2 = convex_combined.area_faces * 1e-6\n",
    "    # Add a small epsilon to prevent division by zero for any zero-area faces\n",
    "    face_areas_m2 += 1e-20\n",
    " \n",
    "    # --- 2. Get Particle Positions (in mm) ---\n",
    "    e_pos = np.array(electrons[\"Post_Step_Position_mm\"].tolist())\n",
    "    p_pos = np.array(protons[\"Post_Step_Position_mm\"].tolist())\n",
    " \n",
    "    # --- 3. Bin Electrons and Protons to Faces to get Q_face ---\n",
    "    # Bin electrons\n",
    "    if len(e_pos) > 0:\n",
    "        _, _, face_id_e = convex_combined.nearest.on_surface(e_pos)\n",
    "        unique_faces_e, counts_e = np.unique(face_id_e, return_counts=True)\n",
    "        face_charges[unique_faces_e] += (counts_e * q_electron)\n",
    " \n",
    "    # Bin protons\n",
    "    if len(p_pos) > 0:\n",
    "        _, _, face_id_p = convex_combined.nearest.on_surface(p_pos)\n",
    "        unique_faces_p, counts_p = np.unique(face_id_p, return_counts=True)\n",
    "        face_charges[unique_faces_p] += (counts_p * q_proton)\n",
    " \n",
    "    # --- 4. Calculate Surface Charge Density (sigma) ---\n",
    "    # sigma = Q_face / A_face (in C/m^2)\n",
    "    sigma_per_face = face_charges / face_areas_m2\n",
    "    # Calculate the sigma^2 / epsilon_0 term (in Pascals, N/m^2)\n",
    "    sigma_sq_over_eps = (sigma_per_face**2) / epsilon_0\n",
    " \n",
    "    # --- 5. Calculate T_xx Component ---\n",
    "    # Get the (N_faces, 3) array of unit normal vectors\n",
    "    face_normals = convex_combined.face_normals\n",
    "    # Get the x-component of each normal vector, n_x\n",
    "    # (n_dot_x) is just normals[:, 0]\n",
    "    n_x = face_normals[:, 0]\n",
    "    # Calculate the (n_x^2 - 0.5) term\n",
    "    geometry_term = (n_x**2) - 0.5\n",
    "    # Calculate the final T_xx for each face\n",
    "    face_Txx_values = sigma_sq_over_eps * geometry_term\n",
    " \n",
    "    # --- 6. Apply Colormap ---\n",
    "    colors_rgba = np.zeros((num_faces, 4), dtype=np.uint8)\n",
    "    # T_xx can be positive or negative, so 'seismic' is the correct colormap\n",
    "    cmap = plt.cm.seismic\n",
    "    norm_func = Normalize(vmin=vmin, vmax=vmax)\n",
    "    colors_rgb = cmap(norm_func(face_Txx_values))[:, :3]\n",
    " \n",
    "    # Assign RGB to all faces\n",
    "    colors_rgba[:, :3] = (colors_rgb * 255).astype(np.uint8)\n",
    "    # Set alpha to opaque so faces are visible\n",
    "    colors_rgba[:, 3] = 255\n",
    " \n",
    "    # Apply colors to mesh\n",
    "    convex_combined.visual.face_colors = colors_rgba\n",
    " \n",
    "    return convex_combined, face_Txx_values, colors_rgba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 3\")\n",
    "# input order: gammas, photoelectrons, protons, electrons, convex_combined,\n",
    "surface,facecolors,_ = plot_surface_Txx_from_charge_density(all_electrons_inside_df, all_gamma_holes_df, stacked_spheres, vmin=-0.01,vmax=0.01)\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_face_illumination(incident_particles, convex_combined, vmin=0, vmax=1):\n",
    "    \"\"\"\n",
    "    Colors the mesh faces based on how many particles are nearest to each face.\n",
    "    \"\"\"\n",
    "    # Step 1: Get particle positions as Nx3 array\n",
    "    points = np.array(incident_particles[\"Post_Step_Position_mm\"].tolist())\n",
    "\n",
    "    # Step 2: Get nearest face indices\n",
    "    _, _, face_ids = convex_combined.nearest.on_surface(points)\n",
    "\n",
    "    # Step 3: Accumulate per-face \"illumination\"\n",
    "    face_illum = np.zeros(len(convex_combined.faces))\n",
    "    np.add.at(face_illum, face_ids, 1)\n",
    "\n",
    "    # Step 4: Normalize face illumination (optional)\n",
    "    norm_f = face_illum.astype(float)\n",
    "\n",
    "    # Step 5: Apply colormap\n",
    "    cmap = plt.cm.OrRd\n",
    "    norm_func = Normalize(vmin=vmin, vmax=vmax)\n",
    "    rgb_f = cmap(norm_func(norm_f))[:, :3]\n",
    "\n",
    "    # Step 6: Build RGBA face colors\n",
    "    colors_f = np.zeros((len(face_illum), 4), dtype=np.uint8)\n",
    "    colors_f[:, :3] = (rgb_f * 255).astype(np.uint8)\n",
    "    colors_f[:, 3] = 255  # Fully opaque\n",
    "\n",
    "    # Step 7: Apply colors to faces\n",
    "    convex_combined.visual.face_colors = colors_f\n",
    "\n",
    "    return convex_combined, face_illum, colors_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf, ilm_values,_ = plot_face_illumination(all_electrons_inside_df, stacked_spheres, vmin=0, vmax=400)\n",
    "print(max(ilm_values))\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surf.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_saturation(t, a, b, tau):\n",
    "    return a - b * np.exp(-t / tau)\n",
    "\n",
    "\n",
    "# Initial parameter guess: a, b, tau\n",
    "initial_guess = [4.0, 2.0, 1.0]\n",
    "popt, pcov = curve_fit(exp_saturation, 10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], p0=initial_guess)\n",
    "\n",
    "# Plot\n",
    "plt.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"],'--',label=\"SW\",color=\"r\",lw=4)\n",
    "plt.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"],'g:',label=\"PE\")\n",
    "plt.plot(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"],'b:',label=\"PE+SW\")\n",
    "plt.plot((efield_PE[\"iter\"] - 101)*convert_iteration_PEtime, abs(mag_values_PE), 'k.-',label=\"Geant4: PE (different incident)\",lw=0.5)\n",
    "#plt.semilogx((efield_PE_noholes[\"iter\"] - 101)*convert_iteration_PEtime2, abs(mag_values_PE_noholes), 'b.-',label=\"Geant4: PE (no holes)\",lw=0.5)\n",
    "#plt.plot((efield_PE_normal[\"iter\"] - 101)*convert_iteration_PEtime, abs(mag_values_PE_normal), 'g.-',label=\"Geant4: PE\",lw=0.5)\n",
    "#plt.plot((efield_SW[\"iter\"] - 1)*convert_iteration_SWtime, mag_values_SW, 'k.-',label=\"Geant4: SW\",lw=0.5)\n",
    "\n",
    "# xdata_zimmerman = np.linspace(1e-2,10,100)\n",
    "# y_fit_zimmerman = exp_saturation(xdata_zimmerman, *popt)\n",
    "# #plt.plot(xdata_zimmerman, y_fit_zimmerman, 'r-',lw=0.2)\n",
    "\n",
    "# # Initial parameter guess: a, b, tau\n",
    "# initial_guess = [4.0, 2.0, 1.0]\n",
    "# popt, pcov = curve_fit(exp_saturation,(efield_PE[\"iter\"] - 101)*convert_iteration_PEtime, abs(mag_values_PE), p0=popt)\n",
    "\n",
    "# xdata = np.linspace(1e-1,5,100)\n",
    "# y_fit = exp_saturation(xdata, *popt)\n",
    "# plt.plot(xdata, y_fit, 'g-',lw=0.2, label=\"prediction\")\n",
    "# #plt.plot(xdata_zimmerman, y_fit_zimmerman-3.8e4, 'r-',lw=0.2)\n",
    "# plt.axvline(x=4)\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(r\"|E| (V/m)\")\n",
    "#plt.ylabel(r\"E$_x$ (V/m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ilm_values, bins=np.logspace(-1,3,100))\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"# of Photons hitting each Voxel\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Distribution Stats: mean {np.mean(ilm_values[ilm_values!=0])}, median {np.median(ilm_values[ilm_values!=0])}, max {np.max(ilm_values)}\")\n",
    "\n",
    "# our area is a factor of 4 smaller than their area \n",
    "print(f\"for one iteration, median # of particles in each equivalently sized voxel is {np.median(ilm_values[ilm_values!=0])/4}\")\n",
    "voxel_area = 0.0004/(1000)**2 # rough area approximated from python (0.4 micron2)\n",
    "zimmerman_charge = 0.5*(1e-6) # C/m2\n",
    "zimmerman_electronnum = (zimmerman_charge/1.60217663e-19)*voxel_area\n",
    "print(f\"will take {zimmerman_electronnum/(np.max(ilm_values[ilm_values!=0])/4)} iterations at this rate to get to the photoemission flux ranges shown at 3 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 33\")\n",
    "surface,pot = plot_surface_potential_fornegativepositive_charge(all_electrons_inside_df, all_gamma_holes_df, stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "print(min(pot),max(pot))\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.2, -0.2, 0])\n",
    "bbox_max = np.array([ 0.2,  0.2,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((stacked_spheres.vertices >= bbox_min) & \n",
    "                (stacked_spheres.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[stacked_spheres.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = stacked_spheres.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap and normalization\n",
    "cmap = plt.cm.seismic\n",
    "norm = Normalize(vmin=-0.2, vmax=0.2)\n",
    "\n",
    "# Create a figure and a single axis for the colorbar\n",
    "fig, ax = plt.subplots(figsize=(4, 0.5))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "# Create the colorbar\n",
    "cb = ColorbarBase(ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "cb.set_label('Surface Potential (mV)')  # Optional label\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "## Case 3: all particles (incident e-, protons, gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"../build-sphere-charging/root/\"\n",
    "configIN = \"allparticles\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*stackediteration*{configIN}*num5000.root\"))\n",
    "\n",
    "all_gamma_holes,all_photoemission_electrons,all_protons_inside,all_electrons_inside = [],[],[],[]\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"gamma_holes_\"+str(number_str)], vars()[\"photoemission_electrons_inside_\"+str(number_str)], \\\n",
    "        vars()[\"protons_inside_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)] = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path), \\\n",
    "                                                                                                             config=configIN)\n",
    "    all_gamma_holes.append(vars()[\"gamma_holes_\"+str(number_str)])\n",
    "    all_photoemission_electrons.append(vars()[\"photoemission_electrons_inside_\"+str(number_str)])\n",
    "    all_protons_inside.append(vars()[\"protons_inside_\"+str(number_str)])\n",
    "    all_electrons_inside.append(vars()[\"electrons_inside_\"+str(number_str)])\n",
    "    print(78*\"-\")\n",
    "\n",
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_photoemission_electrons_df = pd.concat(all_photoemission_electrons, ignore_index=True)\n",
    "all_protons_inside_df = pd.concat(all_protons_inside, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_photoemission_electrons_df = pd.concat(all_photoemission_electrons, ignore_index=True)\n",
    "all_protons_inside_df = pd.concat(all_protons_inside, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 28\")\n",
    "# input order: gammas, photoelectrons, protons, electrons, convex_combined,\n",
    "surface,facecolors = plot_electric_pressure_from_charge_density(all_gamma_holes_df, all_photoemission_electrons_df, all_protons_inside_df, all_electrons_inside_df, \n",
    "                                                      stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.2, -0.3, 0])\n",
    "bbox_max = np.array([ 0.2,  0.1,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap and normalization\n",
    "cmap = plt.cm.seismic\n",
    "norm = Normalize(vmin=-0.2, vmax=0.2)\n",
    "\n",
    "# Create a figure and a single axis for the colorbar\n",
    "fig, ax = plt.subplots(figsize=(4, 0.5))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "# Create the colorbar\n",
    "cb = ColorbarBase(ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "cb.set_label('Surface Potential (mV)')  # Optional label\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 19: only SW ions\")\n",
    "surface,facecolors = plot_surface_potential_fornegativepositive_charge(all_electrons_inside_df, all_protons_inside_df, stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 22: only photons\")\n",
    "surface,facecolors = plot_surface_potential_fornegativepositive_charge(all_photoemission_electrons_df, all_gamma_holes_df, stacked_spheres, vmin=-0.5,vmax=0.5)\n",
    "\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.2, -0.3, 0])\n",
    "bbox_max = np.array([ 0.2,  0.1,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
