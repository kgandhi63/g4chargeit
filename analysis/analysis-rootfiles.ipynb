{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import concurrent.futures # Added for parallel execution\n",
    "\n",
    "from scipy.constants import epsilon_0, e as q_e\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.collections import LineCollection\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "import trimesh\n",
    "from trimesh.points import PointCloud\n",
    "\n",
    "from common_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_spheres = trimesh.load_mesh('../sphere-charging/geometry/stacked_spheres_frompython_cropped.stl') \n",
    "#stacked_spheres.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Visual Representation of the Electric Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "iteration = 2\n",
    "filenames = sorted(glob.glob(f\"../build-600K-Max80-0.05um/fieldmaps/*{configIN}*.txt\"))\n",
    "print(filenames)\n",
    "\n",
    "df  = read_data_format_efficient(filenames,scaling=True)\n",
    "\n",
    "# check to make sure this matches the total nodes in outputlogs\n",
    "#len(df[iteration][\"E_mag\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the field to get a sense for the overall field values before analyzing the field in detail\n",
    "for iteration, thresholdIN in zip(df.keys(), [93.6167, 101.242, 220.685]):\n",
    "    length = len(df[iteration][\"E_mag\"])\n",
    "    plt.hist(df[iteration][\"E_mag\"][df[iteration][\"E_mag\"]>0],bins=np.logspace(0,8,100),alpha=0.5,label=f\"{iteration}: total leaves ={length}\")\n",
    "    plt.axvline(x=thresholdIN*1e3, linestyle=\":\")\n",
    "#plt.hist(df2[df2[\"E_mag\"]>0][\"E_mag\"],bins=np.logspace(-10,8,100),alpha=0.2,label=\"Iteration 78\")\n",
    "#plt.axvline(x=3e2,color=\"k\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"|E| (V/m)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Temperature: 600 K\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Scene Initialization and Data Loading ---\n",
    "\n",
    "# Retrieve the field data dictionary for the current iteration (assumes 'df' is a list/dict)\n",
    "fieldIN = df[iteration]\n",
    "# Define the threshold for filtering electric field magnitude (E_mag)\n",
    "threshold = 3e4 \n",
    "# Set the maximum number of arrows to plot to maintain performance\n",
    "max_arrows = 5000\n",
    "\n",
    "# Initialize the 3D scene by plotting the geometry (e.g., detector structure)\n",
    "# 'stacked_spheres' is the geometry to plot.\n",
    "# 'edge_color' is set to black with an alpha (transparency) value of 350 (out of 511 max for trimesh visual.face_colors/edge_colors)\n",
    "scene = plot_trimesh_edges_only(stacked_spheres, edge_color=[0, 0, 0, 350]) \n",
    "\n",
    "## ----------------------------------------------------\n",
    "## --- 2. Filtering and Sampling High-Magnitude Points ---\n",
    "## ----------------------------------------------------\n",
    "\n",
    "# --- Filtering ---\n",
    "# Create a boolean mask for all points where E_mag exceeds the threshold\n",
    "large_magnitude_mask = fieldIN['E_mag'] > threshold\n",
    "\n",
    "# Apply the mask to all three arrays ('pos', 'E', 'E_mag') simultaneously\n",
    "# to create a new dictionary containing only the high-field points.\n",
    "field_largevalues_masked = {\n",
    "    'pos': fieldIN['pos'][large_magnitude_mask],\n",
    "    'E': fieldIN['E'][large_magnitude_mask],\n",
    "    'E_mag': fieldIN['E_mag'][large_magnitude_mask]\n",
    "}\n",
    "\n",
    "# Get the count of data points after initial filtering\n",
    "N_large = len(field_largevalues_masked['E_mag'])\n",
    "\n",
    "# --- Sampling ---\n",
    "if N_large > max_arrows:\n",
    "    \n",
    "    # Randomly select a subset of indices to stay below the 'max_arrows' limit.\n",
    "    # np.random.choice is the NumPy equivalent of a DataFrame's .sample() method.\n",
    "    np.random.seed(42) # Set seed for reproducible sampling\n",
    "    sample_indices = np.random.choice(\n",
    "        N_large,         # Range of indices to choose from (0 to N_large - 1)\n",
    "        size=max_arrows, # The number of indices to select\n",
    "        replace=False    # Ensure each index is chosen only once\n",
    "    )\n",
    "    \n",
    "    # Apply the sample indices to all arrays to create the final data dictionary for plotting\n",
    "    field_plot = {\n",
    "        'pos': field_largevalues_masked['pos'][sample_indices],\n",
    "        'E': field_largevalues_masked['E'][sample_indices],\n",
    "        'E_mag': field_largevalues_masked['E_mag'][sample_indices]\n",
    "    }\n",
    "    \n",
    "    print(f\"Sampled down to {max_arrows} points from {N_large} large-magnitude points.\")\n",
    "\n",
    "else:\n",
    "    # If the filtered data is small enough, use it directly without sampling\n",
    "    field_plot = field_largevalues_masked\n",
    "    print(f\"Used all {N_large} large-magnitude points for plotting.\")\n",
    "\n",
    "\n",
    "## ----------------------------------------------------\n",
    "## --- 3. Normalization and Coloring Setup ---\n",
    "## ----------------------------------------------------\n",
    "\n",
    "# Get the vector field directions and magnitudes for the sampled points\n",
    "directions = field_plot[\"E\"]\n",
    "\n",
    "# Calculate unit vectors (normalized directions)\n",
    "# Use np.where to prevent division by zero if E_mag is exactly zero (though it shouldn't be after filtering)\n",
    "directions_unit = np.where(\n",
    "    field_plot[\"E_mag\"][:, None] > 0, \n",
    "    directions / field_plot[\"E_mag\"][:, None], \n",
    "    0\n",
    ")\n",
    "\n",
    "# Use Logarithmic scaling for magnitude visualization\n",
    "# Add a small epsilon (1e-12) before log to avoid log(0), which results in -inf\n",
    "log_magnitudes = np.log10(fieldIN['E_mag'][fieldIN['E_mag'] > 1e4]) \n",
    "\n",
    "# Normalize log-magnitudes to the range [0, 1] for colormapping\n",
    "# np.ptp (peak-to-peak) is range (max - min)\n",
    "min_log = log_magnitudes.min()\n",
    "ptp_log = np.ptp(log_magnitudes)\n",
    "\n",
    "# The normalization uses the min/ptp of the *sampled* data, not the full dataset\n",
    "# A small epsilon is added to the divisor to prevent division by zero in case ptp is 0\n",
    "log_magnitudes = np.log10(field_plot['E_mag']) \n",
    "norm_magnitudes = (log_magnitudes - min_log) / (ptp_log + 1e-12)\n",
    "\n",
    "print(f\"Log(E_mag) Min: {min_log}, Range (PtP): {ptp_log}\")\n",
    "\n",
    "# Choose a Colormap (e.g., 'jet')\n",
    "cmap = plt.cm.jet\n",
    "# Map the normalized magnitudes (0 to 1) to colors (RGBA floats 0.0 to 1.0)\n",
    "colors_rgba = cmap(norm_magnitudes)\n",
    "\n",
    "# Convert the RGBA colors from floats (0.0-1.0) to 8-bit integers (0-255) for trimesh\n",
    "colors_rgba = (colors_rgba * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "## ----------------------------------------------------\n",
    "## --- 4. Arrow Creation and Visualization ---\n",
    "## ----------------------------------------------------\n",
    "\n",
    "# Define base dimensions for the visualization\n",
    "base_arrow_length = 0.05  # Base length before scaling by magnitude\n",
    "arrow_radius = 0.001       # Radius of the arrow shaft\n",
    "cone_ratio = 0.2           # Ratio of the cone length to the total arrow length\n",
    "\n",
    "# Scale the base length by the normalized magnitude for visual encoding\n",
    "scaling_factor = 0.5  # Overall factor to control arrow visibility\n",
    "scaled_lengths = base_arrow_length * scaling_factor * norm_magnitudes\n",
    "\n",
    "# Iterate over each sampled point to create and place an arrow\n",
    "for pos, dir_vec, color, magnitude_norm, scaled_length in zip(\n",
    "    field_plot[\"pos\"], \n",
    "    directions_unit, \n",
    "    colors_rgba, \n",
    "    norm_magnitudes,\n",
    "    scaled_lengths\n",
    "):\n",
    "    # Skip if the direction vector has zero magnitude (E_mag was not filtered perfectly or is near zero)\n",
    "    if np.linalg.norm(dir_vec) == 0:\n",
    "        continue\n",
    "\n",
    "    # Calculate arrow dimensions\n",
    "    arrow_length = scaled_length\n",
    "    cone_length = arrow_length * cone_ratio\n",
    "    shaft_length = arrow_length - cone_length\n",
    "\n",
    "    # Create Arrow Geometry in trimesh (built along the Z-axis by default)\n",
    "\n",
    "    # 1. Create shaft (cylinder)\n",
    "    shaft = trimesh.creation.cylinder(radius=arrow_radius, height=shaft_length, sections=12)\n",
    "    # Move the shaft so its base is at z=0\n",
    "    shaft.apply_translation([0, 0, shaft_length / 2]) \n",
    "\n",
    "    # 2. Create cone (cone)\n",
    "    cone = trimesh.creation.cone(radius=arrow_radius * 2, height=cone_length, sections=12)\n",
    "    # Position the cone base to touch the top of the shaft\n",
    "    cone.apply_translation([0, 0, shaft_length])\n",
    "\n",
    "    # 3. Combine parts into a single trimesh object\n",
    "    arrow = trimesh.util.concatenate([shaft, cone])\n",
    "\n",
    "    # Set the computed color (scaled by magnitude) for all faces of the arrow\n",
    "    arrow.visual.face_colors = np.tile(color, (arrow.faces.shape[0], 1))\n",
    "\n",
    "    # Calculate the necessary transformation to place and orient the arrow\n",
    "\n",
    "    # a. Compute rotation matrix to align the default Z-axis ([0, 0, 1]) to the direction vector (dir_vec)\n",
    "    transform = trimesh.geometry.align_vectors([0, 0, 1], dir_vec)\n",
    "    \n",
    "    # b. Set the translation part of the transformation matrix (the arrow's position)\n",
    "    transform[:3, 3] = pos\n",
    "    \n",
    "    # c. Apply the full rotation and translation\n",
    "    arrow.apply_transform(transform)\n",
    "\n",
    "    # Add the colored and positioned arrow to the visualization scene\n",
    "    scene.add_geometry(arrow)\n",
    "\n",
    "# Display the final scene containing the geometry and the vector field arrows\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine appropiate grid spacing for Electric Field calculations\n",
    "\n",
    "# Flatten the mesh grid to create sampling points\n",
    "sampling_points = fieldIN[\"pos\"]\n",
    "print(len(sampling_points))\n",
    "adaptive_grid_points = trimesh.points.PointCloud(sampling_points, colors=[0, 255, 0, 255])\n",
    "\n",
    "scene = plot_trimesh_edges_only(stacked_spheres, edge_color=[0, 0, 0, 128])\n",
    "scene.add_geometry([adaptive_grid_points])\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked_spheres_cropped = stacked_spheres.copy()\n",
    "\n",
    "# bbox_min = np.array([-0.2, -0.3, 0])\n",
    "# bbox_max = np.array([ 0.2,  0.1,  100])\n",
    "\n",
    "# # Filter vertices\n",
    "# in_box = np.all((stacked_spheres_cropped.vertices >= bbox_min) & \n",
    "#                 (stacked_spheres_cropped.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# # Get face indices where all 3 vertices are in the box\n",
    "# face_mask = np.all(in_box[stacked_spheres_cropped.faces], axis=1)\n",
    "\n",
    "# # Extract submesh\n",
    "# stacked_spheres_cropped = stacked_spheres_cropped.submesh([face_mask], only_watertight=False, append=True)\n",
    "\n",
    "# Usage with black semi-transparent edges\n",
    "# scene = plot_trimesh_edges_only(stacked_spheres_cropped, edge_color=[0, 0, 0, 350])  # Black, 50% transparent\n",
    "\n",
    "# # Coordinates for your \"points\"\n",
    "# point_coords = np.array([[-0.1, 0, 0.1-0.015+0.037]])\n",
    "# # Create small spheres at each point\n",
    "# spheres = []\n",
    "# for point in point_coords:\n",
    "#     sphere = trimesh.creation.icosphere(radius=0.006)  # adjust radius for point size\n",
    "#     sphere.apply_translation(point)\n",
    "#     sphere.visual.face_colors = [255, 0, 0, 255]  # red spheres\n",
    "#     spheres.append(sphere)\n",
    "\n",
    "# # Combine all meshes into a scene\n",
    "# scene.add_geometry(spheres)\n",
    "\n",
    "# Create a PointCloud object\n",
    "#gamma_photoemission_sites = trimesh.points.PointCloud(np.array(all_gamma_holes_df[\"Pre_Step_Position_mm\"].tolist()), colors=[0, 255, 0, 255])  # RGBA green points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Case 1: SW electrons and ions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### plot the field over each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlysolarwind\"\n",
    "\n",
    "# --- Configurations ---\n",
    "folder_path = [ \"../build-temp600K-dynamicThreshold\",\"../build-temp425K-dynamicThreshold\", \\\n",
    "               \"../build-425K-nodissipation-initial6-0.8Max-final9\", \"../build-425K-withoutdissipation\"]\n",
    "temperatures = [600,425,425,425]\n",
    "notes = [\"initial6max0.2final9(dissipation)\",\"initial6max0.2final9(dissipation)\",\"initial6max0.7final9(noDissipation)\",\"initial5max0.8final9(noDissipation)\"]\n",
    "\n",
    "# Target point (Fixed for all configurations)\n",
    "location = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) \n",
    "\n",
    "# --- Parallel Processing Worker Function ---\n",
    "\n",
    "def process_config(tempIN, folderIN, noteIN, configIN, location):\n",
    "    \"\"\"\n",
    "    Worker function to process a single configuration. \n",
    "    Returns the unique key and the calculated results.\n",
    "    \"\"\"\n",
    "    key_name = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    print(f\"--- Processing {key_name} in {folderIN} (Worker Process) ---\\n\")\n",
    "\n",
    "    # 1. Read Field Data\n",
    "    filenames = sorted(glob.glob(f\"{folderIN}/fieldmaps/*{configIN}*.txt\"))\n",
    "    fields_SW = read_data_format_efficient(filenames, scaling=True) \n",
    "    first_key = list(fields_SW.keys())[0]\n",
    "\n",
    "    # Prepare dictionary for this single result\n",
    "    config_results: Dict[str, Any] = {}\n",
    "    \n",
    "    # 2. Compute and Store Field at Target Location\n",
    "    # The function now returns a dict {'iter', 'E', 'E_mag'} for all iterations\n",
    "    config_results[\"fieldAtTarget\"] = compute_nearest_field_vector(fields_SW, target=location, start=first_key)\n",
    "    \n",
    "    # 3. Compute and Store the list of leaf lengths (number of points per iteration)\n",
    "    config_results[\"lengthLeaves\"] = [len(fields_SW[keyIN][\"pos\"]) for keyIN in fields_SW.keys()]\n",
    "    config_results[\"gradRefinements\"] = [fields_SW[keyIN][\"gradRefinements\"] for keyIN in fields_SW.keys()]\n",
    "    \n",
    "    # If the fields_PE dictionary is very large, deleting it immediately frees memory\n",
    "    del fields_SW \n",
    "    \n",
    "    # Return the key and the results to the main thread\n",
    "    return key_name, config_results\n",
    "\n",
    "# MASTER DICTIONARY to store all results securely\n",
    "results_data: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# --- Parallel Processing Loop ---\n",
    "\n",
    "# Prepare the list of arguments for the executor\n",
    "configs = zip(temperatures, folder_path, notes)\n",
    "args_list = [(temp, folder, note, configIN, location) for temp, folder, note in configs]\n",
    "\n",
    "MAX_WORKERS = len(temperatures) # Use one worker per configuration\n",
    "\n",
    "print(f\"--- Starting Parallel Processing with {MAX_WORKERS} workers ---\")\n",
    "\n",
    "# Use ProcessPoolExecutor for CPU-bound tasks\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    \n",
    "    # Submit all tasks and store the future objects\n",
    "    futures = [executor.submit(process_config, *args) for args in args_list]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            key, data = future.result()\n",
    "            results_data[key] = data\n",
    "        except Exception as exc:\n",
    "            print(f'Configuration generated an exception: {exc}')\n",
    "\n",
    "print(f\"--- Completed Parallel Processing ---\")\n",
    "\n",
    "# takes 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting and Analysis ---\n",
    "\n",
    "print(\"\\n--- Generating Plot for SW Comparison ---\")\n",
    "\n",
    "# 1. Load comparison data from Zimmerman\n",
    "zimmerman_SWdata = pd.read_csv(\"Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"Fig7a-PE.csv\")\n",
    "zimmerman_PEandSWdata = pd.read_csv(\"Fig7a-PE+SW.csv\")\n",
    "\n",
    "# 2. Define constants and calculate conversion factor\n",
    "# Simulation world area size in m^2 (600 um x 600 um)\n",
    "WORLD_XY_AREA_SQ_M = 600 * 600 / (1e6**2) \n",
    "\n",
    "# Number of particles (protons) injected into the active area per iteration\n",
    "PARTICLES_PER_ITERATION = 160440\n",
    "\n",
    "# Ion flux calculated from the simulation area (ions/m^2)\n",
    "FLUX_PER_ITERATION = PARTICLES_PER_ITERATION / WORLD_XY_AREA_SQ_M \n",
    "\n",
    "# Photoemission (PE) ion flux value (e/m^2/s). This factor combines \n",
    "# the effective current (4 uA/cm^2) and conversion to e/m^2/s.\n",
    "SW_ION_FLUX = 3e-7 * 6.241509e18 \n",
    "\n",
    "# Conversion factor: Time (s) per simulation iteration\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION / SW_ION_FLUX\n",
    "print(f\"Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "# 3. Define plot parameters (Colors)\n",
    "# Choose a continuous colormap and generate discrete colors based on the number of temperatures\n",
    "CMAP_NAME = 'jet' \n",
    "discrete_cmap = plt.get_cmap(CMAP_NAME, len(temperatures))\n",
    "color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, len(temperatures))]\n",
    "\n",
    "# 4. Generate Plot (Log-Log)\n",
    "plt.figure()\n",
    "\n",
    "# Plot reference data (Zimmerman)\n",
    "plt.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"], '--', label=\"SW (Zimmerman 2016)\", color=\"r\", lw=4)\n",
    "plt.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], 'g:', label=\"PE (Zimmerman 2016)\")\n",
    "plt.loglog(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"], 'b:', label=\"PE+SW (Zimmerman 2016)\")\n",
    "\n",
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.plot((results_data[key][\"fieldAtTarget\"][\"iter\"] -0.5)* CONVERT_ITERATION_PE_TIME, \n",
    "             results_data[key][\"fieldAtTarget\"][\"E\"][:,0], #results_data[key][\"fieldAtTarget\"][\"E_mag\"] \n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "# Plotting the E-field magnitude (|E|)\n",
    "plt.ylabel(r\"$|E_x$| (V/m)\") \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set axes limits\n",
    "plt.ylim(4.8e3, 2.8e5)\n",
    "plt.xlim(7.4e-2, 1.25e1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"lengthLeaves\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Total Leaf Nodes (millions)\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.title(\"SW Case\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"SW_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"gradRefinements\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Gradient Refinements (millions)\")\n",
    "plt.title(\"SW Case\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "# configIN = \"onlysolarwind\"\n",
    "# directory_path = \"../build-leakage/\" # takes 12 minutes to read in with this data\n",
    "# #directory_path = \"../build-adaptive-barns-fixed/\"\n",
    "\n",
    "# filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "# fields_SW = read_data_format_efficient(filenames,scaling=True)   \n",
    "# \n",
    "# # Target point\n",
    "# location = np.array([-0.1, 0, 0.1-0.015+0.037]) \n",
    "# # return the electric field at that location\n",
    "# Efield_SW_location = compute_nearest_field_vector(fields_SW, target=location, start=1)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### check the minimum distance between point in field map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "directory_path = \"../build-425K-nodissipation-initial6-0.8Max-final9/\"\n",
    "filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "fields_SW = read_data_format_efficient(filenames,scaling=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fields_PE' is your list/dictionary structure and \n",
    "# fields_PE[1]['pos'] is a NumPy array of shape (N, 3), where N is the number of points.\n",
    "# Example data (replace this with your actual data):\n",
    "data_points = fields_SW[1]['pos'] \n",
    "# data_points = np.array([\n",
    "#     [1.0, 1.0, 1.0],\n",
    "#     [1.001, 1.0, 1.0],  # Very close point\n",
    "#     [2.0, 2.0, 2.0],\n",
    "#     [5.0, 5.0, 5.0]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# 1. Build the KD-Tree\n",
    "# This organizes the points in a spatial structure for efficient nearest neighbor search.\n",
    "tree = cKDTree(data_points)\n",
    "\n",
    "# 2. Query for the 2 nearest neighbors of every point\n",
    "# The 'k=2' parameter tells the query to find the distance to the 2 nearest neighbors:\n",
    "# - The 1st neighbor (k=1) is always the point itself (distance = 0.0).\n",
    "# - The 2nd neighbor (k=2) is the closest *other* point.\n",
    "distances, indices = tree.query(data_points, k=2)\n",
    "\n",
    "# 3. Extract the minimum non-zero distance\n",
    "# The minimum distance between any unique pair of points is the minimum value \n",
    "# in the array of distances to the second nearest neighbor (distances[:, 1]).\n",
    "min_distance = np.min(distances[:, 1])*1000\n",
    "\n",
    "print(f\"The total number of points (voxels) is: {len(data_points)}\")\n",
    "print(f\"The minimum distance between any two unique voxels is: {min_distance} um\")\n",
    "\n",
    "# takes around ~10 seconds to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### calculate # of iterations for direct comparison with Zimmerman:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory_path = \"../build-adaptive-barns/root/\"\n",
    "directory_path = \"../build-leakage-425K-finerbinning/root/\"\n",
    "configIN = \"solarwind\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*num500000.root\"))\n",
    "\n",
    "all_incident_protons_inside, all_incident_electrons_inside = [],[]\n",
    "\n",
    "for fileIN in filelist:\n",
    "\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"protons_inside_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)] = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path), \n",
    "                                                                                                             config=configIN)\n",
    "    all_incident_protons_inside.append(vars()[\"protons_inside_\"+str(number_str)])\n",
    "    all_incident_electrons_inside.append(vars()[\"electrons_inside_\"+str(number_str)])\n",
    "    print(78*\"-\")\n",
    "\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all iterations into single DataFrames\n",
    "all_incident_protons_inside_df = pd.concat(all_incident_protons_inside, ignore_index=True)\n",
    "all_incident_electrons_inside_df = pd.concat(all_incident_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf, ilm_values,_ = plot_face_illumination(electrons_inside_stackediteration0, stacked_spheres, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ilm_values, bins=np.logspace(-1,3,100))\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"# of Photons hitting each Voxel\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Distribution Stats: mean {np.mean(ilm_values[ilm_values!=0])}, median {np.median(ilm_values[ilm_values!=0])}, max {np.max(ilm_values)}\")\n",
    "\n",
    "# our area is a factor of 4 smaller than their area \n",
    "#print(f\"for one iteration, mean # of particles in each equivalently sized voxel is {np.mean(ilm_values[ilm_values!=0])}\") <- no longer needed, made voxels similar sizes\n",
    "voxel_area = 0.0004/(1000)**2 # rough area approximated from python (0.4 micron2)\n",
    "zimmerman_charge = 1*(1e-6) # C/m2\n",
    "zimmerman_electronnum = (zimmerman_charge/1.60217663e-19)*voxel_area\n",
    "print(f\"will take {zimmerman_electronnum/(np.mean(ilm_values[ilm_values!=0]))} iterations at this rate to get to the photoemission flux ranges shown at 3 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### make a movie of all surface potential for each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"figures/solarwind/\"\n",
    "\n",
    "for num in range(0, iterationNUM+1):\n",
    "    surface, _, facecolors = plot_surface_potential_fornegativepositive_charge(\n",
    "        vars()[\"electrons_inside_stackediteration\"+str(num)], \n",
    "        vars()[\"protons_inside_stackediteration\"+str(num)], \n",
    "        stacked_spheres, \n",
    "        vmin=-1.8, vmax=1.8\n",
    "    )\n",
    "\n",
    "    surface_edited = surface.copy()\n",
    "    surface_edited.unmerge_vertices()  # Ensure unique vertices per face\n",
    "\n",
    "    # Crop bounding box\n",
    "    bbox_min = np.array([-0.2, -0.3, 0])\n",
    "    bbox_max = np.array([ 0.2,  0.1, 100])\n",
    "\n",
    "    in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                    (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "    face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "    cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "    cropped_colors = facecolors[face_mask]/255  # Crop facecolors to match cropped mesh\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    mesh = Poly3DCollection(cropped.triangles, alpha=1.0)\n",
    "    mesh.set_facecolor(cropped_colors)  # Apply correct colors\n",
    "    mesh.set_edgecolor('k')          # edge color\n",
    "    mesh.set_linewidths(0.1)         # edge line width\n",
    "\n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_title(f\"Iteration {num}\")\n",
    "    ax.title.set_position((0.5, 0.1))  # manually control title position\n",
    "\n",
    "    # Scale\n",
    "    scale = surface_edited.bounds.flatten()\n",
    "    ax.auto_scale_xyz(scale, scale, scale)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    filename = f\"{directory}iteration_{num}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    if num%10==0:\n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "print(f\"All plots saved to {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"figures/solarwind/\"\n",
    "\n",
    "for num in range(0, iterationNUM + 1):\n",
    "    # Get face illumination for electrons and protons\n",
    "    surface_e, _, facecolors_e = plot_face_illumination(\n",
    "        vars()[\"electrons_inside_stackediteration\" + str(num)],\n",
    "        stacked_spheres, vmin=0, vmax=100\n",
    "    )\n",
    "\n",
    "    surface_p, _, facecolors_p = plot_face_illumination(\n",
    "        vars()[\"protons_inside_stackediteration\" + str(num)],\n",
    "        stacked_spheres, vmin=0, vmax=100\n",
    "    )\n",
    "\n",
    "    def crop_and_prepare(surface, facecolors):\n",
    "        surface = surface.copy()\n",
    "        surface.unmerge_vertices()\n",
    "\n",
    "        # Crop bounding box\n",
    "        bbox_min = np.array([-0.2, -0.3, 0])\n",
    "        bbox_max = np.array([ 0.2,  0.1, 100])\n",
    "\n",
    "        in_box = np.all((surface.vertices >= bbox_min) & \n",
    "                        (surface.vertices <= bbox_max), axis=1)\n",
    "\n",
    "        face_mask = np.all(in_box[surface.faces], axis=1)\n",
    "        cropped = surface.submesh([face_mask], only_watertight=False, append=True)\n",
    "        cropped_colors = facecolors[face_mask] / 255.0\n",
    "        return cropped, cropped_colors\n",
    "\n",
    "    cropped_e, colors_e = crop_and_prepare(surface_e, facecolors_e)\n",
    "    cropped_p, colors_p = crop_and_prepare(surface_p, facecolors_p)\n",
    "\n",
    "    # Plotting both side by side\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "    for i, (cropped, colors, title) in enumerate([\n",
    "        (cropped_e, colors_e, \"Electron Illumination\"),\n",
    "        (cropped_p, colors_p, \"Proton Illumination\")\n",
    "    ]):\n",
    "        ax = fig.add_subplot(1, 2, i + 1, projection='3d')\n",
    "        mesh = Poly3DCollection(cropped.triangles, alpha=1.0)\n",
    "        mesh.set_facecolor(colors)\n",
    "        mesh.set_edgecolor('k')\n",
    "        mesh.set_linewidths(0.1)\n",
    "        ax.add_collection3d(mesh)\n",
    "\n",
    "        scale = cropped.bounds.flatten()\n",
    "        ax.auto_scale_xyz(scale, scale, scale)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(f\"{title}\\nIteration {num}\", pad=5)\n",
    "\n",
    "    filename = f\"{directory}iteration_{num}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    if num % 10 == 0:\n",
    "        print(f\"Saved: {filename}\")\n",
    "        \n",
    "print(f\"âœ… All plots saved to {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterationIN=0\n",
    "\n",
    "directory = \"figures/solarwind/\"\n",
    "\n",
    "for num in range(0,iterationNUM):\n",
    "    surface,_ = plot_surface_potential_fornegativepositive_charge(vars()[\"electrons_inside_stackediteration\"+str(num)], vars()[\"protons_inside_stackediteration\"+str(num)], stacked_spheres, vmin=-10,vmax=10)\n",
    "\n",
    "    # plt.hist(facolors[facolors!=0],bins=50)\n",
    "    # plt.show()\n",
    "\n",
    "    # Make sure each triangle has its own unique vertices\n",
    "    surface_edited = surface.copy()\n",
    "    surface_edited.unmerge_vertices()\n",
    "    surface_edited.visual.vertex_colors = None\n",
    "    surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap and normalization\n",
    "cmap = plt.cm.OrRd #seismic\n",
    "norm = Normalize(vmin=0, vmax=100)\n",
    "\n",
    "# Create a figure and a single axis for the colorbar\n",
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "# Create the colorbar\n",
    "cb = ColorbarBase(ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "cb.set_label('# of Particles / face')  # Optional label\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.25, -0.31, 0])\n",
    "bbox_max = np.array([ 0.15,  0.1,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Case 2: Photoemission (incident gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### plot the field over each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "\n",
    "# --- Configurations ---\n",
    "folder_path = [\"../build-temp600K-dynamicThreshold\", \"../build-temp425K-dynamicThreshold\", \"../build-425K-initial6-0.9Max-0.05um-final10\", \\\n",
    "               \"../build-425K-nodissipation-initial6-0.8Max-final9\", \"../build-425K-withoutdissipation\"]\n",
    "temperatures = [600,425,425,425,425]\n",
    "notes = [\"initial6max0.2final9(dissipation)\", \"initial6max0.2final9(dissipation)\", \"initial6max0.6final10(dissipation)\", \\\n",
    "         \"initial6max0.7final9(noDissipation)\", \"initial5max0.8final9(noDissipation)\"]\n",
    "\n",
    "# Target point (Fixed for all configurations)\n",
    "location = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) \n",
    "\n",
    "# --- Parallel Processing Worker Function ---\n",
    "\n",
    "def process_config(tempIN, folderIN, noteIN, configIN, location):\n",
    "    \"\"\"\n",
    "    Worker function to process a single configuration. \n",
    "    Returns the unique key and the calculated results.\n",
    "    \"\"\"\n",
    "    key_name = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    print(f\"--- Processing {key_name} in {folderIN} (Worker Process) ---\\n\")\n",
    "\n",
    "    # 1. Read Field Data\n",
    "    filenames = sorted(glob.glob(f\"{folderIN}/fieldmaps/*{configIN}*.txt\"))\n",
    "    fields_PE = read_data_format_efficient(filenames, scaling=True) \n",
    "    first_key = list(fields_PE.keys())[0]\n",
    "\n",
    "    # Prepare dictionary for this single result\n",
    "    config_results: Dict[str, Any] = {}\n",
    "    \n",
    "    # 2. Compute and Store Field at Target Location\n",
    "    # The function now returns a dict {'iter', 'E', 'E_mag'} for all iterations\n",
    "    config_results[\"fieldAtTarget\"] = compute_nearest_field_vector(fields_PE, target=location, start=first_key)\n",
    "    \n",
    "    # 3. Compute and Store the list of leaf lengths (number of points per iteration)\n",
    "    config_results[\"lengthLeaves\"] = [len(fields_PE[keyIN][\"pos\"]) for keyIN in fields_PE.keys()]\n",
    "    config_results[\"gradRefinements\"] = [fields_PE[keyIN][\"gradRefinements\"] for keyIN in fields_PE.keys()]\n",
    "    \n",
    "    # If the fields_PE dictionary is very large, deleting it immediately frees memory\n",
    "    del fields_PE \n",
    "    \n",
    "    # Return the key and the results to the main thread\n",
    "    return key_name, config_results\n",
    "\n",
    "# MASTER DICTIONARY to store all results securely\n",
    "results_data: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# --- Parallel Processing Loop ---\n",
    "\n",
    "# Prepare the list of arguments for the executor\n",
    "configs = zip(temperatures, folder_path, notes)\n",
    "args_list = [(temp, folder, note, configIN, location) for temp, folder, note in configs]\n",
    "\n",
    "MAX_WORKERS = len(temperatures) # Use one worker per configuration\n",
    "\n",
    "print(f\"--- Starting Parallel Processing with {MAX_WORKERS} workers ---\")\n",
    "\n",
    "# Use ProcessPoolExecutor for CPU-bound tasks\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    \n",
    "    # Submit all tasks and store the future objects\n",
    "    futures = [executor.submit(process_config, *args) for args in args_list]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            key, data = future.result()\n",
    "            results_data[key] = data\n",
    "        except Exception as exc:\n",
    "            print(f'Configuration generated an exception: {exc}')\n",
    "\n",
    "print(f\"--- Completed Parallel Processing ---\")\n",
    "\n",
    "# takes ~2 minute to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting and Analysis ---\n",
    "\n",
    "print(\"\\n--- Generating Plot for PE Comparison ---\")\n",
    "\n",
    "# 1. Load comparison data from Zimmerman\n",
    "zimmerman_SWdata = pd.read_csv(\"Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"Fig7a-PE.csv\")\n",
    "zimmerman_PEandSWdata = pd.read_csv(\"Fig7a-PE+SW.csv\")\n",
    "\n",
    "# 2. Define constants and calculate conversion factor\n",
    "# Simulation world area size in m^2 (600 um x 600 um)\n",
    "WORLD_XY_AREA_SQ_M = 600 * 600 / (1e6**2) \n",
    "\n",
    "# Number of particles (protons) injected into the active area per iteration\n",
    "PARTICLES_PER_ITERATION = 424975\n",
    "\n",
    "# Ion flux calculated from the simulation area (ions/m^2)\n",
    "FLUX_PER_ITERATION = PARTICLES_PER_ITERATION / WORLD_XY_AREA_SQ_M \n",
    "\n",
    "# Photoemission (PE) ion flux value (e/m^2/s). This factor combines \n",
    "# the effective current (4 uA/cm^2) and conversion to e/m^2/s.\n",
    "PE_ION_FLUX = 4e-6 * 6.241509e18 \n",
    "\n",
    "# Conversion factor: Time (s) per simulation iteration\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION / PE_ION_FLUX\n",
    "print(f\"Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "# 3. Define plot parameters (Colors)\n",
    "# Choose a continuous colormap and generate discrete colors based on the number of temperatures\n",
    "CMAP_NAME = 'jet' \n",
    "discrete_cmap = plt.get_cmap(CMAP_NAME, len(temperatures) + 1)\n",
    "color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, len(temperatures) + 1)]\n",
    "\n",
    "# 4. Generate Plot (Log-Log)\n",
    "plt.figure()\n",
    "\n",
    "# Plot reference data (Zimmerman)\n",
    "plt.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"], '--', label=\"SW (Zimmerman 2016)\", color=\"r\", lw=4)\n",
    "plt.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], 'g:', label=\"PE (Zimmerman 2016)\")\n",
    "plt.loglog(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"], 'b:', label=\"PE+SW (Zimmerman 2016)\")\n",
    "\n",
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.plot((results_data[key][\"fieldAtTarget\"][\"iter\"] -0.5)* CONVERT_ITERATION_PE_TIME, \n",
    "             abs(results_data[key][\"fieldAtTarget\"][\"E\"][:,0]), #results_data[key][\"fieldAtTarget\"][\"E_mag\"] \n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "# Plotting the E-field magnitude (|E|)\n",
    "plt.ylabel(r\"$|E_x$| (V/m)\") \n",
    "plt.legend() #bbox_to_anchor=(1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "# Set axes limits\n",
    "plt.ylim(4.8e3, 2.8e5)\n",
    "plt.xlim(7.4e-2, 1.25e1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"lengthLeaves\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Total Leaf Nodes (millions)\")\n",
    "plt.title(\"PE Case\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "for tempIN, colorIN, noteIN in zip(temperatures, color_list_rgba, notes):    \n",
    "    key = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "    # Plot E-field magnitude at target location\n",
    "    plt.semilogy(results_data[key][\"fieldAtTarget\"][\"iter\"],\n",
    "             np.array(results_data[key][\"gradRefinements\"])/1e6,\n",
    "             '.-',\n",
    "             label=f\"{tempIN} K: {noteIN}\",\n",
    "             lw=0.5, \n",
    "             color=colorIN)\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"# of Gradient Refinements (millions)\")\n",
    "plt.title(\"SW Case\")\n",
    "plt.axhline(y=1, color='k',lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "# configIN = \"onlyphotoemission\"\n",
    "# directory_path = \"../build-disspate-charge/\"\n",
    "\n",
    "# filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "# fields_PE = read_data_format_efficient(filenames,scaling=True) \n",
    "\n",
    "# # Target point\n",
    "# location = np.array([-0.1, 0, 0.1-0.015+0.037]) \n",
    "# # return the electric field at that location\n",
    "# Efield_PE_location = compute_nearest_field_vector(fields_PE, target=location, start=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process different sets of iterations (not in parallel) ##\n",
    "\n",
    "# configIN = \"onlyphotoemission\"\n",
    "# folder_path = [\"../build-temp425K-dynamicThreshold\",  \"../build-temp600K-dynamicThreshold\", \"../build-425K-initial6-0.9Max-0.05um-final10\"]\n",
    "# temperatures = [425,600,425]\n",
    "# notes = [\"initial6max0.2final9\", \"initial6max0.2final9\", \"initial6max0.6final10\"]\n",
    "\n",
    "# # Target point\n",
    "# location = np.array([-0.1, 0, 0.1-0.015+0.037]) \n",
    "\n",
    "# # MASTER DICTIONARY to store all results securely\n",
    "# results_data: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# # --- Processing Loop ---\n",
    "\n",
    "# # Use enumerate for index 'j' and zip the parameter lists together\n",
    "# for j, (tempIN, folderIN, noteIN) in enumerate(zip(temperatures, folder_path, notes)):\n",
    "\n",
    "#     # Create a unique key for storing results\n",
    "#     key_name = f\"PE_{tempIN}K_{noteIN}\"\n",
    "    \n",
    "#     print(f\"--- Processing {j}: {key_name} from {folderIN} ---\")\n",
    "\n",
    "#     # 1. Read Field Data\n",
    "#     filenames = sorted(glob.glob(f\"{folderIN}/fieldmaps/*{configIN}*.txt\"))\n",
    "#     fields_PE = read_data_format_efficient(filenames, scaling=True) \n",
    "#     first_key = list(fields_PE.keys())[0]\n",
    "    \n",
    "#     # 2. Initialize entry in the results dictionary\n",
    "#     results_data[key_name] = {}\n",
    "    \n",
    "#     # 3. Compute and Store Field at Target Location\n",
    "#     results_data[key_name][\"fieldAtTarget\"] = compute_nearest_field_vector(fields_PE, target=location, start=first_key)\n",
    "    \n",
    "#     # 4. Compute and Store the list of leaf lengths (number of points per iteration)\n",
    "#     results_data[key_name][\"lengthLeaves\"] = np.array([len(fields_PE[keyIN][\"pos\"]) for keyIN in fields_PE.keys()])\n",
    "    \n",
    "#     # If the fields_PE dictionary is very large, deleting it immediately frees memory\n",
    "#     #del fields_PE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### check the minimum distance between point in field map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one file ##\n",
    "\n",
    "directory_path = \"../build-425K-initial6-0.9Max-0.05um-final10/\"\n",
    "filenames = sorted(glob.glob(f\"{directory_path}/fieldmaps/*{configIN}*.txt\"))\n",
    "fields_PE = read_data_format_efficient(filenames,scaling=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fields_PE' is your list/dictionary structure and \n",
    "# fields_PE[1]['pos'] is a NumPy array of shape (N, 3), where N is the number of points.\n",
    "# Example data (replace this with your actual data):\n",
    "data_points = fields_PE[1]['pos'] \n",
    "# data_points = np.array([\n",
    "#     [1.0, 1.0, 1.0],\n",
    "#     [1.001, 1.0, 1.0],  # Very close point\n",
    "#     [2.0, 2.0, 2.0],\n",
    "#     [5.0, 5.0, 5.0]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# 1. Build the KD-Tree\n",
    "# This organizes the points in a spatial structure for efficient nearest neighbor search.\n",
    "tree = cKDTree(data_points)\n",
    "\n",
    "# 2. Query for the 2 nearest neighbors of every point\n",
    "# The 'k=2' parameter tells the query to find the distance to the 2 nearest neighbors:\n",
    "# - The 1st neighbor (k=1) is always the point itself (distance = 0.0).\n",
    "# - The 2nd neighbor (k=2) is the closest *other* point.\n",
    "distances, indices = tree.query(data_points, k=2)\n",
    "\n",
    "# 3. Extract the minimum non-zero distance\n",
    "# The minimum distance between any unique pair of points is the minimum value \n",
    "# in the array of distances to the second nearest neighbor (distances[:, 1]).\n",
    "min_distance = np.min(distances[:, 1])*1000\n",
    "\n",
    "print(f\"The total number of points (voxels) is: {len(data_points)}\")\n",
    "print(f\"The minimum distance between any two unique voxels is: {min_distance} um\")\n",
    "\n",
    "# takes around ~10 seconds to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### calculate temperature change over all iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Worker Function for Parallel Execution ---\n",
    "def process_root_file(fileIN: str, directory_path: str, target_volume = \"SiO2\") -> Tuple[int, float]:\n",
    "    \"\"\"Reads a single ROOT file, performs event analysis, and returns the index and calculated total energy.\"\"\"\n",
    "\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    try:\n",
    "        # Read data for the current iteration\n",
    "        df = read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path)\n",
    "\n",
    "    except Exception:\n",
    "        # Catch errors like missing keys or file corruption during read_rootfile\n",
    "        print(f\"-> ERROR: Skipping {fileIN.split('/')[-1]} due to failed file read\")\n",
    "        return iterationNUM, 0.0 # Return 0 energy and the index to maintain order\n",
    "\n",
    "    # 1. Get all incident gamma events (Particle_Type=\"gamma\", Parent_ID=0.0)\n",
    "    incident_gamma = df[(df[\"Particle_Type\"] == \"gamma\") & (df[\"Parent_ID\"] == 0.0)].drop_duplicates(subset=\"Event_Number\", keep=\"first\")\n",
    "\n",
    "    # 2. Get all unique event numbers that resulted in an electron creation\n",
    "    last_e_event = df[(df[\"Particle_Type\"] == \"e-\") & (df[\"Parent_ID\"] > 0.0)].drop_duplicates(subset=\"Event_Number\", keep=\"last\")\n",
    "    event_numbers_with_e_creation = last_e_event[\"Event_Number\"].unique()\n",
    "\n",
    "    # 3. All unique incident gamma event numbers\n",
    "    incident_gamma_event_numbers = incident_gamma[\"Event_Number\"].unique()\n",
    "\n",
    "    # 4. Events where NO electron was created (incident gamma events - events with e- creation)\n",
    "    events_without_photoelectric_e = np.setdiff1d(incident_gamma_event_numbers, event_numbers_with_e_creation)\n",
    "\n",
    "    # 5. Filter the main DataFrame to contain only data from the non-interacting events\n",
    "    events_without_photoelectric_e_df = df[df[\"Event_Number\"].isin(events_without_photoelectric_e)]\n",
    "\n",
    "    # 6. Calculate total energy deposited by gammas that *did not* result in a photoelectric electron\n",
    "    totalEnergy = np.sum(events_without_photoelectric_e_df[\n",
    "        (events_without_photoelectric_e_df[\"Particle_Type\"] == \"gamma\") & \n",
    "        (events_without_photoelectric_e_df[\"Volume_Name_Post\"] == target_volume)\n",
    "    ][\"Kinetic_Energy_Diff_eV\"])\n",
    "\n",
    "    print(f\"-> PROCESSED #{iterationNUM}: {fileIN.split('/')[-1]}\")\n",
    "\n",
    "    return iterationNUM, totalEnergy\n",
    "\n",
    "# --- Configuration ---\n",
    "configIN = \"onlyphotoemission\"\n",
    "directory_path =  \"../build-temp425K-dynamicThreshold/root/\" \n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*.root\"))\n",
    "\n",
    "# --- Main Parallel Execution ---\n",
    "\n",
    "# List to hold the (index, totalEnergy) tuples from parallel processes\n",
    "NUM_FILES = len(filelist)\n",
    "photoEnergyDepositionsforIterations = np.empty(NUM_FILES, dtype=np.float64)\n",
    "\n",
    "print(f\"--- Starting Parallel Processing of {NUM_FILES} files ---\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=NUM_FILES) as executor:\n",
    "    \n",
    "    # Submit tasks, passing the index to ensure results are ordered correctly later\n",
    "    futures = [executor.submit(process_root_file, fileIN, directory_path) for fileIN in filelist]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        index, totalEnergy = future.result()\n",
    "        photoEnergyDepositionsforIterations[index]= totalEnergy\n",
    "\n",
    "# Final assignment to the NumPy array\n",
    "#photoEnergyDepositionsforIterations = np.array([r[1] for r in all_results], dtype=np.float64)\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "initialT = 425\n",
    "heat_capacity = 670+1e3*((initialT-250)/530.6)-1e3*((initialT-250)/498.7)**2 # for lunar regolith\n",
    "density = 2.2/1000 #kg/cm3\n",
    "radius = 5 # assuming that all of the energy is deposited in a 10 um area!!\n",
    "volume = 4/2*np.pi*(radius*1e-4)**3 # cm3\n",
    "mass = volume*density # mass of material\n",
    "# this radius and volume is not true, just calculated as an extreme to see if we need to dynamically adjust the temperature!!\n",
    "\n",
    "print(\"--- Over {NUM_FILES} Iterations ---\")\n",
    "print(f\"Mean Temperature Increase : {np.mean(photoEnergyDepositionsforIterations*1.60218e-19/heat_capacity/mass*100)} K\")\n",
    "print(f\"Total Temperature Increase: {np.sum(photoEnergyDepositionsforIterations*1.60218e-19/heat_capacity/mass*100)} K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### create plots of the face ilumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "directory_path =  \"../build-temp425K-dynamicThreshold/root/\" #\"../build-adaptive-barns-fixed/root/\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*.root\"))\n",
    "\n",
    "all_gamma_holes = []\n",
    "all_electrons_inside = []\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"gamma_holes_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)], _ = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path),\n",
    "                                                                                                             config=configIN)\n",
    "    \n",
    "    surf, ilm_values = plot_face_illumination(vars()[\"gamma_holes_\"+str(number_str)], stacked_spheres, vmin=0, vmax=1)\n",
    "    # Make sure each triangle has its own unique vertices\n",
    "    surface_edited = surface.copy()\n",
    "    surface_edited.unmerge_vertices()\n",
    "    surface_edited.visual.vertex_colors = None\n",
    "    surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf, ilm_values = plot_face_illumination(gamma_holes_stackediteration0, stacked_spheres, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "directory_path =  \"../build-leakage/root/\" #\"../build-adaptive-barns-fixed/root/\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*iteration*{configIN}*_num500000.root\"))\n",
    "\n",
    "all_gamma_holes = []\n",
    "all_electrons_inside = []\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"gamma_holes_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)], _ = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path),\n",
    "                                                                                                             config=configIN)\n",
    "    all_gamma_holes.append(vars()[\"gamma_holes_\"+str(number_str)])\n",
    "    all_electrons_inside.append(vars()[\"electrons_inside_\"+str(number_str)])\n",
    "    print(78*\"-\")\n",
    "\n",
    "    break\n",
    "\n",
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_saturation(t, a, b, tau):\n",
    "    return a - b * np.exp(-t / tau)\n",
    "\n",
    "\n",
    "# Initial parameter guess: a, b, tau\n",
    "initial_guess = [4.0, 2.0, 1.0]\n",
    "popt, pcov = curve_fit(exp_saturation, 10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], p0=initial_guess)\n",
    "\n",
    "# Plot\n",
    "plt.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"],'--',label=\"SW\",color=\"r\",lw=4)\n",
    "plt.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"],'g:',label=\"PE\")\n",
    "plt.plot(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"],'b:',label=\"PE+SW\")\n",
    "plt.plot((efield_PE[\"iter\"] - 101)*convert_iteration_PEtime, abs(mag_values_PE), 'k.-',label=\"Geant4: PE (different incident)\",lw=0.5)\n",
    "#plt.semilogx((efield_PE_noholes[\"iter\"] - 101)*convert_iteration_PEtime2, abs(mag_values_PE_noholes), 'b.-',label=\"Geant4: PE (no holes)\",lw=0.5)\n",
    "#plt.plot((efield_PE_normal[\"iter\"] - 101)*convert_iteration_PEtime, abs(mag_values_PE_normal), 'g.-',label=\"Geant4: PE\",lw=0.5)\n",
    "#plt.plot((efield_SW[\"iter\"] - 1)*convert_iteration_SWtime, mag_values_SW, 'k.-',label=\"Geant4: SW\",lw=0.5)\n",
    "\n",
    "# xdata_zimmerman = np.linspace(1e-2,10,100)\n",
    "# y_fit_zimmerman = exp_saturation(xdata_zimmerman, *popt)\n",
    "# #plt.plot(xdata_zimmerman, y_fit_zimmerman, 'r-',lw=0.2)\n",
    "\n",
    "# # Initial parameter guess: a, b, tau\n",
    "# initial_guess = [4.0, 2.0, 1.0]\n",
    "# popt, pcov = curve_fit(exp_saturation,(efield_PE[\"iter\"] - 101)*convert_iteration_PEtime, abs(mag_values_PE), p0=popt)\n",
    "\n",
    "# xdata = np.linspace(1e-1,5,100)\n",
    "# y_fit = exp_saturation(xdata, *popt)\n",
    "# plt.plot(xdata, y_fit, 'g-',lw=0.2, label=\"prediction\")\n",
    "# #plt.plot(xdata_zimmerman, y_fit_zimmerman-3.8e4, 'r-',lw=0.2)\n",
    "# plt.axvline(x=4)\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(r\"|E| (V/m)\")\n",
    "#plt.ylabel(r\"E$_x$ (V/m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ilm_values, bins=np.logspace(-1,3,100))\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"# of Photons hitting each Voxel\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Distribution Stats: mean {np.mean(ilm_values[ilm_values!=0])}, median {np.median(ilm_values[ilm_values!=0])}, max {np.max(ilm_values)}\")\n",
    "\n",
    "# our area is a factor of 4 smaller than their area \n",
    "print(f\"for one iteration, median # of particles in each equivalently sized voxel is {np.median(ilm_values[ilm_values!=0])/4}\")\n",
    "voxel_area = 0.0004/(1000)**2 # rough area approximated from python (0.4 micron2)\n",
    "zimmerman_charge = 0.5*(1e-6) # C/m2\n",
    "zimmerman_electronnum = (zimmerman_charge/1.60217663e-19)*voxel_area\n",
    "print(f\"will take {zimmerman_electronnum/(np.max(ilm_values[ilm_values!=0])/4)} iterations at this rate to get to the photoemission flux ranges shown at 3 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 33\")\n",
    "surface,pot = plot_surface_potential_fornegativepositive_charge(all_electrons_inside_df, all_gamma_holes_df, stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "print(min(pot),max(pot))\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.2, -0.2, 0])\n",
    "bbox_max = np.array([ 0.2,  0.2,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((stacked_spheres.vertices >= bbox_min) & \n",
    "                (stacked_spheres.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[stacked_spheres.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = stacked_spheres.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap and normalization\n",
    "cmap = plt.cm.seismic\n",
    "norm = Normalize(vmin=-0.2, vmax=0.2)\n",
    "\n",
    "# Create a figure and a single axis for the colorbar\n",
    "fig, ax = plt.subplots(figsize=(4, 0.5))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "# Create the colorbar\n",
    "cb = ColorbarBase(ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "cb.set_label('Surface Potential (mV)')  # Optional label\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Case 3: all particles (incident e-, protons, gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"../build-sphere-charging/root/\"\n",
    "configIN = \"allparticles\"\n",
    "filelist = sorted(glob.glob(f\"{directory_path}/*stackediteration*{configIN}*num5000.root\"))\n",
    "\n",
    "all_gamma_holes,all_photoemission_electrons,all_protons_inside,all_electrons_inside = [],[],[],[]\n",
    "\n",
    "for fileIN in filelist:\n",
    "    print(fileIN.split(\"/\")[-1])\n",
    "    number_str = fileIN.split(\"/\")[-1].split(\"_\")[1]\n",
    "    iterationNUM = int(''.join(filter(str.isdigit, number_str)))\n",
    "\n",
    "    # read data from different iterations\n",
    "    vars()[\"gamma_holes_\"+str(number_str)], vars()[\"photoemission_electrons_inside_\"+str(number_str)], \\\n",
    "        vars()[\"protons_inside_\"+str(number_str)], vars()[\"electrons_inside_\"+str(number_str)] = calculate_stats(read_rootfile(fileIN.split(\"/\")[-1], directory_path=directory_path), \\\n",
    "                                                                                                             config=configIN)\n",
    "    all_gamma_holes.append(vars()[\"gamma_holes_\"+str(number_str)])\n",
    "    all_photoemission_electrons.append(vars()[\"photoemission_electrons_inside_\"+str(number_str)])\n",
    "    all_protons_inside.append(vars()[\"protons_inside_\"+str(number_str)])\n",
    "    all_electrons_inside.append(vars()[\"electrons_inside_\"+str(number_str)])\n",
    "    print(78*\"-\")\n",
    "\n",
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_photoemission_electrons_df = pd.concat(all_photoemission_electrons, ignore_index=True)\n",
    "all_protons_inside_df = pd.concat(all_protons_inside, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all iterations into single DataFrames\n",
    "all_gamma_holes_df = pd.concat(all_gamma_holes, ignore_index=True)\n",
    "all_photoemission_electrons_df = pd.concat(all_photoemission_electrons, ignore_index=True)\n",
    "all_protons_inside_df = pd.concat(all_protons_inside, ignore_index=True)\n",
    "all_electrons_inside_df = pd.concat(all_electrons_inside, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 28\")\n",
    "# input order: gammas, photoelectrons, protons, electrons, convex_combined,\n",
    "surface,facecolors = plot_surface_potential_allparticle_case(all_gamma_holes_df, all_photoemission_electrons_df, all_protons_inside_df, all_electrons_inside_df, \n",
    "                                                      stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.2, -0.3, 0])\n",
    "bbox_max = np.array([ 0.2,  0.1,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap and normalization\n",
    "cmap = plt.cm.seismic\n",
    "norm = Normalize(vmin=-0.2, vmax=0.2)\n",
    "\n",
    "# Create a figure and a single axis for the colorbar\n",
    "fig, ax = plt.subplots(figsize=(4, 0.5))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "# Create the colorbar\n",
    "cb = ColorbarBase(ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "cb.set_label('Surface Potential (mV)')  # Optional label\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 19: only SW ions\")\n",
    "surface,facecolors = plot_surface_potential_fornegativepositive_charge(all_electrons_inside_df, all_protons_inside_df, stacked_spheres, vmin=-0.2,vmax=0.2)\n",
    "\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"up through iteration 22: only photons\")\n",
    "surface,facecolors = plot_surface_potential_fornegativepositive_charge(all_photoemission_electrons_df, all_gamma_holes_df, stacked_spheres, vmin=-0.5,vmax=0.5)\n",
    "\n",
    "print(min(facecolors),max(facecolors))\n",
    "plt.plot(facecolors[facecolors!=0])\n",
    "plt.show()\n",
    "\n",
    "# Make sure each triangle has its own unique vertices\n",
    "surface_edited = surface.copy()\n",
    "surface_edited.unmerge_vertices()\n",
    "surface_edited.visual.vertex_colors = None\n",
    "surface_edited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_min = np.array([-0.2, -0.3, 0])\n",
    "bbox_max = np.array([ 0.2,  0.1,  100])\n",
    "\n",
    "# Filter vertices\n",
    "in_box = np.all((surface_edited.vertices >= bbox_min) & \n",
    "                (surface_edited.vertices <= bbox_max), axis=1)\n",
    "\n",
    "# Get face indices where all 3 vertices are in the box\n",
    "face_mask = np.all(in_box[surface_edited.faces], axis=1)\n",
    "\n",
    "# Extract submesh\n",
    "cropped = surface_edited.submesh([face_mask], only_watertight=False, append=True)\n",
    "cropped.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
