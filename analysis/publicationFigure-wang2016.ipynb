{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pyvista as pv\n",
    "pv.set_jupyter_backend('trame')  # or 'panel' if using panel\n",
    "\n",
    "from scipy.constants import epsilon_0, e as q_e\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Import interpolate for numerical method\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.collections import LineCollection\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import matplotlib as mpl\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "import trimesh\n",
    "import h5py\n",
    "from trimesh.points import PointCloud\n",
    "\n",
    "from common_functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Quantitative Comparison with Zimmerman et al. 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze one preprocessed h5 file\n",
    "\n",
    "# directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output111025/processed-fieldmaps\"\n",
    "# processedResults = load_h5_to_dict(f\"{directory}/PE_425K_initial8max0.8final12_noDissipation_sphere50um-throughXX.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- configuration ---\n",
    "directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output111025/processed-fieldmaps\"\n",
    "h5_filenames = glob.glob(f\"{directory}/PE_425K*Refined*pos-0.1*.h5\")\n",
    "\n",
    "# --- helper function for one key ---\n",
    "def process_key(args):\n",
    "\n",
    "    keyIN, val, target_point = args\n",
    "\n",
    "    points = val[\"pos\"]\n",
    "    vectors = val[\"E\"]\n",
    "    magnitudes = val[\"E_mag\"]\n",
    "\n",
    "    # radius = 6e-3  # spherical averaging radius\n",
    "    radius = 5e-3  # spherical averaging radius\n",
    "    \n",
    "    mask = np.sum((points - target_point)**2, axis=1) <= radius**2\n",
    "    if not np.any(mask):\n",
    "        # Return placeholders if no points in sphere\n",
    "        return -1, np.full(3, np.nan), np.nan, np.full(3, np.nan), 0\n",
    "\n",
    "    avg_position = points[mask].mean(axis=0)\n",
    "    E_vec = vectors[mask].mean(axis=0)\n",
    "    E_mag = magnitudes[mask].mean(axis=0)\n",
    "    E_vec_errors = vectors[mask].std(axis=0) /np.sqrt(len(magnitudes[mask]))\n",
    "    #point_err = np.abs(avg_position - target_point)\n",
    "    return int(keyIN.split(\"_\")[1]), E_vec, E_mag, E_vec_errors,len(magnitudes[mask]) \n",
    "\n",
    "    # tree = cKDTree(points)\n",
    "    # dist, idx = tree.query(target_point)\n",
    "\n",
    "    # # Nearest neighbor field\n",
    "    # E_vec_at_point = vectors[idx]\n",
    "    # E_mag_at_point = magnitudes[idx]\n",
    "\n",
    "    # return int(keyIN.split(\"_\")[1]), E_vec_at_point, E_mag_at_point, dist \n",
    "\n",
    "# --- extract metadata from filename ---\n",
    "def parse_filename_metadata(filename):\n",
    "    \"\"\"\n",
    "    Only processes files with:\n",
    "      - Temperature: 425 K\n",
    "      - pos_value: -0.1\n",
    "    Example:\n",
    "    PE_425K_initial8max0.8final12_RefinedGridDissipation_500000particles_Sphere20um_pos-0.1_through26.h5\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    base = os.path.basename(filename)\n",
    "\n",
    "    case = base.split(\"_\")[0]\n",
    "\n",
    "    # Temperature\n",
    "    temp_match = re.search(r'_(\\d+)K_', base)\n",
    "    temperature = int(temp_match.group(1)) if temp_match else np.nan\n",
    "\n",
    "    # Position\n",
    "    pos_match = re.search(r'_pos([-+]?\\d*\\.?\\d+)_through', base)\n",
    "    pos_value = float(pos_match.group(1)) if pos_match else np.nan\n",
    "\n",
    "    # Sphere size (if any)\n",
    "    sphere_match = re.search(r'_sphere(\\d+)um', base, re.IGNORECASE)\n",
    "    sphere_um = int(sphere_match.group(1)) if sphere_match else np.nan\n",
    "\n",
    "    # Octree parameters: initial, grad threshold, final\n",
    "    octree_match = re.search(r'_initial(\\d+)max([-+]?\\d*\\.?\\d+)final(\\d+)', base)\n",
    "    if octree_match:\n",
    "        octree_params = {\n",
    "            \"initial_depth\": int(octree_match.group(1)),\n",
    "            \"percent_gradThreshold\": float(octree_match.group(2)),\n",
    "            \"final_depth\": int(octree_match.group(3))\n",
    "        }\n",
    "    else:\n",
    "        octree_params = {\"initial_depth\": np.nan, \"grad_threshold\": np.nan, \"final_depth\": np.nan}\n",
    "\n",
    "    # Target point\n",
    "    #target_point = np.array([pos_value - 0.0073, 0, 0.1 - 0.015 + 0.037 - 0.00073])\n",
    "    target_point = np.array([pos_value - 0.005, 0, 0.1 - 0.015 + 0.037])\n",
    "    #target_point = np.array([pos_value, 0, 0.1 - 0.015 + 0.037])\n",
    "\n",
    "    metadata = {\n",
    "        \"filename\": base,\n",
    "        \"case\": case,\n",
    "        \"temperature\": temperature,\n",
    "        \"target_point\": target_point,\n",
    "        \"sphere_um\": sphere_um,\n",
    "        \"octree\": octree_params,\n",
    "       # \"num_particles\": num_particles\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# --- worker for one file ---\n",
    "def process_file(fileIN):\n",
    "    metadata = parse_filename_metadata(fileIN)\n",
    "    \n",
    "    # # Filter: skip files with 'through' files that have processed < 40 iterations\n",
    "    # through_idx = int(re.search(r'_through(\\d+)\\.h5', fileIN).group(1))\n",
    "    # if through_idx < 40:\n",
    "    #     return None\n",
    "\n",
    "    if metadata is None:\n",
    "        return None\n",
    "\n",
    "    print(f\"→ Started {os.path.basename(fileIN)}\\n\", flush=True)\n",
    "    processedResults = load_h5_to_dict(fileIN)\n",
    "    key_prefix = os.path.basename(fileIN).split('_through')[0]\n",
    "\n",
    "    keys = list(processedResults.keys())\n",
    "    n_keys = len(keys)\n",
    "\n",
    "    # --- inner parallelization across keys ---\n",
    "    args_list = [(k, processedResults[k], metadata[\"target_point\"]) for k in keys]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as tpool:  # inner parallel threads\n",
    "        results = list(tpool.map(process_key, args_list))\n",
    "\n",
    "    ids, E_vecs, E_mags, standard_errors, N = zip(*results)\n",
    "    ids = np.array(ids)\n",
    "    E_vecs = np.array(E_vecs)\n",
    "    E_mags = np.array(E_mags)\n",
    "    standard_errors = np.array(standard_errors)\n",
    "    num_points = np.array(N)\n",
    "\n",
    "    print(f\"✓ Finished {os.path.basename(fileIN)}\", flush=True)\n",
    "    return key_prefix, {\n",
    "        \"iter\": ids,\"E_vecs\": E_vecs,\"E_mags\": E_mags,\"point_errors\": standard_errors, \"N\":num_points, \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "# --- parallel execution across files ---\n",
    "num_cores = 2\n",
    "all_processed = {}\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "    futures = {executor.submit(process_file, f): f for f in h5_filenames}\n",
    "    for fut in as_completed(futures):\n",
    "        fileIN = futures[fut]\n",
    "        try:\n",
    "            result = fut.result()\n",
    "            if result is not None:\n",
    "                key_prefix, data = result\n",
    "                all_processed[key_prefix] = data\n",
    "                print(f\"✔ Processed {os.path.basename(fileIN)}\\n\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {os.path.basename(fileIN)}: {e}\\n\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- configuration ---\n",
    "#directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/stacked-sphere/output111025/processed-fieldmaps\"\n",
    "directory = \"processed-fieldmaps/\"\n",
    "h5_filenames = glob.glob(f\"{directory}/*425K*Refined*.h5\")\n",
    "\n",
    "# Define field averaging parameters\n",
    "FIELD_AVERAGE_RADIUS = 2e-3\n",
    "offsetLimits = 0.01\n",
    "\n",
    "offsetLimitsX = 0.006\n",
    "offsetLimitsY = 0.006\n",
    "new_step_x = offsetLimitsX/1\n",
    "new_step_y = offsetLimitsY/1\n",
    "\n",
    "# Create grid of target points\n",
    "xoffset = np.round(np.arange(-offsetLimitsX, offsetLimitsX+1e-9, new_step_x),5)\n",
    "yoffset = np.round(np.arange(-offsetLimitsY, offsetLimitsY+1e-9, new_step_y),5)\n",
    "X, Y = np.meshgrid(xoffset, yoffset)\n",
    "target_points_array = np.vstack([\n",
    "    -0.1 - X.flatten(), \n",
    "    np.zeros(len(X.flatten())), \n",
    "    0.1 - 0.015 + 0.037 - Y.flatten()\n",
    "]).T\n",
    "\n",
    "print(f\"Processing {len(target_points_array)} target points with radius {FIELD_AVERAGE_RADIUS*1000} um\")\n",
    "\n",
    "# --- helper function for one key and one target point ---\n",
    "def process_key_target(args):\n",
    "    keyIN, val, target_point, radius = args\n",
    "\n",
    "    points = val[\"pos\"]\n",
    "    vectors = val[\"E\"]\n",
    "    magnitudes = val[\"E_mag\"]\n",
    "    \n",
    "    # Mask for spherical averaging around target point\n",
    "    mask = np.sum((points - target_point)**2, axis=1) <= radius**2\n",
    "    \n",
    "    if not np.any(mask):\n",
    "        # Return placeholders if no points in sphere\n",
    "        return -1, np.full(3, np.nan), np.nan, np.full(3, np.nan), 0\n",
    "\n",
    "    avg_position = points[mask].mean(axis=0)\n",
    "    E_vec = vectors[mask].mean(axis=0)\n",
    "    E_mag = magnitudes[mask].mean(axis=0)\n",
    "    E_vec_errors = vectors[mask].std(axis=0) / np.sqrt(len(magnitudes[mask]))\n",
    "    \n",
    "    return int(keyIN.split(\"_\")[1]), E_vec, E_mag, avg_position, len(magnitudes[mask])\n",
    "\n",
    "# --- extract metadata from filename ---\n",
    "def parse_filename_metadata(filename):\n",
    "    \"\"\"\n",
    "    Processes files and extracts metadata.\n",
    "    Example:\n",
    "    PE_425K_initial8max0.8final12_RefinedGridDissipation_500000particles_Sphere20um_pos-0.1_through26.h5\n",
    "    \"\"\"\n",
    "\n",
    "    base = os.path.basename(filename)\n",
    "    case = base.split(\"_\")[0]\n",
    "\n",
    "    # Temperature\n",
    "    temp_match = re.search(r'_(\\d+)K_', base)\n",
    "    temperature = int(temp_match.group(1)) if temp_match else np.nan\n",
    "\n",
    "    # Position\n",
    "    pos_match = re.search(r'_pos([-+]?\\d*\\.?\\d+)_through', base)\n",
    "    pos_value = float(pos_match.group(1)) if pos_match else np.nan\n",
    "\n",
    "    # Sphere size (if any)\n",
    "    sphere_match = re.search(r'_sphere(\\d+)um', base, re.IGNORECASE)\n",
    "    sphere_um = int(sphere_match.group(1)) if sphere_match else np.nan\n",
    "\n",
    "    # Octree parameters: initial, grad threshold, final\n",
    "    octree_match = re.search(r'_initial(\\d+)max([-+]?\\d*\\.?\\d+)final(\\d+)', base)\n",
    "    if octree_match:\n",
    "        octree_params = {\n",
    "            \"initial_depth\": int(octree_match.group(1)),\n",
    "            \"percent_gradThreshold\": float(octree_match.group(2)),\n",
    "            \"final_depth\": int(octree_match.group(3))\n",
    "        }\n",
    "    else:\n",
    "        octree_params = {\"initial_depth\": np.nan, \"grad_threshold\": np.nan, \"final_depth\": np.nan}\n",
    "\n",
    "    metadata = {\n",
    "        \"filename\": base,\n",
    "        \"case\": case,\n",
    "        \"temperature\": temperature,\n",
    "        \"sphere_um\": sphere_um,\n",
    "        \"octree\": octree_params,\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# --- worker for one file ---\n",
    "def process_file(fileIN):\n",
    "    metadata = parse_filename_metadata(fileIN)\n",
    "\n",
    "    if metadata is None:\n",
    "        return None\n",
    "\n",
    "    print(f\"→ Started {os.path.basename(fileIN)}\\n\", flush=True)\n",
    "    processedResults = load_h5_to_dict(fileIN)\n",
    "    key_prefix = os.path.basename(fileIN).split('_through')[0]\n",
    "\n",
    "    keys = list(processedResults.keys())\n",
    "\n",
    "    # Dictionary to hold results for each target point\n",
    "    target_point_results = {}\n",
    "\n",
    "    # Process each target point\n",
    "    for tp_idx, target_point in enumerate(target_points_array):\n",
    "        tp_key = f\"target_{tp_idx:04d}\"\n",
    "        \n",
    "        # --- inner parallelization across keys for this target point ---\n",
    "        args_list = [(k, processedResults[k], target_point, FIELD_AVERAGE_RADIUS) for k in keys]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=8) as tpool:\n",
    "            results = list(tpool.map(process_key_target, args_list))\n",
    "\n",
    "        ids, E_vecs, E_mags, standard_errors, N = zip(*results)\n",
    "        ids = np.array(ids)\n",
    "        E_vecs = np.array(E_vecs)\n",
    "        E_mags = np.array(E_mags)\n",
    "        standard_errors = np.array(standard_errors)\n",
    "        num_points = np.array(N)\n",
    "\n",
    "        target_point_results[tp_key] = {\n",
    "            \"iter\": ids,\n",
    "            \"E_vecs\": E_vecs,\n",
    "            \"E_mags\": E_mags,\n",
    "            \"point_errors\": standard_errors,\n",
    "            \"N\": num_points,\n",
    "            \"target_point\": target_point,\n",
    "            \"radius\": FIELD_AVERAGE_RADIUS\n",
    "        }\n",
    "        \n",
    "        if (tp_idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {tp_idx + 1}/{len(target_points_array)} target points\", flush=True)\n",
    "\n",
    "    print(f\"✓ Finished {os.path.basename(fileIN)}\", flush=True)\n",
    "    return key_prefix, {\n",
    "        \"target_points\": target_point_results,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "# --- parallel execution across files ---\n",
    "num_cores = 2\n",
    "all_processed = {}\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "    futures = {executor.submit(process_file, f): f for f in h5_filenames}\n",
    "    for fut in as_completed(futures):\n",
    "        fileIN = futures[fut]\n",
    "        try:\n",
    "            result = fut.result()\n",
    "            if result is not None:\n",
    "                key_prefix, data = result\n",
    "                all_processed[key_prefix] = data\n",
    "                print(f\"✔ Processed {os.path.basename(fileIN)}\\n\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {os.path.basename(fileIN)}: {e}\\n\", flush=True)\n",
    "\n",
    "print(f\"\\n=== Processing Complete ===\")\n",
    "print(f\"Total files processed: {len(all_processed)}\")\n",
    "print(f\"Target points per file: {len(target_points_array)}\")\n",
    "\n",
    "# --- Access results example ---\n",
    "# all_processed[key_prefix][\"target_points\"][\"target_0000\"][\"E_vecs\"]\n",
    "# all_processed[key_prefix][\"target_points\"][\"target_0000\"][\"target_point\"]\n",
    "# all_processed[key_prefix][\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LaTeX rendering\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Set the global font size\n",
    "mpl.rcParams.update({'font.size': 14})\n",
    "\n",
    "# --- 1. CONFIGURATION AND DATA LOADING ---\n",
    "print(\"\\n--- Starting Data Processing and Plot Generation ---\")\n",
    "\n",
    "# Define Data Keys\n",
    "#KEY_TARGET_PE = 'PE_425K_initial8max0.8final12_RefinedGridDissipation_sphere40um_pos-0.1'\n",
    "KEY_TARGET_PE = 'PE_425K_initial8max0.8final12_RefinedGridDissipation_sphere40um_adjustedworld'\n",
    "\n",
    "# Load external literature data\n",
    "zimmerman_SWdata = pd.read_csv(\"literature-data/Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"literature-data/Fig7a-PE.csv\")\n",
    "\n",
    "# Simulation Parameters (used for time conversion)\n",
    "WORLD_XY_AREA_SQ_M = 300 * 300 / (1e6**2)  # World area (m^2)\n",
    "\n",
    "# PE Conversion Factor\n",
    "PARTICLES_PER_ITERATION_PE = 81775\n",
    "FLUX_PER_ITERATION_PE = PARTICLES_PER_ITERATION_PE / WORLD_XY_AREA_SQ_M\n",
    "PE_ION_FLUX = 4e-6 * 6.241509e18\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION_PE / PE_ION_FLUX\n",
    "print(f\"PE Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "# SW Conversion Factor\n",
    "PARTICLES_PER_ITERATION_SW = 30601\n",
    "FLUX_PER_ITERATION_SW = PARTICLES_PER_ITERATION_SW / WORLD_XY_AREA_SQ_M\n",
    "SW_ION_FLUX = 3e-7 * 6.241509e18\n",
    "CONVERT_ITERATION_SW_TIME = FLUX_PER_ITERATION_SW / SW_ION_FLUX\n",
    "print(f\"SW Conversion Factor (s/iteration): {CONVERT_ITERATION_SW_TIME:.3e}\")\n",
    "\n",
    "# --- 2. PREPARE LITERATURE DATA ---\n",
    "x_lit_sw = 10**zimmerman_SWdata[\"x\"]\n",
    "y_lit_sw = zimmerman_SWdata[\" y\"]\n",
    "x_lit_pe = 10**zimmerman_PEdata[\"x\"]\n",
    "y_lit_pe = zimmerman_PEdata[\" y\"]\n",
    "\n",
    "# --- 3. PLOTTING SETUP ---\n",
    "fig, ax_main = plt.subplots(figsize=(8.01, 4.6))\n",
    "\n",
    "# --- 4. PLOT LITERATURE DATA ---\n",
    "ax_main.plot(x_lit_sw, y_lit_sw, '-', color=\"k\", lw=3, alpha=0.3, label=None)\n",
    "ax_main.plot(x_lit_pe, y_lit_pe, '-', color=\"k\", lw=3, alpha=0.3, label=None)\n",
    "\n",
    "# --- 5. PLOT SIMULATION DATA FOR EACH TARGET POINT ---\n",
    "selectkey = KEY_TARGET_PE\n",
    "\n",
    "# Color Map Setup\n",
    "CMAP_NAME = 'jet'\n",
    "n_targets = len(all_processed[selectkey][\"target_points\"].keys())\n",
    "discrete_cmap = plt.get_cmap(CMAP_NAME, n_targets + 1)\n",
    "color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, n_targets + 1)]\n",
    "\n",
    "# Get case type for time conversion\n",
    "case = all_processed[selectkey][\"metadata\"][\"case\"]\n",
    "factor = CONVERT_ITERATION_PE_TIME if case == \"PE\" else CONVERT_ITERATION_SW_TIME\n",
    "\n",
    "# Plot each target point\n",
    "for targetIN, colorIN in zip(all_processed[selectkey][\"target_points\"].keys(), color_list_rgba):\n",
    "    \n",
    "    # Extract data\n",
    "    x_data = np.array(all_processed[selectkey][\"target_points\"][targetIN][\"iter\"] - 1) * factor\n",
    "    y_data = abs(all_processed[selectkey][\"target_points\"][targetIN][\"E_vecs\"][:, 0])\n",
    "    y_err = all_processed[selectkey][\"target_points\"][targetIN][\"point_errors\"][:, 0]\n",
    "    target_point = all_processed[selectkey][\"target_points\"][targetIN][\"target_point\"]\n",
    "    \n",
    "    # Plot the data line\n",
    "    ax_main.plot(x_data, y_data, '-', color=colorIN, lw=1.5, \n",
    "                 label=f'x:{target_point[0]:.3f} z:{target_point[2]:.3f}')\n",
    "    \n",
    "    # Optional: Add error region\n",
    "    # ax_main.fill_between(x_data, y_data - y_err, y_data + y_err, \n",
    "    #                      color=colorIN, alpha=0.15, label=None)\n",
    "\n",
    "# --- 6. FORMAT MAIN PLOT ---\n",
    "ax_main.set_xlabel(\"Time [s]\")\n",
    "ax_main.set_ylabel(r\"$|E_x|$ (V/m)\")\n",
    "ax_main.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "# ax_main.set_ylim(0, 2.2e5)\n",
    "ax_main.set_xlim(0, 1)\n",
    "ax_main.legend(loc=\"upper right\", fontsize=8, ncol=2)\n",
    "ax_main.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "# --- 7. SAVE AND SHOW ---\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/field_evolution_target_points.jpeg\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Plot Generation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LaTeX rendering\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Set the global font size\n",
    "mpl.rcParams.update({'font.size': 14})\n",
    "\n",
    "# --- 1. CONFIGURATION AND DATA LOADING ---\n",
    "print(\"\\n--- Starting Data Processing and Plot Generation ---\")\n",
    "\n",
    "# Define Data Keys for Fitting and Subtraction\n",
    "KEY_FIT = 'PE_425K_initial8max0.8final12_noDissipation_sphere50um_pos-0.1'\n",
    "KEY_TARGET_PE = 'PE_425K_initial8max0.8final12_RefinedGridDissipation_sphere40um_pos-0.1' # Renamed for clarity\n",
    "KEY_TARGET_SW = 'SW_425K_initial8max0.8final12_RefinedGridDissipation_sphere20um_pos-0.1' # Key for SW comparison\n",
    "\n",
    "# Load external literature data\n",
    "zimmerman_SWdata = pd.read_csv(\"literature-data/Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"literature-data/Fig7a-PE.csv\")\n",
    "zimmerman_PEandSWdata = pd.read_csv(\"literature-data/Fig7a-PE+SW.csv\")\n",
    "\n",
    "# Simulation Parameters (used for time conversion)\n",
    "WORLD_XY_AREA_SQ_M = 300 * 300 / (1e6**2) # World area (m^2)\n",
    "\n",
    "# PE Conversion Factor\n",
    "PARTICLES_PER_ITERATION_PE = 81775\n",
    "FLUX_PER_ITERATION_PE = PARTICLES_PER_ITERATION_PE / WORLD_XY_AREA_SQ_M\n",
    "PE_ION_FLUX = 4e-6 * 6.241509e18\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION_PE / PE_ION_FLUX\n",
    "print(f\"PE Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "# SW Conversion Factor\n",
    "PARTICLES_PER_ITERATION_SW = 30601\n",
    "FLUX_PER_ITERATION_SW = PARTICLES_PER_ITERATION_SW / WORLD_XY_AREA_SQ_M\n",
    "SW_ION_FLUX = 3e-7 * 6.241509e18\n",
    "CONVERT_ITERATION_SW_TIME = FLUX_PER_ITERATION_SW / SW_ION_FLUX\n",
    "print(f\"SW Conversion Factor (s/iteration): {CONVERT_ITERATION_SW_TIME:.3e}\")\n",
    "\n",
    "# Color Map Setup\n",
    "CMAP_NAME = 'Dark2'\n",
    "discrete_cmap = plt.get_cmap(CMAP_NAME, len(all_processed.keys()) + 1)\n",
    "color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, len(all_processed.keys()) + 1)]\n",
    "\n",
    "# --- 2. CURVE FITTING AND EXTRAPOLATION (UNCHANGED) ---\n",
    "\n",
    "# Define the new fitting function (Polynomial of Order 3)\n",
    "def poly_curve(t, a, b, c, d):\n",
    "    \"\"\"\n",
    "    Function: a*t^3 + b*t^2 + c*t + d (Polynomial of Order 3)\n",
    "    \"\"\"\n",
    "    return a*t**3 + b*t**2 + c*t + d\n",
    "\n",
    "# Prepare data for fitting (PE_425K)\n",
    "data_fit = all_processed[KEY_FIT]\n",
    "x_fit = np.array(data_fit[\"iter\"] - 1) * CONVERT_ITERATION_PE_TIME\n",
    "y_fit = abs(data_fit[\"E_vecs\"][:, 0])\n",
    "y_fit_errors = data_fit[\"point_errors\"][:, 0]\n",
    "\n",
    "# Prepare Target Data (PE_425K Refined)\n",
    "data_target_pe = all_processed[KEY_TARGET_PE]\n",
    "x_extrapolate = np.array(data_target_pe[\"iter\"] - 1) * CONVERT_ITERATION_PE_TIME\n",
    "y_target_pe = abs(data_target_pe[\"E_vecs\"][:, 0])\n",
    "y_target_pe_errors = data_target_pe[\"point_errors\"][:, 0]\n",
    "\n",
    "# --- Fitting ---\n",
    "method_label = f\"Fit of {KEY_FIT.split('_')[1]} (Poly Order 3)\"\n",
    "try:\n",
    "    popt, pcov = curve_fit(poly_curve, x_fit, y_fit, \n",
    "                           p0=[0, 0, 0, 1e4], sigma=y_fit_errors, absolute_sigma=True)\n",
    "    \n",
    "    A_fit, B_fit, C_fit, D_fit = popt\n",
    "    print(f\"\\nFit Parameters for {KEY_FIT.split('_')[1]} (Poly): a={A_fit:.2e}, b={B_fit:.2e}, c={C_fit:.2e}, d={D_fit:.2e}\")\n",
    "\n",
    "    # Extrapolate and Calculate Subtraction\n",
    "    y_extrapolated = poly_curve(x_extrapolate, *popt)\n",
    "    y_subtraction = y_extrapolated - y_target_pe\n",
    "\n",
    "    # Error estimation for the extrapolation\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    J = np.array([3*x_extrapolate**2, 2*x_extrapolate, np.ones_like(x_extrapolate), np.zeros_like(x_extrapolate)]).T\n",
    "    y_extrapolated_errors = np.sqrt(np.diag(J @ pcov @ J.T))\n",
    "\n",
    "\n",
    "except RuntimeError:\n",
    "    print(\"\\n⚠️ Warning: Curve fitting failed. Falling back to zeros.\")\n",
    "    y_extrapolated = np.zeros_like(x_fit)\n",
    "    y_subtraction = np.zeros_like(x_fit)\n",
    "    x_extrapolate = x_fit\n",
    "    method_label = f\"Fit of {KEY_FIT.split('_')[1]} (Failed, showing Zeros)\"\n",
    "    y_extrapolated_errors = np.zeros_like(x_extrapolate)\n",
    "\n",
    "# --- Calculate Percent Difference for Fit vs. Target ---\n",
    "y_denominator = np.where(y_extrapolated == 0, 1e-10, y_extrapolated)\n",
    "y_percent_diff = (y_subtraction / y_denominator) * 100\n",
    "y_subtraction_errors = np.sqrt(y_extrapolated_errors**2 + y_target_pe_errors**2)\n",
    "y_percent_diff_errors = (y_subtraction_errors / np.abs(y_denominator)) * 100 \n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# --- 3. ZIMMERMAN COMPARISON CALCULATIONS (UPDATED FOR ERROR) ---\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- PE Simulation vs. Zimmerman PE Literature ---\n",
    "x_lit_pe = 10**zimmerman_PEdata[\"x\"]\n",
    "y_lit_pe = zimmerman_PEdata[\" y\"]\n",
    "\n",
    "# Interpolate Zimmerman PE data to the simulation time points (x_extrapolate)\n",
    "f_interp_pe = interp1d(x_lit_pe, y_lit_pe, kind='linear', fill_value=\"extrapolate\")\n",
    "y_lit_pe_interp = f_interp_pe(x_extrapolate)\n",
    "\n",
    "# Calculate % Difference: (Sim - Lit) / Lit\n",
    "y_zimmerman_pe_diff = ((y_target_pe - y_lit_pe_interp) / y_lit_pe_interp) * 100\n",
    "# Calculate % Difference Error: (Sim_err / Lit_interp) * 100 (Assuming Lit error is negligible)\n",
    "y_zimmerman_pe_diff_errors = (y_target_pe_errors / np.abs(y_lit_pe_interp)) * 100\n",
    "\n",
    "\n",
    "# --- SW Simulation vs. Zimmerman SW Literature ---\n",
    "# 1. Get SW simulation data\n",
    "KEY_SIM_SW = KEY_TARGET_SW\n",
    "data_target_sw = all_processed.get(KEY_SIM_SW, None)\n",
    "\n",
    "if data_target_sw is not None:\n",
    "    x_target_sw = np.array(data_target_sw[\"iter\"] - 1) * CONVERT_ITERATION_SW_TIME\n",
    "    y_target_sw = abs(data_target_sw[\"E_vecs\"][:, 0])\n",
    "    y_target_sw_errors = data_target_sw[\"point_errors\"][:, 0] # EXTRACTED SW ERRORS\n",
    "\n",
    "    # 2. Get SW literature data\n",
    "    x_lit_sw = 10**zimmerman_SWdata[\"x\"]\n",
    "    y_lit_sw = zimmerman_SWdata[\" y\"]\n",
    "\n",
    "    # Interpolate Zimmerman SW data to the SW simulation time points (x_target_sw)\n",
    "    f_interp_sw = interp1d(x_lit_sw, y_lit_sw, kind='linear', fill_value=\"extrapolate\")\n",
    "    y_lit_sw_interp = f_interp_sw(x_target_sw)\n",
    "\n",
    "    # Calculate % Difference: (Sim - Lit) / Lit\n",
    "    y_zimmerman_sw_diff = ((y_target_sw - y_lit_sw_interp) / y_lit_sw_interp) * 100\n",
    "    # Calculate % Difference Error: (Sim_err / Lit_interp) * 100 (Assuming Lit error is negligible)\n",
    "    y_zimmerman_sw_diff_errors = (y_target_sw_errors / np.abs(y_lit_sw_interp)) * 100 \n",
    "else:\n",
    "    print(f\"⚠️ Warning: SW target key '{KEY_SIM_SW}' not found in all_processed. Skipping SW comparison.\")\n",
    "    x_target_sw = np.array([])\n",
    "    y_zimmerman_sw_diff = np.array([])\n",
    "    y_zimmerman_sw_diff_errors = np.array([]) # Defined for consistent error calculation\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- 4. PLOTTING SETUP (MAIN + 2 SUBPLOTS) ---\n",
    "\n",
    "# Set up figure and grid layout (5:1:1 height ratio for main plot vs. residual vs. benchmark plot)\n",
    "fig = plt.figure(figsize=(8.01, 4.6))\n",
    "gs = gridspec.GridSpec(3, 1, hspace=0.15, height_ratios=[4, 0.8, 0.8]) \n",
    "\n",
    "# Main Plot (Top)\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "# Fit Residual Plot (Middle), sharing the x-axis\n",
    "ax_fit_res = fig.add_subplot(gs[1], sharex=ax_main)\n",
    "# Zimmerman Benchmark Plot (Bottom), sharing the x-axis\n",
    "ax_zimm_bench = fig.add_subplot(gs[2], sharex=ax_main)\n",
    "\n",
    "# color_list is defined here for use in section 7\n",
    "color_list = [color_list_rgba[2],color_list_rgba[3],color_list_rgba[6]]\n",
    "\n",
    "# --- 5. MAIN PLOT GENERATION (ax_main) ---\n",
    "\n",
    "# Plot reference data (Zimmerman)\n",
    "ax_main.plot(x_lit_sw, y_lit_sw, '-', color=\"k\", lw=3, alpha=0.3, label=\"Zimmerman SW/PE/PE+SW Ref.\")\n",
    "ax_main.plot(x_lit_pe, y_lit_pe, '-', color=\"k\", lw=3, alpha=0.3)\n",
    "#ax_main.plot(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"], '--', color=\"k\", lw=3, alpha=0.3)\n",
    "\n",
    "# Plot simulation data\n",
    "color_list = [color_list_rgba[2],color_list_rgba[3],color_list_rgba[6]]\n",
    "i=0\n",
    "\n",
    "# # Plot the fitted/extrapolated curve\n",
    "ax_main.plot(x_extrapolate, y_extrapolated, '-', color=color_list[-1], lw=2, \n",
    "             label=f\"{method_label} (Extrapolated)\")\n",
    "\n",
    "for keyIN, colorIN in zip(all_processed.keys(), color_list_rgba):\n",
    "    \n",
    "    # Define plotting variables outside of loop to use them later\n",
    "    case = all_processed[keyIN][\"metadata\"][\"case\"]\n",
    "    factor = CONVERT_ITERATION_PE_TIME if case == \"PE\" else CONVERT_ITERATION_SW_TIME\n",
    "    tempIN = all_processed[keyIN][\"metadata\"][\"temperature\"]\n",
    "    targetIN = all_processed[keyIN][\"metadata\"][\"target_point\"]\n",
    "    \n",
    "    x_data = np.array(all_processed[keyIN][\"iter\"] - 1) * factor\n",
    "    y_data = abs(all_processed[keyIN][\"E_vecs\"][:, 0])\n",
    "    y_err = all_processed[keyIN][\"point_errors\"][:, 0]\n",
    "    \n",
    "    tempIN = all_processed[keyIN][\"metadata\"][\"temperature\"]\n",
    "    targetIN = all_processed[keyIN][\"metadata\"][\"target_point\"]\n",
    "    # noteIN = keyIN.split(\"_\")[2] # Not used in label for brevity\n",
    "\n",
    "    # Filter plotting to only the relevant cases (e.g., specific position and T=425)\n",
    "    if (targetIN[0] < 0) & (tempIN == 425) & (\"Total\" not in keyIN.split(\"_\")[3]):\n",
    "        \n",
    "        print(keyIN)\n",
    "\n",
    "        plot_color = color_list[i]\n",
    "\n",
    "        if keyIN == KEY_FIT:\n",
    "            # # Plot the data line\n",
    "            # ax_main.plot(x_data, y_data, '--', color=plot_color, lw=1)\n",
    "            continue\n",
    "        else:\n",
    "            # Plot the data line\n",
    "            ax_main.plot(x_data, y_data, '-', color=plot_color, lw=1.5)\n",
    "            \n",
    "            # Use fill_between for the error region (Replaces errorbars)\n",
    "            ax_main.fill_between(x_data, y_data - y_err, y_data + y_err, \n",
    "                                color=plot_color, alpha=0.15, \n",
    "                                label=None) # Set label=None to avoid extra legend entry\n",
    "        i+=1\n",
    "# Clean up main plot\n",
    "ax_main.set_ylabel(r\"$|E_x|$ (V/m)\")\n",
    "ax_main.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "ax_main.set_ylim(0,2.2e5)\n",
    "# ax_main.axvline(x=0.32754498952096356)\n",
    "# ax_main.axvline(x=3.2390560074850843)\n",
    "# ax_main.grid(True, linestyle=':', alpha=0.5) \n",
    "# ax_main.legend(loc='lower left', fontsize=8, ncol=2)\n",
    "# Remove X-tick labels from the main plot\n",
    "plt.setp(ax_main.get_xticklabels(), visible=False) \n",
    "\n",
    "\n",
    "# --- 6. FIT RESIDUAL PLOT GENERATION (ax_fit_res) ---\n",
    "\n",
    "# PLOT PERCENT DIFFERENCE WITH SHADED ERROR REGION\n",
    "ax_fit_res.plot(x_extrapolate, y_percent_diff, '-', color=color_list[-2], lw=2,\n",
    "            label=r\"% Diff: $\\frac{|E_{Fit}| - |E_{Target}|}{|E_{Fit}|}$\")\n",
    "# Shaded region\n",
    "# ax_fit_res.fill_between(x_extrapolate, y_percent_diff - y_percent_diff_errors, \n",
    "#                     y_percent_diff + y_percent_diff_errors, \n",
    "#                     color=color_list[2], alpha=0.2, label=\"Fit Error Region\")\n",
    "# ax_fit_res.set_ylabel(r\"Fit % Diff\")\n",
    "# ax_fit_res.grid(True, linestyle=':', alpha=0.6)\n",
    "# ax_fit_res.legend(loc='upper right', fontsize=8)\n",
    "#ax_fit_res.set_ylim(0,10) \n",
    "#ax_fit_res.set_yticks([0,4,8])\n",
    "# Remove X-tick labels from the fit residual plot\n",
    "plt.setp(ax_fit_res.get_xticklabels(), visible=False) \n",
    "\n",
    "# --- 7. ZIMMERMAN BENCHMARK PLOT GENERATION (ax_zimm_bench) (UPDATED WITH SHADED REGION) ---\n",
    "\n",
    "# PE Comparison\n",
    "ax_zimm_bench.plot(x_extrapolate, y_zimmerman_pe_diff, '-', color=color_list[1], lw=2,\n",
    "                   label=r\"% Diff: $\\frac{|E_{\\text{PE Sim}}| - |E_{\\text{PE Lit}}|}{|E_{\\text{PE Lit}}|}$\")\n",
    "\n",
    "# # PE Shaded region (using same color as line, color_list[1])\n",
    "# ax_zimm_bench.fill_between(x_extrapolate, y_zimmerman_pe_diff - y_zimmerman_pe_diff_errors, \n",
    "#                            y_zimmerman_pe_diff + y_zimmerman_pe_diff_errors, \n",
    "#                            color=color_list[1], alpha=0.2, label=None)\n",
    "\n",
    "# SW Comparison\n",
    "if x_target_sw.size > 0:\n",
    "    ax_zimm_bench.plot(x_target_sw, y_zimmerman_sw_diff, '-', color=color_list[0], lw=2,\n",
    "                       label=r\"% Diff: $\\frac{|E_{\\text{SW Sim}}| - |E_{\\text{SW Lit}}|}{|E_{\\text{SW Lit}}|}$\")\n",
    "    \n",
    "    # # SW Shaded region (using same color as line, color_list[0])\n",
    "    # ax_zimm_bench.fill_between(x_target_sw, y_zimmerman_sw_diff - y_zimmerman_sw_diff_errors, \n",
    "    #                            y_zimmerman_sw_diff + y_zimmerman_sw_diff_errors, \n",
    "    #                            color=color_list[0], alpha=0.2, label=\"Error Region\") # Labeled the SW error region\n",
    "\n",
    "ax_zimm_bench.axhline(0, color='k', linestyle='-', lw=0.5, alpha=0.8) # Zero line\n",
    "ax_zimm_bench.set_xlabel(\"Time [s]\")\n",
    "ax_zimm_bench.set_ylabel(r\"% Diff\",loc=\"top\")\n",
    "# ax_zimm_bench.grid(True, linestyle=':', alpha=0.6) \n",
    "# ax_zimm_bench.legend(loc='upper right', fontsize=8)\n",
    "ax_zimm_bench.set_xlim(0,8) \n",
    "ax_zimm_bench.set_ylim(-40, 20) # A wider limit for benchmark comparison\n",
    "ax_zimm_bench.set_yticks([-40,-20, 0, 20])\n",
    "\n",
    "\n",
    "# --- 8. SAVE AND SHOW ---\n",
    "plt.savefig(\"figures/zimmerman_benchmark_summary_with_lit_comparison.jpeg\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LaTeX rendering\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Set the global font size\n",
    "mpl.rcParams.update({'font.size': 12})\n",
    "\n",
    "# --- 1. CONFIGURATION AND DATA LOADING ---\n",
    "print(\"\\n--- Starting Data Processing and Plot Generation ---\")\n",
    "\n",
    "# Define Data Keys for Fitting and Subtraction\n",
    "KEY_FIT = 'PE_425K_initial8max0.8final12_noDissipation_sphere50um_pos-0.1'\n",
    "KEY_TARGET = 'PE_425K_initial8max0.8final12_RefinedGridDissipation_sphere20um_pos-0.1'\n",
    "\n",
    "# Load external literature data\n",
    "zimmerman_SWdata = pd.read_csv(\"literature-data/Fig7a-SW.csv\")\n",
    "zimmerman_PEdata = pd.read_csv(\"literature-data/Fig7a-PE.csv\")\n",
    "zimmerman_PEandSWdata = pd.read_csv(\"literature-data/Fig7a-PE+SW.csv\")\n",
    "\n",
    "# Simulation Parameters (used for time conversion)\n",
    "WORLD_XY_AREA_SQ_M = 300 * 300 / (1e6**2) # World area (m^2)\n",
    "\n",
    "# PE Conversion Factor\n",
    "PARTICLES_PER_ITERATION_PE = 81775\n",
    "FLUX_PER_ITERATION_PE = PARTICLES_PER_ITERATION_PE / WORLD_XY_AREA_SQ_M\n",
    "PE_ION_FLUX = 4e-6 * 6.241509e18\n",
    "CONVERT_ITERATION_PE_TIME = FLUX_PER_ITERATION_PE / PE_ION_FLUX\n",
    "print(f\"PE Conversion Factor (s/iteration): {CONVERT_ITERATION_PE_TIME:.3e}\")\n",
    "\n",
    "# SW Conversion Factor\n",
    "PARTICLES_PER_ITERATION_SW = 30601\n",
    "FLUX_PER_ITERATION_SW = PARTICLES_PER_ITERATION_SW / WORLD_XY_AREA_SQ_M\n",
    "SW_ION_FLUX = 3e-7 * 6.241509e18\n",
    "CONVERT_ITERATION_SW_TIME = FLUX_PER_ITERATION_SW / SW_ION_FLUX\n",
    "print(f\"SW Conversion Factor (s/iteration): {CONVERT_ITERATION_SW_TIME:.3e}\")\n",
    "\n",
    "# Color Map Setup\n",
    "CMAP_NAME = 'Dark2'\n",
    "discrete_cmap = plt.get_cmap(CMAP_NAME, len(all_processed.keys()) + 1)\n",
    "color_list_rgba = [discrete_cmap(i) for i in np.linspace(0, 1, len(all_processed.keys()) + 1)]\n",
    "\n",
    "# --- 2. CURVE FITTING AND EXTRAPOLATION ---\n",
    "\n",
    "# Define the new fitting function (Polynomial of Order 3)\n",
    "def poly_curve(t, a, b, c, d):\n",
    "    \"\"\"\n",
    "    Function: a*t^3 + b*t^2 + c*t + d (Polynomial of Order 3)\n",
    "    \"\"\"\n",
    "    return a*t**3 + b*t**2 + c*t + d\n",
    "\n",
    "\n",
    "# Prepare data for fitting (PE_425K)\n",
    "data_fit = all_processed[KEY_FIT]\n",
    "x_fit = np.array(data_fit[\"iter\"] - 1) * CONVERT_ITERATION_PE_TIME\n",
    "y_fit = abs(data_fit[\"E_vecs\"][:, 0])\n",
    "y_fit_errors = data_fit[\"point_errors\"][:, 0] # Get errors for KEY_FIT (50um)\n",
    "\n",
    "# Prepare Target Data (PE_600K range)\n",
    "data_target = all_processed[KEY_TARGET]\n",
    "x_extrapolate = np.array(data_target[\"iter\"] - 1) * CONVERT_ITERATION_PE_TIME\n",
    "y_target = abs(data_target[\"E_vecs\"][:, 0])\n",
    "y_target_errors = data_target[\"point_errors\"][:, 0] # Get errors for KEY_TARGET (20um)\n",
    "\n",
    "# --- Choose Fitting/Extrapolation Method ---\n",
    "\n",
    "# Method 1: Basic Polynomial Fit (Order 3) - CURRENT DEFAULT\n",
    "method_label = f\"Fit of {KEY_FIT.split('_')[1]} (Poly Order 3)\"\n",
    "try:\n",
    "    # Using y_fit_errors as sigma for weighted fit\n",
    "    popt, pcov = curve_fit(poly_curve, x_fit, y_fit, \n",
    "                           p0=[0, 0, 0, 1e4], sigma=y_fit_errors, absolute_sigma=True) # Added sigma\n",
    "    \n",
    "    A_fit, B_fit, C_fit, D_fit = popt\n",
    "    print(f\"\\nFit Parameters for {KEY_FIT.split('_')[1]} (Poly): a={A_fit:.2e}, b={B_fit:.2e}, c={C_fit:.2e}, d={D_fit:.2e}\")\n",
    "\n",
    "    # Extrapolate and Calculate Subtraction\n",
    "    y_extrapolated = poly_curve(x_extrapolate, *popt)\n",
    "    y_subtraction = y_extrapolated - y_target\n",
    "\n",
    "    # Estimate error in extrapolated fit for error propagation\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    # NOTE: J calculation assumes the x_extrapolate points are the basis for the estimated error\n",
    "    J = np.array([3*x_extrapolate**2, 2*x_extrapolate, np.ones_like(x_extrapolate), np.zeros_like(x_extrapolate)]).T\n",
    "    y_extrapolated_errors = np.sqrt(np.diag(J @ pcov @ J.T))\n",
    "\n",
    "\n",
    "except RuntimeError:\n",
    "    print(\"\\n⚠️ Warning: Curve fitting failed. Check initial guess (p0) or fitting range.\")\n",
    "    y_extrapolated = np.zeros_like(x_fit)\n",
    "    y_subtraction = np.zeros_like(x_fit)\n",
    "    x_extrapolate = x_fit\n",
    "    method_label = f\"Fit of {KEY_FIT.split('_')[1]} (Failed, showing Zeros)\"\n",
    "    y_extrapolated_errors = np.zeros_like(x_extrapolate) # Set errors to zero if fit fails\n",
    "\n",
    "\n",
    "# --- Calculate Percent Difference and its Error ---\n",
    "y_denominator = np.where(y_extrapolated == 0, 1e-10, y_extrapolated)\n",
    "y_percent_diff = (y_subtraction / y_denominator) * 100\n",
    "\n",
    "# Error propagation for the difference: sqrt(error_fit^2 + error_target^2)\n",
    "# Direct assignment works because y_target_errors has the same length as x_extrapolate\n",
    "y_target_errors_interp = y_target_errors \n",
    "\n",
    "y_subtraction_errors = np.sqrt(y_extrapolated_errors**2 + y_target_errors_interp**2)\n",
    "\n",
    "# Error propagation for the percentage\n",
    "y_percent_diff_errors = (y_subtraction_errors / np.abs(y_denominator)) * 100 \n",
    "# ---------------------------------------------------\n",
    "\n",
    "# --- 3. PLOTTING SETUP (MAIN + SUBPLOT) ---\n",
    "\n",
    "# Set up figure and grid layout (5:1 height ratio for main plot vs. residual plot)\n",
    "fig = plt.figure(figsize=(8.01, 3.22))\n",
    "gs = gridspec.GridSpec(2, 1, hspace=0.1, height_ratios=[5, 1])\n",
    "\n",
    "# Main Plot (Top)\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "# Subtraction Plot (Bottom), sharing the x-axis\n",
    "ax_sub = fig.add_subplot(gs[1], sharex=ax_main)\n",
    "\n",
    "# --- 4. MAIN PLOT GENERATION (ax_main) ---\n",
    "\n",
    "# Plot reference data (Zimmerman)\n",
    "ax_main.plot(10**zimmerman_SWdata[\"x\"], zimmerman_SWdata[\" y\"], '-', color=\"k\", lw=3, alpha=0.3, label=\"Zimmerman SW/PE/PE+SW Ref.\")\n",
    "ax_main.plot(10**zimmerman_PEdata[\"x\"], zimmerman_PEdata[\" y\"], '-', color=\"k\", lw=3, alpha=0.3)\n",
    "#ax_main.plot(10**zimmerman_PEandSWdata[\"x\"], zimmerman_PEandSWdata[\" y\"], '--', color=\"k\", lw=3, alpha=0.3)\n",
    "\n",
    "# Plot simulation data\n",
    "color_list = [color_list_rgba[2],color_list_rgba[3],color_list_rgba[3]]\n",
    "i=0\n",
    "\n",
    "# # Plot the fitted/extrapolated curve\n",
    "ax_main.plot(x_extrapolate, y_extrapolated, ':', color=color_list[-1], lw=2, \n",
    "             label=f\"{method_label} (Extrapolated)\")\n",
    "\n",
    "for keyIN, colorIN in zip(all_processed.keys(), color_list_rgba):\n",
    "    \n",
    "    # Define plotting variables outside of loop to use them later\n",
    "    case = all_processed[keyIN][\"metadata\"][\"case\"]\n",
    "    factor = CONVERT_ITERATION_PE_TIME if case == \"PE\" else CONVERT_ITERATION_SW_TIME\n",
    "    tempIN = all_processed[keyIN][\"metadata\"][\"temperature\"]\n",
    "    targetIN = all_processed[keyIN][\"metadata\"][\"target_point\"]\n",
    "    \n",
    "    x_data = np.array(all_processed[keyIN][\"iter\"] - 1) * factor\n",
    "    y_data = abs(all_processed[keyIN][\"E_vecs\"][:, 0])\n",
    "    y_err = all_processed[keyIN][\"point_errors\"][:, 0]\n",
    "    \n",
    "    tempIN = all_processed[keyIN][\"metadata\"][\"temperature\"]\n",
    "    targetIN = all_processed[keyIN][\"metadata\"][\"target_point\"]\n",
    "    # noteIN = keyIN.split(\"_\")[2] # Not used in label for brevity\n",
    "\n",
    "    # Filter plotting to only the relevant cases (e.g., specific position and T=425)\n",
    "    if (targetIN[0] < 0) & (tempIN == 425) & (\"Total\" not in keyIN.split(\"_\")[3]):\n",
    "        \n",
    "        print(keyIN)\n",
    "\n",
    "        plot_color = color_list[i]\n",
    "\n",
    "        if keyIN == KEY_FIT:\n",
    "            # # Plot the data line\n",
    "            # ax_main.plot(x_data, y_data, '--', color=plot_color, lw=1)\n",
    "            continue\n",
    "        else:\n",
    "            # Plot the data line\n",
    "            ax_main.plot(x_data, y_data, '-', color=plot_color, lw=1.5)\n",
    "            \n",
    "            # Use fill_between for the error region (Replaces errorbars)\n",
    "            ax_main.fill_between(x_data, y_data - y_err, y_data + y_err, \n",
    "                                color=plot_color, alpha=0.15, \n",
    "                                label=None) # Set label=None to avoid extra legend entry\n",
    "        i+=1\n",
    "\n",
    "# Add original uncommented features back to ax_main\n",
    "#ax_main.axvline(x=65 * CONVERT_ITERATION_PE_TIME, color='gray', linestyle='-.', lw=1, alpha=0.7, label=\"Vertical Marker\")\n",
    "ax_main.set_ylabel(r\"$|E_x|$ (V/m)\")\n",
    "ax_main.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "#ax_main.set_yscale('log') # Use log scale for better visualization of power-law decay\n",
    "ax_main.set_ylim(0,2.4e5) # Uncommented limits\n",
    "# ax_main.set_xlim(7.4e-1, 1.25e1) # Uncommented limits\n",
    "\n",
    "# Clean up main plot\n",
    "# ax_main.grid(True, linestyle=':', alpha=0.5)\n",
    "# ax_main.legend(loc='lower left', fontsize=8, ncol=2)\n",
    "# Remove X-tick labels from the main plot\n",
    "plt.setp(ax_main.get_xticklabels(), visible=False) \n",
    "\n",
    "# --- 5. SUBTRACTION PLOT GENERATION (ax_sub) ---\n",
    "\n",
    "# PLOT PERCENT DIFFERENCE WITH SHADED ERROR REGION\n",
    "ax_sub.plot(x_extrapolate, y_percent_diff, '-', color=color_list_rgba[3], lw=2,\n",
    "            label=r\"Relative Error: $\\frac{|E_{Fit}| - |E_{Target}|}{|E_{Fit}|}$\")\n",
    "# Shaded region\n",
    "# ax_sub.fill_between(x_extrapolate, y_percent_diff - y_percent_diff_errors, \n",
    "#                     y_percent_diff + y_percent_diff_errors, \n",
    "#                     color=color_list_rgba[2], alpha=0.2, label=\"Error Region\")\n",
    "ax_sub.set_xlabel(\"Time [s]\")\n",
    "#ax_sub.set_ylabel(r\"% Diff\")\n",
    "# ax_sub.ticklabel_format(axis='y', style='sci', scilimits=(0, 0)) # Removed, % difference is typically not sci notation\n",
    "# ax_sub.grid(True, linestyle=':', alpha=0.6)\n",
    "# ax_sub.legend(loc='upper right', fontsize=8)\n",
    "ax_sub.set_xlim(0,6) # Uncommented limit check (if sharing xlim works)\n",
    "ax_sub.set_ylim(0,10) # Uncommented limit check (if sharing xlim works)\n",
    "ax_sub.set_yticks([0,4,8])\n",
    "\n",
    "# --- 6. SAVE AND SHOW ---\n",
    "plt.savefig(\"figures/zimmerman_benchmark_summary.jpeg\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 90\n",
    "print(f\"PE equivalent time for iteration#{iteration}: {(iteration-1)*CONVERT_ITERATION_PE_TIME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2D Representation of the Electric Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN STACKED SPHERES GEOMETRY ## \n",
    "\n",
    "stacked_spheres = trimesh.load_mesh('../sphere-charging/geometry/isolated_grain_interpolated.stl') \n",
    "\n",
    "# # Visualize with Trimesh\n",
    "# stacked_spheres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_spheres.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 8\n",
    "\n",
    "configIN = \"onlyphotoemission\"\n",
    "#directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/build-dissipationRefinedGrid-initial8max0.8final12/\"\n",
    "directory = \"../build-wang-comparison\"\n",
    "\n",
    "if iteration <10 :\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*00{iteration}*{configIN}*.txt\")) #{iteration}\n",
    "else:\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*{iteration}*{configIN}*.txt\")) #{iteration}\n",
    "print(filenames)\n",
    "\n",
    "df  = read_data_format_efficient(filenames,scaling=True)\n",
    "\n",
    "# check to make sure this matches the total nodes in outputlogs\n",
    "#len(df[iteration][\"E_mag\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS HERE ARE OPTIMIZED FOR ITERATION 86 ##\n",
    "\n",
    "fieldIN = df[iteration]\n",
    "\n",
    "center = stacked_spheres.centroid\n",
    "N_DOWNSAMPLE_EMAG = 1\n",
    "ARROW_VOXEL_SPACING = 0.02 \n",
    "Y_SLICE = 0.0 + center[1]\n",
    "THICKNESS = 0.001\n",
    "VECTOR_SCALE_FACTOR = 1e-6 #2e-7 #2e-3 #5e-6 #2e-3 #5e-6 # Global scaling for glyphs\n",
    "FIELD_AVERAGE_RADIUS = 2.5e-3 #2e-3\n",
    "\n",
    "vmin, vmax = (-2e5, 2e5) # in log(E_mag) units\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) # \n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Voxel Downsampling Helper Function\n",
    "# Ensures uniform spatial distribution of points in the slice\n",
    "# ----------------------------------------------------\n",
    "def voxel_downsample_points(points, spacing):\n",
    "    \"\"\"\n",
    "    Selects one point per voxel defined by the spacing.\n",
    "    Assumes points are 3D, but only uses X and Z for 2D density control.\n",
    "    \"\"\"\n",
    "    # 1. Normalize coordinates to voxel indices (focus on X and Z for the 2D slice)\n",
    "    min_x, _, min_z = points.min(axis=0)\n",
    "    \n",
    "    # Calculate bin indices for the points\n",
    "    # We use X (column 0) and Z (column 2)\n",
    "    x_indices = np.floor((points[:, 0] - min_x) / spacing).astype(int)\n",
    "    z_indices = np.floor((points[:, 2] - min_z) / spacing).astype(int)\n",
    "    \n",
    "    # Combine X and Z indices into a unique hash/key\n",
    "    max_x_index = x_indices.max() + 1\n",
    "    voxel_keys = z_indices * max_x_index + x_indices\n",
    "\n",
    "    # 2. Find the unique keys and their first occurrence\n",
    "    # `return_index=True` gives the index of the first occurrence of each unique key\n",
    "    unique_keys, unique_indices = np.unique(voxel_keys, return_index=True)\n",
    "    \n",
    "    return unique_indices\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 0: Load, Filter, and Downsample Data (Single Pass)\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    "\n",
    "# Apply initial filtering (z > 0 and magnitude > 0)\n",
    "initial_mask = (points[:, 2] > 0) & (magnitudes > 0)\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    "\n",
    "# Aggressive Downsample (for point cloud, typically N_DOWNSAMPLE_EMAG=1 is best)\n",
    "points_ds = points[::N_DOWNSAMPLE_EMAG]\n",
    "vectors_ds = vectors[::N_DOWNSAMPLE_EMAG]\n",
    "magnitudes_ds = magnitudes[::N_DOWNSAMPLE_EMAG]\n",
    "\n",
    "# Create a PyVista Point Cloud (PolyData)\n",
    "point_cloud = pv.PolyData(points_ds)\n",
    "point_cloud[\"E_mag\"] = magnitudes_ds   # Store log magnitude for visualization\n",
    "point_cloud[\"Ex_val\"] = vectors_ds[:,0] # Store vectors\n",
    "point_cloud[\"Ez_val\"] = vectors_ds[:,2] # Store vectors\n",
    "\n",
    "print(f\"Starting points (filtered by z > 0 & mag > 0): {len(points)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup and Slicing\n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    "\n",
    "# 1a. Load and Crop Geometry\n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    "\n",
    "# Define bounding box based on the downsampled field data\n",
    "bbox_bounds = point_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = pv_spheres.clip_box(bbox, invert=False)\n",
    "\n",
    "# 1b. Define the slice plane (ZX plane, normal along Y)\n",
    "normal = [0, 1, 0] # ZX plane (normal along Y)\n",
    "\n",
    "# Create a plane mesh for interpolation (this will be the magnitude slice)\n",
    "plane_bounds = [\n",
    "    point_cloud.bounds[0], point_cloud.bounds[1], # X bounds\n",
    "    Y_SLICE, Y_SLICE,                             # Y (fixed)\n",
    "    point_cloud.bounds[4], point_cloud.bounds[5]  # Z bounds\n",
    "]\n",
    "\n",
    "field_slice_mesh = pv.Plane(\n",
    "    center=center, \n",
    "    direction=normal,\n",
    "    j_size=bbox_bounds[1] - bbox_bounds[0], # X span\n",
    "    i_size=bbox_bounds[5] - bbox_bounds[4], # Z span\n",
    "    i_resolution=250, \n",
    "    j_resolution=250\n",
    ")\n",
    "\n",
    "# --- MODIFIED INTERPOLATION CALL FOR NEAREST NEIGHBOR ---\n",
    "field_slice_interpolated = field_slice_mesh.interpolate(\n",
    "    point_cloud,\n",
    "    sharpness=3.0,      # High sharpness often helps with point data\n",
    "    radius=0.001, #1e-12,       # Set radius to near-zero to minimize interpolation\n",
    "    \n",
    "    # 1. Provide a float placeholder to satisfy the TypeError\n",
    "    null_value=1, \n",
    "\n",
    "    # 2. Force the strategy to use the nearest point (Nearest Neighbor)\n",
    "    strategy='closest_point' # <--- This achieves the extrapolation you want\n",
    ")\n",
    "# --------------------------------------------------------\n",
    "\n",
    "field_slice_interpolated.points[:, 1] = Y_SLICE\n",
    "\n",
    "# Also update the geometry slice\n",
    "geo_slice = pv_spheres_cropped.slice(normal=normal, origin=center)\n",
    "print(f\"Geometry and slicing preparation complete in {time.time() - start_time_geo:.2f}s\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 2: Vector Field Glyphs (Arrows)\n",
    "# ----------------------------------------------------\n",
    "start_time_vectors = time.time()\n",
    "\n",
    "# 2a. Filter the downsampled points again to extract only those in the slice volume\n",
    "# We use NumPy masking directly on the downsampled data (points_ds)\n",
    "vector_mask = np.abs(points_ds[:, 1] - Y_SLICE) < THICKNESS\n",
    "points_slice_full = points_ds[vector_mask]\n",
    "vectors_slice_full = vectors_ds[vector_mask]\n",
    "magnitudes_slice_full = magnitudes_ds[vector_mask]\n",
    "\n",
    "# 2b. Apply Voxel Downsampling to achieve uniform density\n",
    "unique_indices = voxel_downsample_points(points_slice_full, ARROW_VOXEL_SPACING)\n",
    "\n",
    "points_slice = points_slice_full[unique_indices]\n",
    "vectors_slice = vectors_slice_full[unique_indices]\n",
    "magnitudes_slice = magnitudes_slice_full[unique_indices]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# MODIFICATION: Calculate Clamping Limit and Apply Clamping\n",
    "# ----------------------------------------------------\n",
    "# The maximum allowed length of an arrow is ARROW_VOXEL_SPACING.\n",
    "# The glyph length = magnitude * VECTOR_SCALE_FACTOR * arrow_length_in_geom (which is 1.0 for pv.Arrow).\n",
    "# To ensure: glyph_length <= ARROW_VOXEL_SPACING\n",
    "# We need: magnitude * VECTOR_SCALE_FACTOR <= ARROW_VOXEL_SPACING\n",
    "# Therefore: magnitude_clamped <= ARROW_VOXEL_SPACING / VECTOR_SCALE_FACTOR\n",
    "\n",
    "# Define the maximum magnitude allowed\n",
    "MAGNITUDE_MAX_CLAMP = ARROW_VOXEL_SPACING / VECTOR_SCALE_FACTOR /2\n",
    "\n",
    "# Apply the clamping (upper bound) to the magnitude array\n",
    "magnitudes_slice_clamped = np.clip(magnitudes_slice, a_min=None, a_max=MAGNITUDE_MAX_CLAMP)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# 2c. Create a PolyData object for glyphs\n",
    "points_slice[:,1] = Y_SLICE - 2*THICKNESS# Force y-coordinate to the slice plane for visualization\n",
    "vectors_slice[:,1] = 0.0 - 2* THICKNESS# Zero out Y component for 2D slice visualization\n",
    "slice_mesh_vectors = pv.PolyData(points_slice)\n",
    "slice_mesh_vectors['vectors'] = vectors_slice\n",
    "# Use the CLAMPED magnitude array for scaling\n",
    "slice_mesh_vectors['magnitude'] = magnitudes_slice_clamped\n",
    "#slice_mesh_vectors['magnitude'] = np.log10(magnitudes_slice)\n",
    "\n",
    "# # 2c. Create a PolyData object for glyphs\n",
    "# points_slice[:,1] = Y_SLICE - 2*THICKNESS# Force y-coordinate to the slice plane for visualization\n",
    "# vectors_slice[:,1] = 0.0 - 2* THICKNESS# Zero out Y component for 2D slice visualization\n",
    "# slice_mesh_vectors = pv.PolyData(points_slice)\n",
    "# slice_mesh_vectors['vectors'] = vectors_slice\n",
    "# #slice_mesh_vectors['magnitude'] = np.log10(magnitudes_slice)\n",
    "# slice_mesh_vectors['magnitude'] = magnitudes_slice\n",
    "\n",
    "print(f\"Points in vector slice (after density control): {len(points_slice)}, old length: {len(points_slice_full)}...\")\n",
    "\n",
    "# 2d. Create the glyphs\n",
    "arrow = pv.Arrow(tip_length=0.3, tip_radius=0.2, shaft_radius=0.04)\n",
    "glyphs = slice_mesh_vectors.glyph(\n",
    "    orient='vectors',\n",
    "    scale='magnitude',\n",
    "    factor=VECTOR_SCALE_FACTOR,\n",
    "    geom=arrow\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 3: Visualization\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"Ex_val\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    opacity=1,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # <-- set fixed color range here\n",
    "    # --- COLORBAR POSITIONING ---\n",
    "    scalar_bar_args={\n",
    "        'title':None, # r'log$_{10}$(E$_{mag}$)', # Updated title format\n",
    "        'vertical': False,            # Make it horizontal\n",
    "        'position_x': 0.20,           # User-specified start position\n",
    "        'position_y': 0.12,           # User-specified vertical position\n",
    "        'width': 0.6,                 # User-specified width\n",
    "        'height': 0.05,               # User-specified height\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add sliced geometry (outline only)\n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=5,opacity=0.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='black', show_scalar_bar=False, line_width=4,opacity=1)\n",
    "\n",
    "# # Optional marker\n",
    "sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "pl.add_mesh(sphere, color=\"red\", opacity=1)\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "\n",
    "# Align camera perpendicular to the slice\n",
    "pl.view_xz() \n",
    "\n",
    "# --- ADD THIS LINE BEFORE pl.show() ---\n",
    "pl.screenshot(f'figures/fieldvectors_{configIN}#{iteration}.jpeg', scale=4)\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Your existing setup code here...\n",
    "# -----------------------------------------\n",
    "fieldIN = df[iteration]\n",
    "FIELD_AVERAGE_RADIUS = 2.5e-3\n",
    "vmin, vmax = (-0.005, 0.005)\n",
    " \n",
    "geometry_center = stacked_spheres_centroid  # replace with your centroid\n",
    "red_point = np.array([-0.1, 0., 0.1 + 0.037]) + geometry_center\n",
    " \n",
    "# ----------------------------------------------------\n",
    "# Step 0: Filter data\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    " \n",
    "initial_mask = (magnitudes > 0)\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    " \n",
    "epsilon_0 = 8.854187817e-12\n",
    " \n",
    "# Create field cloud\n",
    "field_cloud = pv.PolyData(points)\n",
    "field_cloud[\"E_x\"] = vectors[:, 0]\n",
    "field_cloud[\"E_y\"] = vectors[:, 1]\n",
    "field_cloud[\"E_z\"] = vectors[:, 2]\n",
    " \n",
    "print(f\"Starting points (filtered): {len(points)}\")\n",
    " \n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup from vertices\n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    " \n",
    "# Convert vertex list to Nx3 numpy array\n",
    "vertices = np.array(stacked_spheres)  # replace stacked_spheres with your vertex list\n",
    " \n",
    "# Create a point cloud\n",
    "cloud = pv.PolyData(vertices)\n",
    " \n",
    "# Reconstruct a mesh from vertices\n",
    "# Use delaunay_2d if roughly planar, otherwise use reconstruct_surface\n",
    "print(\"before the construction\")\n",
    "pv_spheres = cloud.reconstruct_surface(nbr_neighbors=10)  # safer for 3D shapes\n",
    "print(\"finished the construction\")\n",
    "pv_spheres.compute_normals(inplace=True)\n",
    " \n",
    "# Crop mesh to field bounds\n",
    "bbox_bounds = field_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = (\n",
    "    pv_spheres\n",
    "    .clip_box(bbox, invert=False)\n",
    "    .extract_surface()\n",
    "    .compute_normals(point_normals=True, cell_normals=True, inplace=False)\n",
    ")\n",
    " \n",
    "# ============================================================\n",
    "# Interpolate field to face centers\n",
    "# ============================================================\n",
    "face_centers = pv_spheres_cropped.cell_centers().points\n",
    "face_field_cloud = pv.PolyData(face_centers)\n",
    "face_field_interp = face_field_cloud.interpolate(\n",
    "    field_cloud,\n",
    "    radius=0.002,\n",
    "    strategy='closest_point',\n",
    "    sharpness=3.0,\n",
    "    null_value=0.0\n",
    ")\n",
    " \n",
    "# Extract face-centered E-fields\n",
    "E_x_faces = face_field_interp[\"E_x\"]\n",
    "E_y_faces = face_field_interp[\"E_y\"]\n",
    "E_z_faces = face_field_interp[\"E_z\"]\n",
    "E_vec_faces = np.stack([E_x_faces, E_y_faces, E_z_faces], axis=1)\n",
    " \n",
    "# Face normals\n",
    "face_normals = pv_spheres_cropped.cell_normals\n",
    "nx = face_normals[:, 0]   # x-direction component\n",
    " \n",
    "# ============================================================\n",
    "# Compute Maxwell electric pressure (normal)\n",
    "# ============================================================\n",
    "E_dot_n = np.einsum('ij,ij->i', E_vec_faces, face_normals)\n",
    "E_mag_sq = np.einsum('ij,ij->i', E_vec_faces, E_vec_faces)\n",
    " \n",
    "# Normal pressure (scalar)\n",
    "P_normal_faces = epsilon_0 * (E_dot_n**2 - 0.5 * E_mag_sq)\n",
    " \n",
    "# ============================================================\n",
    "# Compute X-directed electric pressure\n",
    "# ============================================================\n",
    "P_x_faces = P_normal_faces * nx\n",
    "pv_spheres_cropped.cell_data[\"electric_pressure_x\"] = P_x_faces\n",
    " \n",
    "print(f\"Computed x-directed electric pressure in {time.time() - start_time_geo:.2f}s\")\n",
    " \n",
    "# ============================================================\n",
    "# Plotting using Px\n",
    "# ============================================================\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    " \n",
    "pl.add_mesh(\n",
    "    pv_spheres_cropped,\n",
    "    scalars=\"electric_pressure_x\",\n",
    "    cmap=\"seismic\",\n",
    "    opacity=1,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],\n",
    "    interpolate_before_map=False,\n",
    "    preference=\"cell\"\n",
    ")\n",
    " \n",
    "pl.view_xy()\n",
    "pl.show(jupyter_backend='static')\n",
    " \n",
    "# ============================================================\n",
    "# Extract Line Plot Data (y=0 top surface)\n",
    "# ============================================================\n",
    "y_tolerance = 0.01\n",
    "z_min = 0.08\n",
    " \n",
    "x_centers = face_centers[:, 0]\n",
    "y_centers = face_centers[:, 1]\n",
    "z_centers = face_centers[:, 2]\n",
    " \n",
    "Px_faces = P_x_faces  # shorthand\n",
    " \n",
    "line_mask = (np.abs(y_centers) < y_tolerance) & (z_centers > z_min)\n",
    " \n",
    "x_line = x_centers[line_mask]\n",
    "z_line = z_centers[line_mask]\n",
    "Px_line = Px_faces[line_mask]\n",
    " \n",
    "# Bin + average\n",
    "x_bin_width = 0.005\n",
    "x_min, x_max = x_line.min(), x_line.max()\n",
    "x_bins = np.arange(x_min, x_max + x_bin_width, x_bin_width)\n",
    "x_bin_centers = (x_bins[:-1] + x_bins[1:]) / 2\n",
    " \n",
    "bin_indices = np.digitize(x_line, x_bins)\n",
    " \n",
    "x_line_avg, z_line_avg, Px_line_avg = [], [], []\n",
    " \n",
    "for i in range(1, len(x_bins)):\n",
    "    mask = (bin_indices == i)\n",
    "    if mask.any():\n",
    "        x_line_avg.append(x_line[mask].mean())\n",
    "        z_line_avg.append(z_line[mask].mean())\n",
    "        Px_line_avg.append(Px_line[mask].mean())\n",
    " \n",
    "# Sort\n",
    "x_line_sorted_PE = np.array(x_line_avg)\n",
    "z_line_sorted_PE = np.array(z_line_avg)\n",
    "pressure_line_sorted_PE = np.array(Px_line_avg)\n",
    " \n",
    "sort_idx = np.argsort(x_line_sorted_PE)\n",
    "x_line_sorted_PE = x_line_sorted_PE[sort_idx]\n",
    "z_line_sorted_PE = z_line_sorted_PE[sort_idx]\n",
    "pressure_line_sorted_PE = pressure_line_sorted_PE[sort_idx]\n",
    " \n",
    "print(f\"Extracted {len(x_line)} raw points, averaged into {len(x_line_sorted_PE)} bins along y=0 line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Step 3: Visualization\n",
    "# ----------------------------------------------------\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add interpolated magnitude slice\n",
    "pl.add_mesh(\n",
    "    field_slice_interpolated,\n",
    "    scalars=\"Ex_val\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    opacity=1,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],   # <-- set fixed color range here\n",
    "    # --- COLORBAR POSITIONING FIX ---\n",
    "    scalar_bar_args={\n",
    "        'title':None, # r'log$_{10}$(E$_{mag}$)', # Updated title format\n",
    "        'vertical': False,            # Make it horizontal\n",
    "        'position_x': 0.20,           # User-specified start position\n",
    "        'position_y': 0.12,           # User-specified vertical position\n",
    "        'width': 0.6,                 # User-specified width\n",
    "        'height': 0.05,               # User-specified height\n",
    "    }\n",
    "    # -------------------------------\n",
    ")\n",
    "\n",
    "# Add sliced geometry (outline only)\n",
    "pl.add_mesh(geo_slice, color=\"black\", line_width=5,opacity=0.5)\n",
    "\n",
    "# Add vector glyphs\n",
    "pl.add_mesh(glyphs, color='black', show_scalar_bar=False, line_width=4,opacity=1)\n",
    "\n",
    "\n",
    "\n",
    "# # Optional marker\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.036]) # \n",
    "\n",
    "# Define field averaging parameters\n",
    "FIELD_AVERAGE_RADIUS = 2e-3\n",
    "\n",
    "offsetLimitsX = 0.009\n",
    "new_step_x = offsetLimitsX/3\n",
    "\n",
    "offsetLimitsY = 0.011\n",
    "new_step_y = offsetLimitsY/3\n",
    "\n",
    "xoffset = np.round(np.arange(-offsetLimitsX, offsetLimitsX, new_step_x),4)\n",
    "yoffset = np.round(np.arange(-offsetLimitsY, offsetLimitsY, new_step_y),4)\n",
    "\n",
    "X, Y = np.meshgrid(xoffset, yoffset)\n",
    "target_points_array = np.vstack([\n",
    "    -0.1 - X.flatten(), \n",
    "    np.zeros(len(X.flatten())), \n",
    "    0.1 - 0.015 + 0.037 - Y.flatten()\n",
    "]).T\n",
    "\n",
    "print(f\"Processing {len(target_points_array)} target points with radius {FIELD_AVERAGE_RADIUS} mm\")\n",
    "for red_point in target_points_array:\n",
    "\n",
    "    sphere = pv.Sphere(radius=FIELD_AVERAGE_RADIUS, center=red_point)\n",
    "    pl.add_mesh(sphere, color=\"red\", opacity=1)\n",
    "\n",
    "# # Combine into (N,3) array\n",
    "# polyline = pv.PolyData(target_points_array)\n",
    "# pl.add_mesh(polyline, color='r', point_size=1, opacity=0.8) # Add to the plot\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "\n",
    "# # Optional marker\n",
    "red_point = np.array([-0.1, 0, 0.1 - 0.015 + 0.037]) # \n",
    "radius_mm = 40/1000/2   # 20 µm\n",
    "center = red_point  # your np.array([-0.1-0.015, 0, 0.1 - 0.015 + 0.036])\n",
    "\n",
    "# Create circle points manually (in XZ plane)\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "x = center[0] + radius_mm * np.cos(theta)\n",
    "y = np.full_like(theta, center[1])   # constant y value (so it's in XZ plane)\n",
    "z = center[2] + radius_mm * np.sin(theta)\n",
    "\n",
    "# Combine into (N,3) array\n",
    "points = np.column_stack((x, y, z))\n",
    "polyline = pv.PolyData(points)\n",
    "pl.add_mesh(polyline, color='k', point_size=0.5, opacity=0.8) # Add to the plot\n",
    "\n",
    "\n",
    "# # Create the line and add to the plot\n",
    "# x_fixed,y_fixed = -0.1,0 \n",
    "# zmin, zmax = 0, 0.2  # adjust to fit your plot domain\n",
    "# points = np.array([[x_fixed, y_fixed, zmin],[x_fixed, y_fixed, zmax]])\n",
    "# line = pv.Line(points[0], points[1])\n",
    "# pl.add_mesh(line, color='black', line_width=2)\n",
    "\n",
    "# Force 2D (orthographic) projection and camera alignment for the ZX slice\n",
    "pl.enable_parallel_projection()\n",
    "pl.enable_2d_style()\n",
    "\n",
    "# Align camera perpendicular to the slice\n",
    "pl.view_xz() \n",
    "\n",
    "# --- ADD THIS LINE BEFORE pl.show() ---\n",
    "#pl.screenshot(f'figures/fieldvectors_{configIN}#{iteration}.jpeg', scale=4)\n",
    "\n",
    "# Show the plot\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f}s\")\n",
    "pl.show(jupyter_backend='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define field averaging parameters\n",
    "FIELD_AVERAGE_RADIUS = 2e-3\n",
    "\n",
    "offsetLimitsX = 0.009\n",
    "new_step_x = offsetLimitsX/3\n",
    "\n",
    "offsetLimitsY = 0.011\n",
    "new_step_y = offsetLimitsY/3\n",
    "\n",
    "xoffset = np.round(np.arange(-offsetLimitsX, offsetLimitsX, new_step_x),4)\n",
    "yoffset = np.round(np.arange(-offsetLimitsY, offsetLimitsY, new_step_y),4)\n",
    "X, Y = np.meshgrid(xoffset, yoffset)\n",
    "target_points_array = np.vstack([\n",
    "    -0.1 - X.flatten(), \n",
    "    np.zeros(len(X.flatten())), \n",
    "    0.1 - 0.015 + 0.037 - Y.flatten()\n",
    "]).T\n",
    "\n",
    "print(f\"Processing {len(target_points_array)} target points with radius {FIELD_AVERAGE_RADIUS} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsetLimitsX = 0.009\n",
    "new_step_x = offsetLimitsX/3\n",
    "\n",
    "# np.arange(start, stop, step)\n",
    "# We add a small epsilon (1e-9) to the stop value to guarantee\n",
    "# that the last value, 0.009, is included due to floating-point arithmetic.\n",
    "xoffset = np.round(np.arange(-offsetLimitsX, offsetLimitsX, new_step_x),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "xoffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LaTeX rendering\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Set the global font size\n",
    "mpl.rcParams.update({'font.size': 16})\n",
    "\n",
    "cmap = plt.cm.YlGnBu \n",
    "vmin, vmax = (-2e5, 2e5) # in log(E_mag) units\n",
    "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 0.1))\n",
    "\n",
    "# --- 1. Create and Configure the ScalarFormatter ---\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "\n",
    "formatter.set_useOffset(False) \n",
    "formatter.set_powerlimits((0, 0)) \n",
    "\n",
    "# --- 2. Create the Colorbar and apply the Formatter ---\n",
    "cb = mpl.colorbar.ColorbarBase(\n",
    "    ax, \n",
    "    cmap=cmap, \n",
    "    norm=norm, \n",
    "    orientation='horizontal', label=r\"E$_x$ (V/m)\"\n",
    ")\n",
    "\n",
    "# Apply the formatter to the colorbar's x-axis\n",
    "cb.ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# --- 3. Display the Plot ---\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 3D Representation of the Electric Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_SW, iteration_PE = 10, 13\n",
    "print(f\"SW equivalent time for iteration#{iteration_SW}: {(iteration_SW-1)*CONVERT_ITERATION_SW_TIME}\")\n",
    "print(f\"PE equivalent time for iteration#{iteration_PE}: {(iteration_PE-1)*CONVERT_ITERATION_PE_TIME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlyphotoemission\"\n",
    "directory = \"../build-zimmerman-comparison\"\n",
    "#directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/build-dissipationRefinedGrid-initial8max0.8final12\"\n",
    "\n",
    "if iteration_PE <10 :\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*00{iteration_PE}*{configIN}*.txt\")) #{iteration}\n",
    "else:\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*{iteration_PE}*{configIN}*.txt\")) #{iteration}\n",
    "print(filenames)\n",
    "\n",
    "df_PE  = read_data_format_efficient(filenames,scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "configIN = \"onlysolarwind\"\n",
    "directory = \"../build-zimmerman-SWonly-comparison\"\n",
    "#directory = \"/storage/scratch1/5/avira7/Grain-Charging-Simulation-Data/build-dissipationRefinedGrid-initial8max0.8final12\"\n",
    "\n",
    "if iteration_SW <10 :\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*00{iteration_SW}*{configIN}*.txt\")) #{iteration}\n",
    "else:\n",
    "    filenames = sorted(glob.glob(f\"{directory}/fieldmaps/*{iteration_SW}*{configIN}*.txt\")) #{iteration}\n",
    "print(filenames)\n",
    "\n",
    "df_SW = read_data_format_efficient(filenames,scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Your existing setup code here...\n",
    "# -----------------------------------------\n",
    "fieldIN = df_PE[iteration_PE]\n",
    "FIELD_AVERAGE_RADIUS = 2.5e-3\n",
    "vmin, vmax = (-0.005, 0.005) \n",
    " \n",
    "geometry_center = stacked_spheres.centroid\n",
    "red_point = np.array([-0.1, 0., 0.1 + 0.037]) + geometry_center\n",
    " \n",
    "# ----------------------------------------------------\n",
    "# Step 0: Filter data\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    " \n",
    "initial_mask = (magnitudes > 0) & (points[:,1]>=-0.1+geometry_center[1]) & (points[:,1]<=0.1+geometry_center[1])\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    " \n",
    "epsilon_0 = 8.854187817e-12\n",
    " \n",
    "# Create field cloud\n",
    "field_cloud = pv.PolyData(points)\n",
    "field_cloud[\"E_x\"] = vectors[:, 0]\n",
    "field_cloud[\"E_y\"] = vectors[:, 1]\n",
    "field_cloud[\"E_z\"] = vectors[:, 2]\n",
    " \n",
    "print(f\"Starting points (filtered): {len(points)}\")\n",
    " \n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup\n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    " \n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    " \n",
    "bbox_bounds = field_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = (\n",
    "    pv_spheres\n",
    "    .clip_box(bbox, invert=False)\n",
    "    .extract_surface()\n",
    "    .compute_normals(point_normals=True, cell_normals=True, inplace=False)\n",
    ")\n",
    " \n",
    "# ============================================================\n",
    "# Interpolate field to face centers\n",
    "# ============================================================\n",
    "face_centers = pv_spheres_cropped.cell_centers().points\n",
    " \n",
    "face_field_cloud = pv.PolyData(face_centers)\n",
    "face_field_interp = face_field_cloud.interpolate(\n",
    "    field_cloud,\n",
    "    radius=0.002,\n",
    "    strategy='closest_point',\n",
    "    sharpness=3.0,\n",
    "    null_value=0.0\n",
    ")\n",
    " \n",
    "# Extract face-centered E-fields\n",
    "E_x_faces = face_field_interp[\"E_x\"]\n",
    "E_y_faces = face_field_interp[\"E_y\"]\n",
    "E_z_faces = face_field_interp[\"E_z\"]\n",
    "E_vec_faces = np.stack([E_x_faces, E_y_faces, E_z_faces], axis=1)\n",
    " \n",
    "# Face normals\n",
    "face_normals = pv_spheres_cropped.cell_normals\n",
    "nx = face_normals[:, 0]   # <-- x-direction component\n",
    " \n",
    "# ============================================================\n",
    "# Compute Maxwell electric pressure (normal)\n",
    "# ============================================================\n",
    "E_dot_n = np.einsum('ij,ij->i', E_vec_faces, face_normals)\n",
    "E_mag_sq = np.einsum('ij,ij->i', E_vec_faces, E_vec_faces)\n",
    " \n",
    "# Normal pressure (scalar)\n",
    "P_normal_faces = epsilon_0 * (E_dot_n**2 - 0.5 * E_mag_sq)\n",
    " \n",
    "# ============================================================\n",
    "# Compute X-directed electric pressure\n",
    "# ============================================================\n",
    "P_x_faces = P_normal_faces * nx   # <-- THIS IS WHAT YOU WANTED\n",
    " \n",
    "# Save to cell data\n",
    "pv_spheres_cropped.cell_data[\"electric_pressure_x\"] = P_x_faces\n",
    " \n",
    "print(f\"Computed x-directed electric pressure in {time.time() - start_time_geo:.2f}s\")\n",
    " \n",
    "# ============================================================\n",
    "# Plotting using Px\n",
    "# ============================================================\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    " \n",
    "pl.add_mesh(\n",
    "    pv_spheres_cropped,\n",
    "    scalars=\"electric_pressure_x\",\n",
    "    cmap=\"seismic\",\n",
    "    opacity=1,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],\n",
    "    interpolate_before_map=False,\n",
    "    preference=\"cell\"\n",
    ")\n",
    " \n",
    "pl.view_xy()\n",
    "pl.show(jupyter_backend='static')\n",
    " \n",
    "# ============================================================\n",
    "# Extract Line Plot Data (y=0 top surface)\n",
    "# ============================================================\n",
    "y_tolerance = 0.01\n",
    "z_min = 0.08\n",
    " \n",
    "x_centers = face_centers[:, 0]\n",
    "y_centers = face_centers[:, 1]\n",
    "z_centers = face_centers[:, 2]\n",
    " \n",
    "Px_faces = P_x_faces  # shorthand\n",
    " \n",
    "line_mask = (np.abs(y_centers) < y_tolerance) & (z_centers > z_min)\n",
    " \n",
    "x_line = x_centers[line_mask]\n",
    "z_line = z_centers[line_mask]\n",
    "Px_line = Px_faces[line_mask]\n",
    " \n",
    "# Bin + average\n",
    "x_bin_width = 0.005\n",
    "x_min, x_max = x_line.min(), x_line.max()\n",
    "x_bins = np.arange(x_min, x_max + x_bin_width, x_bin_width)\n",
    "x_bin_centers = (x_bins[:-1] + x_bins[1:]) / 2\n",
    " \n",
    "bin_indices = np.digitize(x_line, x_bins)\n",
    " \n",
    "x_line_avg, z_line_avg, Px_line_avg = [], [], []\n",
    " \n",
    "for i in range(1, len(x_bins)):\n",
    "    mask = (bin_indices == i)\n",
    "    if mask.any():\n",
    "        x_line_avg.append(x_line[mask].mean())\n",
    "        z_line_avg.append(z_line[mask].mean())\n",
    "        Px_line_avg.append(Px_line[mask].mean())\n",
    " \n",
    "# Sort\n",
    "x_line_sorted_PE = np.array(x_line_avg)\n",
    "z_line_sorted_PE = np.array(z_line_avg)\n",
    "pressure_line_sorted_PE = np.array(Px_line_avg)\n",
    " \n",
    "sort_idx = np.argsort(x_line_sorted_PE)\n",
    "x_line_sorted_PE = x_line_sorted_PE[sort_idx]\n",
    "z_line_sorted_PE = z_line_sorted_PE[sort_idx]\n",
    "pressure_line_sorted_PE = pressure_line_sorted_PE[sort_idx]\n",
    " \n",
    "print(f\"Extracted {len(x_line)} raw points, averaged into {len(x_line_sorted_PE)} bins along y=0 line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Your existing setup code here...\n",
    "# -----------------------------------------\n",
    "fieldIN = df_SW[iteration_SW]\n",
    "FIELD_AVERAGE_RADIUS = 2.5e-3\n",
    "vmin, vmax = (-0.01, 0.01) \n",
    " \n",
    "geometry_center = stacked_spheres.centroid\n",
    "red_point = np.array([-0.1, 0., 0.1 + 0.037]) + geometry_center\n",
    " \n",
    "# ----------------------------------------------------\n",
    "# Step 0: Filter data\n",
    "# ----------------------------------------------------\n",
    "start_time = time.time()\n",
    "points = fieldIN[\"pos\"]\n",
    "vectors = fieldIN[\"E\"]\n",
    "magnitudes = fieldIN[\"E_mag\"]\n",
    " \n",
    "initial_mask = (magnitudes > 0) & (points[:,1]>=-0.1+geometry_center[1]) & (points[:,1]<=0.1+geometry_center[1])\n",
    "points = points[initial_mask]\n",
    "vectors = vectors[initial_mask]\n",
    "magnitudes = magnitudes[initial_mask]\n",
    " \n",
    "epsilon_0 = 8.854187817e-12\n",
    " \n",
    "# Create field cloud\n",
    "field_cloud = pv.PolyData(points)\n",
    "field_cloud[\"E_x\"] = vectors[:, 0]\n",
    "field_cloud[\"E_y\"] = vectors[:, 1]\n",
    "field_cloud[\"E_z\"] = vectors[:, 2]\n",
    " \n",
    "print(f\"Starting points (filtered): {len(points)}\")\n",
    " \n",
    "# ----------------------------------------------------\n",
    "# Step 1: Geometry Setup\n",
    "# ----------------------------------------------------\n",
    "start_time_geo = time.time()\n",
    " \n",
    "pv_spheres = pv.PolyData(\n",
    "    stacked_spheres.vertices,\n",
    "    np.hstack([np.full((len(stacked_spheres.faces), 1), 3), stacked_spheres.faces])\n",
    ").compute_normals()\n",
    " \n",
    "bbox_bounds = field_cloud.bounds\n",
    "bbox = pv.Box(bounds=bbox_bounds)\n",
    "pv_spheres_cropped = (\n",
    "    pv_spheres\n",
    "    .clip_box(bbox, invert=False)\n",
    "    .extract_surface()\n",
    "    .compute_normals(point_normals=True, cell_normals=True, inplace=False)\n",
    ")\n",
    " \n",
    "# ============================================================\n",
    "# Interpolate field to face centers\n",
    "# ============================================================\n",
    "face_centers = pv_spheres_cropped.cell_centers().points\n",
    " \n",
    "face_field_cloud = pv.PolyData(face_centers)\n",
    "face_field_interp = face_field_cloud.interpolate(\n",
    "    field_cloud,\n",
    "    radius=0.002,\n",
    "    strategy='closest_point',\n",
    "    sharpness=3.0,\n",
    "    null_value=0.0\n",
    ")\n",
    " \n",
    "# Extract face-centered E-fields\n",
    "E_x_faces = face_field_interp[\"E_x\"]\n",
    "E_y_faces = face_field_interp[\"E_y\"]\n",
    "E_z_faces = face_field_interp[\"E_z\"]\n",
    "E_vec_faces = np.stack([E_x_faces, E_y_faces, E_z_faces], axis=1)\n",
    " \n",
    "# Face normals\n",
    "face_normals = pv_spheres_cropped.cell_normals\n",
    "nx = face_normals[:, 0]   # <-- x-direction component\n",
    " \n",
    "# ============================================================\n",
    "# Compute Maxwell electric pressure (normal)\n",
    "# ============================================================\n",
    "E_dot_n = np.einsum('ij,ij->i', E_vec_faces, face_normals)\n",
    "E_mag_sq = np.einsum('ij,ij->i', E_vec_faces, E_vec_faces)\n",
    " \n",
    "# Normal pressure (scalar)\n",
    "P_normal_faces = epsilon_0 * (E_dot_n**2 - 0.5 * E_mag_sq)\n",
    " \n",
    "# ============================================================\n",
    "# Compute X-directed electric pressure\n",
    "# ============================================================\n",
    "P_x_faces = P_normal_faces * nx   # <-- THIS IS WHAT YOU WANTED\n",
    " \n",
    "# Save to cell data\n",
    "pv_spheres_cropped.cell_data[\"electric_pressure_x\"] = P_x_faces\n",
    " \n",
    "print(f\"Computed x-directed electric pressure in {time.time() - start_time_geo:.2f}s\")\n",
    " \n",
    "# ============================================================\n",
    "# Plotting using Px\n",
    "# ============================================================\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    " \n",
    "pl.add_mesh(\n",
    "    pv_spheres_cropped,\n",
    "    scalars=\"electric_pressure_x\",\n",
    "    cmap=\"seismic\",\n",
    "    opacity=1,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],\n",
    "    interpolate_before_map=False,\n",
    "    preference=\"cell\"\n",
    ")\n",
    " \n",
    "pl.view_xy()\n",
    "pl.show(jupyter_backend='static')\n",
    " \n",
    "# ============================================================\n",
    "# Extract Line Plot Data (y=0 top surface)\n",
    "# ============================================================\n",
    "y_tolerance = 0.01\n",
    "z_min = 0.08\n",
    " \n",
    "x_centers = face_centers[:, 0]\n",
    "y_centers = face_centers[:, 1]\n",
    "z_centers = face_centers[:, 2]\n",
    " \n",
    "Px_faces = P_x_faces  # shorthand\n",
    " \n",
    "line_mask = (np.abs(y_centers) < y_tolerance) & (z_centers > z_min)\n",
    " \n",
    "x_line = x_centers[line_mask]\n",
    "z_line = z_centers[line_mask]\n",
    "Px_line = Px_faces[line_mask]\n",
    " \n",
    "# Bin + average\n",
    "x_bin_width = 0.005\n",
    "x_min, x_max = x_line.min(), x_line.max()\n",
    "x_bins = np.arange(x_min, x_max + x_bin_width, x_bin_width)\n",
    "x_bin_centers = (x_bins[:-1] + x_bins[1:]) / 2\n",
    " \n",
    "bin_indices = np.digitize(x_line, x_bins)\n",
    " \n",
    "x_line_avg, z_line_avg, Px_line_avg = [], [], []\n",
    " \n",
    "for i in range(1, len(x_bins)):\n",
    "    mask = (bin_indices == i)\n",
    "    if mask.any():\n",
    "        x_line_avg.append(x_line[mask].mean())\n",
    "        z_line_avg.append(z_line[mask].mean())\n",
    "        Px_line_avg.append(Px_line[mask].mean())\n",
    " \n",
    "# Sort\n",
    "x_line_sorted_PE = np.array(x_line_avg)\n",
    "z_line_sorted_PE = np.array(z_line_avg)\n",
    "pressure_line_sorted_PE = np.array(Px_line_avg)\n",
    " \n",
    "sort_idx = np.argsort(x_line_sorted_PE)\n",
    "x_line_sorted_PE = x_line_sorted_PE[sort_idx]\n",
    "z_line_sorted_PE = z_line_sorted_PE[sort_idx]\n",
    "pressure_line_sorted_PE = pressure_line_sorted_PE[sort_idx]\n",
    " \n",
    "print(f\"Extracted {len(x_line)} raw points, averaged into {len(x_line_sorted_PE)} bins along y=0 line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LaTeX rendering\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Set the global font size\n",
    "mpl.rcParams.update({'font.size': 14})\n",
    "\n",
    "cmap = plt.cm.seismic \n",
    "vmin, vmax = (-1, 1)\n",
    "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 0.1))\n",
    "\n",
    "# --- 1. Create and Configure the ScalarFormatter ---\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "\n",
    "formatter.set_useOffset(False) \n",
    "formatter.set_powerlimits((0, 0)) \n",
    "\n",
    "# --- 2. Create the Colorbar and apply the Formatter ---\n",
    "cb = mpl.colorbar.ColorbarBase(\n",
    "    ax, \n",
    "    cmap=cmap, \n",
    "    norm=norm, \n",
    "    orientation='horizontal', label=r\"Electric Pressure (Pa)\"\n",
    ")\n",
    "\n",
    "# Apply the formatter to the colorbar's x-axis\n",
    "cb.ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# --- 3. Display the Plot ---\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LaTeX rendering\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Set the global font size\n",
    "mpl.rcParams.update({'font.size': 14})\n",
    "\n",
    "cmap = plt.cm.seismic \n",
    "vmin, vmax = (-0.05, 0.05) \n",
    "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 0.1))\n",
    "\n",
    "# --- 1. Create and Configure the ScalarFormatter ---\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "\n",
    "formatter.set_useOffset(False) \n",
    "formatter.set_powerlimits((0, 0)) \n",
    "\n",
    "# --- 2. Create the Colorbar and apply the Formatter ---\n",
    "cb = mpl.colorbar.ColorbarBase(\n",
    "    ax, \n",
    "    cmap=cmap, \n",
    "    norm=norm, \n",
    "    orientation='horizontal', label=r\"Electric Pressure (Pa)\"\n",
    ")\n",
    "\n",
    "# Apply the formatter to the colorbar's x-axis\n",
    "cb.ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# --- 3. Display the Plot ---\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Create Line Plot\n",
    "# ============================================================\n",
    "\n",
    "# Create a figure with the line plot overlaid on a slice view\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "# Plot the pressure line\n",
    "# ax.plot(x_line_sorted_PE * 1000, pressure_line_sorted_PE, '-', linewidth=2, color=color_list[1],\n",
    "#         label='Photoemission', zorder=5)\n",
    "# ax.axhline(0, color='k', linestyle='-', lw=0.5, alpha=0.8)\n",
    "# ax.tick_params(axis='y', labelcolor=color_list[1])\n",
    "# ax.set_xlabel(r'X Position ($\\mu$m)', fontsize=12)\n",
    "# ax.set_ylabel('Electric Pressure (Pa)', fontsize=12)\n",
    "# ax.set_ylim([-0.6,0.3])\n",
    "plt.axis('off')\n",
    "\n",
    "# Add a background showing the geometry profile (top surface outline)\n",
    "# Extract all top surface points for context\n",
    "top_mask = (np.abs(y_centers) < y_tolerance) & (z_centers > z_min - 0.02)\n",
    "x_top = x_centers[top_mask]\n",
    "z_top = z_centers[top_mask]\n",
    "\n",
    "# Sort and plot as scatter to show geometry extent\n",
    "sort_top = np.argsort(x_top)\n",
    "ax2 = ax.twinx()\n",
    "# ax2.plot(x_line_sorted_SW * 1000, pressure_line_sorted_SW, '-', linewidth=2, color=color_list[2],\n",
    "#         label='Solar Wind', zorder=5)\n",
    "ax2.scatter(x_top * 1000, z_top * 1000 , c='gray', alpha=0.2, s=3, label=None)\n",
    "# ax2.set_ylabel(r'Z Position ($\\mu$m)', fontsize=12, color='gray')\n",
    "# ax2.set_ylim([-0.02,0.01])\n",
    "ax2.set_xlim([-150,150])\n",
    "ax2.tick_params(axis='y', labelcolor=color_list[2])\n",
    "plt.axis('off')\n",
    "plt.savefig(\"figures/sphere_trace.svg\",transparent=True,dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Create Line Plot\n",
    "# ============================================================\n",
    "\n",
    "# Create a figure with the line plot overlaid on a slice view\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "# Plot the pressure line\n",
    "ax.semilogy(x_line_sorted_PE * 1000, pressure_line_sorted_PE, '-', linewidth=2, color='g',\n",
    "        label='Photoemission', zorder=5)\n",
    "ax.axhline(0, color='k', linestyle='-', lw=0.5, alpha=0.8)\n",
    "#ax.tick_params(axis='y', labelcolor=color_list[1])\n",
    "ax.set_xlabel(r'X Position ($\\mu$m)', fontsize=12)\n",
    "ax.set_ylabel('Electric Pressure (Pa)', fontsize=12)\n",
    "ax.set_ylim([-0.6,0.3])\n",
    "\n",
    "# # Add a background showing the geometry profile (top surface outline)\n",
    "# # Extract all top surface points for context\n",
    "# top_mask = (np.abs(y_centers) < y_tolerance) & (z_centers > z_min - 0.02)\n",
    "# x_top = x_centers[top_mask]\n",
    "# z_top = z_centers[top_mask]\n",
    "\n",
    "# # Sort and plot as scatter to show geometry extent\n",
    "# sort_top = np.argsort(x_top)\n",
    "# ax2 = ax.twinx()\n",
    "# ax2.plot(x_line_sorted_SW * 1000, pressure_line_sorted_SW, '-', linewidth=2, color='b',\n",
    "#         label='Solar Wind', zorder=5)\n",
    "ax2.set_ylim([-0.02,0.01])\n",
    "ax2.set_xlim([-150,150])\n",
    "#ax2.tick_params(axis='y', labelcolor=color_list[0])\n",
    "fig.savefig(\"figures/electric_pressure_linesplot.jpeg\",transparent=True,dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Optional: Show 3D visualization with line highlighted\n",
    "# ============================================================\n",
    "\n",
    "pl = pv.Plotter()\n",
    "pl.set_background('white')\n",
    "\n",
    "# Add full mesh with face colors\n",
    "pl.add_mesh(\n",
    "    pv_spheres_cropped,\n",
    "    scalars=\"electric_pressure\",\n",
    "    cmap=\"seismic\",\n",
    "    opacity=0.7,\n",
    "    show_edges=False,\n",
    "    clim=[vmin, vmax],\n",
    "    interpolate_before_map=False,\n",
    "    preference=\"cell\"\n",
    ")\n",
    "\n",
    "# Highlight the extracted line points\n",
    "line_points_3d = np.column_stack([x_line_sorted, np.zeros_like(x_line_sorted), z_line_sorted])\n",
    "line_polydata = pv.PolyData(line_points_3d)\n",
    "pl.add_mesh(line_polydata, color='yellow', point_size=2, render_points_as_spheres=True)\n",
    "\n",
    "# Add connecting line\n",
    "if len(line_points_3d) > 1:\n",
    "    line_cells = np.column_stack([\n",
    "        np.full(len(line_points_3d)-1, 2),\n",
    "        np.arange(len(line_points_3d)-1),\n",
    "        np.arange(1, len(line_points_3d))\n",
    "    ]).flatten()\n",
    "    line_mesh = pv.PolyData(line_points_3d, lines=line_cells)\n",
    "    pl.add_mesh(line_mesh, color='yellow', line_width=5)\n",
    "\n",
    "pl.view_xy()\n",
    "pl.show(jupyter_backend='static')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvista-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
